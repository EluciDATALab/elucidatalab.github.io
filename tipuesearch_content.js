var tipuesearch = {"pages":[{"title":"    EluciDATA Lab\n","text":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    EluciDATA Lab\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\nHome\n\n\n\n\nBlog\n\n\n\n\nStarterkits\n\n\n\n\nAbout\n\n\n\n\nArchive\n\n\n\n\n\n\nSearch\n\n\n\n\n\n\n\n\n\n\n\n\n\n EluciDATA Lab\nWe are the Data and AI Competence lab of Sirris.\nRead More\n\n\n\n\n\nAI Starter Kit Platform\nDiscover our interactive platform to extend your AI and Data Science skills for several relevant industrial use cases!\nRead More\n\n\n\n\n\nPrevious\n\n\n\nNext\n\n\n\n\n\n\n\n\n\nAI Starter Kits with complementary video tutorials\n... to guide you through the different steps AI-based methodology for several industrial use cases, complemented with autodidactic video tutorials.\n\n\n\n\n\nInteractive Experimenting Environment\n... in which all required software is available to get started in no time and allowing you to import your own datasets.\n\n\n\n\n\nBring Your Own Data workshops\n... in which you are coached to apply the methodology solutions to your own data & problem setting.\n\n\n\n\n\n\n\n\n\n\n\nABOUT EluciDATA Lab\n\n\nelucidatalab@sirris.be\n\n\n\n\n\nHOME\nABOUT\nARCHIVE\n\n\n¬© Sirris Gebruikersovereenkomst Privacy policyEluciDATA Lab -The Data and AI Competence Lab of Sirris\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":"","url":"index.html"},{"title":"Welcome","text":"Category: Blog Welcome to the official ElcuiDATA Lab bloging and starterkit platfrom. Here you will hopefully find some latest machine learning content and tutorials. Enjoy!","tags":"Blog","url":"/Blog/first-post.html","loc":"/Blog/first-post.html"},{"title":"Key Take Away Messages","text":"Key Take Away Messages In the video tutorial for this AI Starter Kit, we demonstrated forecasting the demand for a particular resource at a particular point in the future, illustrated on the case of energy demand forecasting. In doing so, we have taken you through the different steps in the data science workflow. We first performed a number of required data preprocessing steps in order to prepare the data for further analysis. This included fusing the electricity and weather data, requiring to resampling the former data to a lower sampling rate. Subsequently, we explored the fused dataset in order to better understand which factors influence the household energy consumption. To this end, we resorted to both visual and statistical exploration methods. Based on this, we could identify a number of yearly, weekly, and daily patterns, including particular behavior during weekends and some holiday periods. This information formed the basis for the feature extraction that followed next, such as the energy consumption a week before, the day of the week and the hour of the day. Finally, we served the extracted features to two different models, namely Random Forest Regression and Support Vector Regression to perform the actual forecasting. Through different experiments, we analyzed the influence of the training strategy, type of machine learning model and effect of data normalization on the model performance. Based on this, it became clear that achieving good prediction results depends on different factors, including the appropriate data preprocessing, the amount of available training data and the algorithm parametrisation. As next steps, we invite you to further explore each of these influences in more detail. Especially with respect to the hyperparameter tuning, it proves worth to further explore the different parameter settings and study how these changes impact the model performance. Furthermore, you can also try to adapt the different steps to the context of your own dataset. While the details of each of the steps might differ, the methodological steps we presented are typically the required phases you need to go through when solving a resource demand forecasting problem. We thank you for completing this video series and hope to welcome you in another AI Starter Kit tutorial. Additional information The video material in this website was developed in the context of the SKAIDive project , financially supported by the European Social Fund , the European Union and Flanders. For more information, please contact us at elucidatalab@sirris.be","tags":"Resource demand Forecasting","url":"/Resource demand Forecasting/2020-01-08-key-take-away-messages.html","loc":"/Resource demand Forecasting/2020-01-08-key-take-away-messages.html"},{"title":"Data Modelling and Forecasting","text":"Data Modelling and Forecasting Now that we gained the necessary insights from the data, extracted a number of meaningful features, and gained some theoretical background on regression models, we will use the AI Starter Kit to discover the most important factors for training a machine learning model. More specifically, we will analyze the influence of the training strategy, type of machine learning model and effect of data normalization on the model performance. To evaluate the performance of the models the mean absolute error or MAE is used - a metric commonly used in literature for this purpose. This metric quantifies to what extent the model forecasts are close to the real values. As the name suggests, the mean absolute error is an average of the absolute errors. It holds that the lower the MAE of the model, the better its performance. As just mentioned, the training strategy, the machine learning model and the data normalization may influence the quality of the predictions. Before we start training a model, let us dive a bit deeper in these three influences. First of all, let us turn to the training strategy. The training data typically highly influences quality of the model. Therefore, in this starter kit, we will experiment with six different training strategies, to study their influence on the quality of the resulting model. First, we will use 1 month before the test month as training data. In the Starter Kit, several models are trained for the predictions of each individual month. Each model is trained on one month before the month we are making predictions for. A one-month gap is introduced between the training and the test month in order to avoid that the last day of the training set is also included in the first day of the test set. For example, the models that will make the predictions for April 2008 will be trained on the data from February 2008. Secondly, we will use 6 months before the test month as training data. This is thus highly similar to the above experiment, but with 6 months of training data. It still includes a 1-month gap. Or we can go back even further in time and use 1 year before the test month as training data. Similarly, we can use 1 month the year before as training data. This is similar to strategy number 1, but with a gap of 11 months. This way, the training data and test data are taken from the same month, one year apart. Further, we can also use all months before the test month as training data. For each test month, a model is trained using all the data prior to this (including a 1-month gap). This strategy simulates the scenario where the model is retrained as new data comes in. Since this requires training several models based on potentially large training sets, this might require a certain computational time. Finally, we experiment with training on the first year of data and make predictions on the rest. This strategy is commonly used for training these kinds of models. With these training strategies, we will see how strongly the amount of data but also the seasonal pattern will influence the quality of the model. As already introduced in the former video, we will train two different types of models, namely a Random Forest Regressor and a Support Vector Regressor. Besides these models, we will also use a simple benchmark model that we use to compare the performance of the models against. For the prediction of the energy of a certain day, this benchmark model simply takes the energy consumption of the previous day at the same time. Note that for this approach no model is built and consequently it does not include any training phase. Finally, a short note on normalization. We saw already in the video on data understanding that the scale for the outside temperature and global active power are quite different. This is also true for the remaining features that we introduced. Therefore, it might be necessary to normalize the data, that is, rescale it such that all input and output values are between 0 and 1. This is indeed a requirement for the correct training of some machine learning models. We will test in the Starter Kit whether it makes a difference for the models suggested. At first, let us run the baseline model as we will later on compare all model results to these results. The mean absolute error is 604.45. Putting this number into relation with the mean global active power of 1118, the error is comparably high. For reasons of comparability, each result will be shown in the table just below the interface. As discussed above, we first want to analyze the influence of the training strategy. Therefore, we train the Support Vector Regressor on strategy 1, so for 1 month of data, and on strategy 2, for 6 months of data. In this first experiment, we will not normalize the data beforehand. For both strategy 1 and 2, the mean absolute error is even bigger than for the benchmark model. Therefore, we are going to use the normalized data instead. With that, the model predictions improve and for both strategies, this results in better forecasts than the baseline model. We can do the same for the Random Forest Regressor. We train if for both strategies ‚Äì namely strategy 1 and 2 ‚Äì and on the normalized and non-normalized data. In all four cases, the results are better than those obtained by both the baseline model and the Support Vector Regressor. Now it is up to you. Train different models and find out which model returns the best results and which influences are the strongest. If you want, you can pause the video for this. These are the basic findings when training all models: Concerning the training strategy, we see that for the Support Vector Regressor the standard train-test split has the best performance, although using all months and using a one-year window before the test month as training data both have a similar performance. Furthermore, we clearly see that using a one-year window before the test month as training data leads to better performance than the other training strategies. We note that for both this \"one year window\" strategy and the standard train-test split, the training data is one year. The difference, however, is that for the former the training set changes, while for the latter it is fixed meaning that all the data from 2017 is used. A general trend that can be observed is that the performance increases as a larger training set is used. We therefore urge the user to look at the predictions made using the training strategy where all months before the test month are used as training data: we can expect that the predictions for the later months will be better than for the first months since more training data was used. However, this is hard to see by eye in the prediction plot directly. Regarding the chosen model, we can see that both the Support Vector Regressor and Random Forest Regressor outperform the simple baseline we set up. Further, the Random Forest Regressor outperforms the Support Vector Regressor for all training strategies. This shows the importance of knowing which model is best suited for your problem and testing different ones. Important to note though, is that no extensive hyperparameter tuning was performed, which could alter this observation. This is required for a proper validation of the algorithms and therefore we encourage the user to also experiment with altering the hyperparameters and study the influence on the results. Finally, normalizing the data, rescaling it such that the input and output variables all have values within a similar range - in this case, between 0 and 1 - is a common step when setting up machine learning models. This is because, depending on the model, they work much better with values of this order of magnitude. We see that normalization indeed greatly improves the predictions for the Support Vector Regressor but has little influence on the random forest regressor. Indeed, algorithms such as random forests and decision trees are not influenced by the scale of the input variables. We hope that you have gained more insights on how the training strategy, the type of machine learning model and data normalization influence the model performance and are familiar now with the usage of the interface. We suggest that you try a number of additional combinations of settings and think about in which way they influence the quality of the model. In the next video, we will summarize the key take away messages and provide you with a number of suggestions for additional experiments to gain additional insights. Additional information The video material in this website was developed in the context of the SKAIDive project , financially supported by the European Social Fund , the European Union and Flanders. For more information, please contact us at elucidatalab@sirris.be","tags":"Resource demand Forecasting","url":"/Resource demand Forecasting/2020-01-07-data-modelling-and-forecasting.html","loc":"/Resource demand Forecasting/2020-01-07-data-modelling-and-forecasting.html"},{"title":"Key Take Away Messages","text":"Key Take Away Messages In the video tutorial for this AI Starter Kit, we have demonstrated techniques that can be used to preprocess time series data in order to improve its quality for further exploitation. To illustrate the respective methods and techniques, we have used a real-world dataset containing wind turbine data that exhibits the typical characteristics of industrial datasets, including noise, outliers, missing values, and seasonal patterns. First, we demonstrated how resampling and smoothing can be applied to better understand the behaviour of the time series. As we have shown, resampling techniques allow to reduce the temporal granularity, thereby revealing longer-term trends and hiding sharp, fast fluctuations in the signal. Smoothing on the other hand modifies the time series data such that individual points that are higher than the adjacent points - presumably caused by noise - are reduced, and points that are lower than the adjacent points are increased leading to a smoother signal. Subsequently, we showed how the quality of the data can be improved through normalization and outlier detection. Normalization rescales the range of a particular variable in order to make different variables with different ranges comparable. Outlier detection tried to identify the extreme values in the time series that deviate from other observations in the data. The detected outliers can subsequently be examined in more detail to investigate whether these were caused by measurement errors, in which case they can be removed, or if it concerns unexpected phenomena which are interesting to look into in more detail. Finally, another frequently occurring problem with time series data is that it suffers from missing data, caused by for example senor malfunctioning, communication errors, or due to outliers that were removed. We illustrated how to impute this missing data by means of 3 different techniques: linear interpolation, fleet-based imputation and pattern-based interpolation, and outlined what are the advantages and disadvantages of these techniques. The use of these methods allows to improve the quality of the data and to prepare it for further exploration, analysis and modelling purposes. This is a crucial step, as it will improve the completeness and reliability of the input data, and consequently the accuracy of your results. We thank you for completing this video series and hope to welcome you in another AI Starter Kit tutorial.","tags":"Time Series Preprocessing","url":"/Time Series Preprocessing/2020-01-07-key-take-away-messages.html","loc":"/Time Series Preprocessing/2020-01-07-key-take-away-messages.html"},{"title":"Key Take Away Messages","text":"Key Take Away Messages In the video tutorial for this AI Starter Kit, we covered the basics of using deep learning for remaining useful lifetime prediction, which is one of the central topics in predictive maintenance. We illustrated the approach on the use case of predicting the remaining useful life of aircraft engines based on run-to-failure data. In doing so, we have taken you through the different steps in the data science workflow. We started with the business and data understanding, which allowed us to gain more insights into the specific problem to solve and the structure of the data. Based on this information, we continued with the data preprocessing, in which we explained how to appropriately prepare the data. This included the construction of a training, test and validation set to learn and evaluate a model to predict whether an engine will fail within a certain number of operational cycles. In the video on data modelling and analysis, we showed you how to use deep learning to construct a model for remaining useful life prediction. In particular, we applied a deep learning approach called long short-term memory (LSTM) networks which allows the construction of a predictive model without the time-consuming feature engineering step. This voided the need to manually extract the characteristics from which the algorithm can learn. Finally, we also explained you how to experimentally validate the resulting model and compare the results of different models. While the use of deep learning in some cases might eliminate the need for manual feature engineering, it remains important to tune the models with the appropriate parameter settings, which requires the necessary effort. While the presented approach already gives promising results, there are still quite some improvements possible. Therefore, we recommend you to further try tuning the different parameters, and study the effect on the results. You can also continue to experiment with different network architectures, for example by altering the number of layers and nodes. If you want to gain deeper insights in remaining useful lifetime prediction, a good exercise is trying to cast the prediction problem as a regression or multi-class classification task instead of as a binary classification task. It might also be worth to experiment with the methodology on a larger dataset with a higher number of assets or to try to adapt the different steps to the context of your own dataset. While the details of each of the steps might differ, the methodological steps we presented are typically the required phases you need to go through when solving a remaining useful lifetime prediction model. We thank you for completing this video series and hope to welcome you in another AI Starter Kit tutorial.","tags":"Remaining Useful Life Prediction","url":"/Remaining Useful Life Prediction/2020-01-07-key-take-away-messages.html","loc":"/Remaining Useful Life Prediction/2020-01-07-key-take-away-messages.html"},{"title":"Data Modelling and Analysis","text":"Data Modelling and Analysis Welcome to the fourth and last video of the tutorial for the AI Starter Kit on remaining useful lifetime prediction! In this video, we will explain you in detail how a deep learning algorithm can be used to train a model that is able to determine if an aircraft engine is entering the last 30 cycles of its remaining useful lifetime. In the introductory video to Deep Learning, we discussed the difference between Machine Learning and Deep Learning. One of the main differences was given by the need of feature extraction. This can be a very time and knowledge demanding task. As we will concentrate on LSTMs in this AI starter kit, fortunately this step is taken care of by the deep learning algorithm itself. Nevertheless, some data modelling is necessary beforehand. As a first step in the modelling phase, we will prepare the data to serve as input for the LSTM network. When using LSTMs in the time-series domain, one important parameter to pick is the sequence length which is the window for LSTMs to look back in time. We discussed already in the data understanding video how differently the single variables for a given engine behave and consequently that the time when the degradation becomes visible is different for different engines. Hence, the window size chosen for the training data strongly influences the classification results. In order to model the data for training the algorithm, we first need to reshape the input information. So far, the data consists of the sequential measurements for each of the sensors and settings over time, engine per engine, meaning that one row per cycle and engine is given in the table. This format is however not so suitable for an LSTM model. Therefore, we create a matrix in the format (samples x sequence length x features), where 'samples' is the number of sequences that we will use for model training, ‚Äòsequence length' is the look back window as discussed before and 'features' is the number of input values of each sequence at each time step. In our case, we selected all sensor values, as well as the 3 settings parameters and the (normalised) current cycle the machine is in at this point. With the data finally prepared, how can we now actually implement the LSTM architecture? Fortunately, there is no need to start from scratch but there are several open-source tools available that support you when building Deep Learning models. Keras is a deep learning library written in Python. It acts as an interface to the popular tensorflow library. This makes the implementation a lot more feasible. In the AI Starter Kit though, this is even simpler. Instead of implementing the Deep Learning network yourself, an easy-to-use interface was set up, to adjust the most important variables of the neural network. Once you decide that the parameters specified for building the model are correct, you simply push the \"Train model\" button at the bottom of the page and the whole implementation and model training is done automatically. Of course, it is also possible to modify the different parameters directly in the code. For more information on the meaning of each of the settings, you can have a look at the documentation page of Keras. Note that modifying the parameters outside of the scope that is defined in the interactive module can heavily influence the training time and the classification quality. So now it's time to build your own deep learning model. First of all, select the number of intermediate or so-called ‚Äòhidden' layers. For this first experiment, we select one single intermediate layer only. We set its size to 30 neurons. This corresponds to a rather small network but will be sufficient for a first test. As discussed, we need to decide on the size of the look back window as well. In this first test, we will use a sequence length of 50 cycles. Furthermore, we add a dropout layer after each LSTM layer. The dropout consists in randomly setting a fraction rate of neurons to 0 at each update during training time. This helps preventing overfitting. We choose a value of 0.2 for this first experiment. Finally, we need to select the number of epochs to train. The epochs define the number of times to iterate over the training data arrays. The aim is that the model improves during each training epoch. In general, the more epochs, the better the results until we reach the given model's limit. Here, we will use 15 epochs. This model can now be trained. Now that we have a trained model, we can evaluate its performance. We will first evaluate it on the training data and subsequently on the test data. If both evaluations result in approximately the same score, this gives an indication that the model is generalizable to unseen datasets and thus not overfitting on the training data. In the interface, we switch to the tab Evaluation on the top of the animation to get more insights into the quality of the trained model. Pushing the button \"Evaluate model\" will lead to the evaluation of the last trained model. Depending on the use case, different evaluation metrics can be important. Therefore, we will on the one hand evaluate the model in function of accuracy, which measures the fraction of all instances that are correctly categorized. More formally, it is the ratio of the number of correct classifications to the total number of correct or incorrect classifications. On the other hand, we also show the so-called confusion matrix. The confusion matrix shows that the model is able to correctly classify that a specific engine is not going to fail within N cycles for almost all of the more than 12,000 samples. Vice versa, the model is able to correctly classify for almost all of the more than 3000 cases that a specific engine is going to fail within N cycles. In the summary table at the bottom of the page, several additional metrics are shown. Take your time to go through them once you run these experiments yourself. Note that this evaluation is performed on the training data. In order to evaluate the model against the unknown data, we continue to the next tab in the interface. In this case, the same evaluation as before is performed but on the test data. Also here, the results are promising. Now run an experiment yourself. For this, chose the following parameters and rerun the pipeline while pausing the video. Did it work properly? Congratulations! The only differences between the two experiments are the number of layers and their corresponding sizes. Which difference do you observe? Compare the results in the tables in both tabs - evaluation and testing. When comparing the two models, which one would you use and why? We hope that you have gained more insights in the key factors that influence the quality of a deep neural network and are familiar now with the usage of the interface. We suggest that you try different combinations of settings and think about in which way they influence the quality of the model. In the next video, we will summarize the key take away messages and provide you with a number of suggestions for additional experiments to gain additional insights.","tags":"Remaining Useful Life Prediction","url":"/Remaining Useful Life Prediction/2020-01-06-data-modelling-and-analysis.html","loc":"/Remaining Useful Life Prediction/2020-01-06-data-modelling-and-analysis.html"},{"title":"Data Science Theory - Advanced Regression Models","text":"Data Science Theory: Advanced Regression Models Before deciding on the most appropriate algorithm to solve a particular data science problem, a first step is to decide which type of task you are trying to solve. In order to do so, you usually need to start with finding the answer to a number of questions, based on the case under consideration. Without a clear understanding of the use case, even the best data science model will not help. A first question to answer in that respect is which type of outcome is expected from the use case owner. Is the aim to predict a category, such as ‚Äònormal', ‚Äòdegrading' or ‚Äòfailed'? If yes, the next question to answer is whether labelled data is available or not. Labelled data is data for which examples are available that are annotated by a domain expert with the classes to predict. Put differently, for each data point or set of data points, a class is defined. Usually, the number of unique classes is rather small. This data will be used by the algorithm for training a model. Once trained, it can be evaluated on a test data set, for which the classes are known but will not be visible to the model. Evaluating the ratio of correctly predicted classes gives a measure of the quality of the model. Often used algorithms for classification are k-Nearest Neighbors, Decision Trees or Support Vector Machines. But what can we do if we do not have information available on possible classes? In that case, introducing a similarity between the examples that you have available makes it possible to cluster different data points into groups. These groups can then be used to gain deeper insights into the data, and in some cases can be mapped to particular classes. Partitioning-based clustering, hierarchical clustering or density-based clustering are often used techniques for this purpose. The situation is different in case a numerical value should be predicted. It is similar to the classification task, but the prediction range is continuous. For these so-called regression tasks, Ordinary Least Squares or Support Vector Regression are often used. If the goal is neither to predict a class nor a numerical value, but rather a future state, one typically turns to graphical modelling algorithms in order to predict these states. These techniques also allow one to include the available background knowledge into the modelling process. Examples of graphical modelling techniques are Hidden Markov Models and Bayesian Networks. To make the difference between the single categories a bit more clear, we discuss some examples: In case no labelled data is available, clustering of data points can provide insights in different modes in the data. An example is performance benchmarking of industrial assets. Let's assume the data to analyze comes from a wind turbine park. When looking at several measurements, like for example of the power curve, the wind speed, and the wind direction, we can identify different modes in which the single wind turbines are operating. In contrast, assume that we are interested in the expected power production of a particular wind turbine in the following days for which we have a weather forecast. We can use this information as input variables for a regression model and therewith predict the power production. If labels are attached to the gathered data, for example on the root cause of particular sensor failures, a classification algorithm can be used to train a model that is able to determine which fault is associated with a certain set of sensor readings. With this knowledge, we can decide which type of algorithms is suitable for the forecasting of electricity consumption. Let us use the decision tree for this: Do we want to predict a category? No, right, we are looking for a continuous value. So, we go to the right. And yes, we need to predict a value. Therefore, it is a regression task we are facing here. The question we want to answer is precisely: How much electricity will be consumed in the next hour, taking into account historical information regarding the electricity consumption and the outside temperature? There is a bunch of regression algorithms that can be used in various contexts. In our case, we have a comparably small feature set and all of them are numerical. Therefore, we will go for two commonly used algorithms for the prediction, namely Random Forest Regressors and Support Vector Regressors. We will introduce both algorithms in more detail in the remainder of this video. Random Forest Regression We start with Random Forest Regression. The base to build a random forest is a decision tree ‚Äì which works similarly to the one we just used to determine which class of algorithms is suitable for the electricity forecasting. Since in a random forest, the model is defined by a combination of trees, it is a so-called ensemble method. Ensemble methods help improve machine learning results by combining several models. This approach allows the production of better predictive performance compared to a single model. From each decision tree a value is predicted, and the final prediction will be a weighted function of all predictions. Here we see a very simplistic version of a random forest regressor with only three decision trees. All trees are trained in parallel and each one will predict a value for a given set of input variables. The final prediction in this case would be the mean value of all predictions, ergo 10,67. In order to improve the performance of a model, you need to tune the algorithm's hyperparameters. Hyperparameters can be considered as the algorithm's settings, or put simply, the knobs that you can turn to gain a better result. These hyperparameters are tuned during the training phase by the data scientist. In the case of a random forest, the most important hyperparameters include the number of decision trees in the forest, the maximum depth of each of these trees, and the minimum number of examples and maximum number of features to consider when splitting an internal node in one of these trees. Support Vector Regression Another type of regression approach is support vector regression. While support vectors are mainly used in the field of classification, with some adaptions, it also works for regression tasks. It works similarly to an ordinary least squares regression where the linear regression line is targeted with the smallest overall deviation from the data points. This is very handy in case of linear dependencies and for clean data. But as soon as there are several outliers in the data set or the relation between the data points is non-linear, the quality of the model can decrease significantly. Especially in the context of industrial data, this can never be fully avoided. For Support Vector Regression a band of width epsilon Œµ is defined. We call that band the hyperplane . The aim is to search the hyperplane that includes most points while at the same time the sum of the distance of the outlying points may not exceed a given threshold. The training instances closest to the hyperplane that help define the margins are called Support Vectors . As for random forest regression, also support vector regression has a number of important hyperparameters that can be adjusted to optimize the performance. A first important hyperparameter is the choice for the type of kernel to use. A kernel is a set of mathematical functions that takes data as input and transform it into the required form. This kernel is used for finding a hyperplane in a higher dimensional space. The most widely used kernels include Linear, Non-Linear, Polynomial, Radial Basis Function (RBF) and Sigmoid. The selection of the type of kernel typically depends on the characteristics of the dataset. The cost parameter C tells the SVR optimization how much you want to avoid a wrong regression for each of the training examples. For large values of C, the optimization will choose a smaller-margin hyperplane if that hyperplane does a better job of getting all the training points predicted correctly, and vice versa. The size of this margin can be set by epsilon, which specifies the band within which no penalty is associated in the training loss function with points predicted within a distance epsilon from the actual value. Now that we gained some more knowledge on these two frequently used regression approaches, in the next video we will explain how to train a regression model for our household energy consumption prediction problem. Additional information The video material in this website was developed in the context of the SKAIDive project , financially supported by the European Social Fund , the European Union and Flanders. For more information, please contact us at elucidatalab@sirris.be","tags":"Resource demand Forecasting","url":"/Resource demand Forecasting/2020-01-06-data-science-theory-advanced-regression-models.html","loc":"/Resource demand Forecasting/2020-01-06-data-science-theory-advanced-regression-models.html"},{"title":"Handling Missing Data","text":"Handling Missing Data In this video, we will finally use the insights from before and impute missing values in the dataset. Real-world industrial datasets often suffer from missing data due to several reasons. Missing data occurs often due to sensor malfunctioning or communication errors. In addition, if we have removed outliers as shown in the previous video, these will also show up as missing values in the data. In both cases, we can fill the missing values using so-called data imputation techniques. Multiple techniques exist and which one to choose depends on the data characteristics. The choice depends on the presence of trends, the length of the missing period, etc. In this video, we will present 3 different imputation techniques, namely * Linear interpolation * Fleet-based interpolation, and * Pattern-based imputation To evaluate the efficiency of the different methods, we will create a set of synthetic missing data events with varying durations. The main advantage of this approach is that it allows us to compare the outcome of the imputation procedure with the real, observed values. We start with the linear interpolation of values and interpolation based on the fleet median. Linear interpolation is a simple technique that is frequently used in case a small number of points are missing within a particular period. It simply connects the last point before and the first point after the missing data episode with a straight line. Fleet median, on the other hand, exploits the fleet-based aspect of our asset for imputing periods of missing data. For the dataset under investigation, we know the assets are co-located and are, therefore, exposed to similar conditions like for example the wind direction and speed. Hence, we can compute the median value of the wind speed for the turbines that do have data and use those values as an estimation of the missing values. Linear interpolation is sensitive to the event duration, meaning the longer the missing data event, the less likely it is that a linear interpolation will follow the real values. Fleet median interpolation may result in unexpected results if there are too few assets in the fleet or too few assets with non-missing values at the time. On top of that, as indicated before, the latter method is dependent on the different assets being co-located and exposed to similar conditions, which is a condition that might be too stringent in particular contexts. In the interactive starter kit, we can analyse how both methods perform for missing data events of different durations. We can also change the number of turbines that are considered for the fleet median interpolation and see how that affects the accuracy of the prediction. In our dataset, there are 4 turbines available, so you can choose between 1 and 3 turbines to use for the fleet median interpolation. The red trace corresponds to actual observed values. This means that the closer the blue or the green line, representing the linear and fleet mean interpolation respectively, are to this red line, the better the interpolation is. In order to understand the pros and cons of the methods, we perform some experiments: First, we decrease the duration of the missing data from 30 to 10 minutes, corresponding to one missing data point in the signal. We see that the linear interpolation approximates the original signal fairly well. When increasing the duration of missing data to 100, on the other hand, the interpolation deviates significantly from the original signal. Thus, the longer the period of missing data points, the worse the linear interpolation. Similarly, we can change the number of turbines for calculating the fleet median to a lower number. With only one turbine taken into account, also here the quality of the imputation drops but still returns a better approximation than the linear interpolation, especially for longer durations of missing data. Note, that this is only the case because we know that the wind turbines are co-located. In the starterkit, you can further inspect how the two methods are performing in different scenarios. When linear interpolation or fleet-based data imputation techniques do not lead to a sufficiently good approximation to the original signal, we can still use pattern-based imputation. In the following we illustrate this method on two attributes, namely the time series for temperature and wind speed. Pattern-based interpolation performs well on signals that follow a predictable pattern, as is the case for signals that show a strong seasonal modulation. We can appreciate this by comparing the interpolation of the temperature and wind speed signals. The former, as we have seen before, follows a daily pattern, ergo night vs. day and a seasonal pattern hence, winter vs. summer and therewith also shows a high-quality pattern-based interpolation. The latter, - the wind speed - however, has a weaker seasonal modulation reflected in a less accurate interpolation. As the basis for this pattern-based interpolation, we can use seasonal ARIMA - an extension of the ARIMA model - for forecasting seasonal time-series data. More specifically, in our case we can use seasonal ARIMA for forecasting the evolution of the temperature based on the data coming from a single asset. Note how varying the duration of the missing data event affects the interpolation quality. You can also anticipate the start of the (synthetic) missing data event and see if that affects the prediction. Here, we first change the start of the missing data by 36 hours. The prediction fails to mimic the original values in the beginning. Indeed, this type of interpolation might be sensitive to the starting point for the forecasting. Here we used one year of data prior to the missing data event for the training of the ARIMA model. Using a longer period - including multiple seasonal patterns- nonetheless will improve the forecasting and avoid the aforementioned shortcomings. Furthermore, we analyse how strong the duration of missing data influences the quality of the interpolation. When increasing this duration to 24 hours, or put differently, when aiming to interpolate the data for a whole day, the pattern-based interpolation still returns fairly satisfactory results for the temperature values but does not predict the wind speed with sufficiently high quality. Therefore, keep in mind that it is important to understand the data before choosing an interpolation method.","tags":"Time Series Preprocessing","url":"/Time Series Preprocessing/2020-01-06-handling-missing-data.html","loc":"/Time Series Preprocessing/2020-01-06-handling-missing-data.html"},{"title":"Data Preprocessing","text":"Data Preprocessing Welcome to the fourth video of the tutorial for the AI Starter Kit on remaining useful lifetime prediction! In this video, we will explain you how the data needs to be preprocessed such that it can be served as input for a machine learning algorithm. Let's get back to the NASA Engine data and our use case. Before we can start with training a model to predict the remaining useful lifetime, a number of preparatory steps need to be taken. First of all, let us assign which type of problems we are dealing with. What precisely are we interested in? Is it about predicting a categorical value, which can take only a limited set of possible values, or a number? Indeed, it's not so easy to answer. It depends on how we define our use case: We can either ask: How long is the remaining useful lifetime? Then, a real number is looked for, and we deal with a so-called regression task. Alternatively, we can ask: Will the engine fail within the next 50 cycles? Then this boils down to a so-called binary classification task, in which the prediction is ‚Äòyes' or ‚Äòno'. The choice for one or the other task mainly depends on the business question you want to solve. In most cases, a binary classification problem is easier to handle for a Machine Learning algorithm and therefore, this is the question we will answer in this AI Starter Kit. However, the solution methodology is largely similar when you would opt for predicting the remaining useful lifetime using regression. In the previous video, we explained that for classification tasks, labelled data is essential. Therefore, as a first step, we need to create the binary labels: Has the machine failed within a given period of time or not? For the training data, we know that the final cycle per engine id is the time of failure. In order to determine the remaining number of cycles at each time point, first the maximum number of cycles per engine is determined. Subsequently, the current cycle number is subtracted from the maximal number of cycles to arrive at the number of cycles remaining at a particular time point. [Can we visualize this nicely on a screen on an example? √† Show a timeline next to the dataframe, annotated with max/number of cycles/labels] We will add this value as a new variable to the data. However, this is not yet a binary label that can be used as input for the classification model we want to train. This is created by determining whether the calculated remaining useful lifetime is smaller than or equal to the threshold N ‚Äì the period of time we want to predict the failure in. In this tutorial, we will use 30 cycles as a threshold, meaning that we aim to answer the question whether or not the engine will fail within the next 30 cycles at a particular point in time. This binary label can now be used as input value for the classification model. In order to decide on the goodness of the model, the test data needs the same type of label. In that case though, the final cycles per engine does not automatically determine the time of failure. How can we proceed instead? The ground truth data helps in this case. By joining the unlabelled test data and the ground truth data, we know the time of failure. Once done, we can analogously to the training data calculate the binary label for the test data. Don't forget, these labels will only be used for evaluating the model and are not shown to the algorithm beforehand. A second observation that can be made from the training data sample is that the scales of the values differ significantly across columns ‚Äì both for sensor values and machine settings. This difference in the scale of the numbers could cause problems when the model needs to calculate the similarity between different cycles - namely the rows in the table - during modeling. To address this problem, we will normalize the data. The goal of normalization is to change the values of numeric columns in the dataset to use a common scale, without distorting differences in the ranges of values or losing information. The normalized values maintain the general distribution and ratios in the source data, while keeping values within a scale applied across all numeric columns used in the model. One of the most popular normalization techniques is so-called Min-Max normalization and that is also what we will use here. It scales the values of a variable to a range between 0 and 1 using this formula where ùëã represents the value to be normalized, ùëãùëöùëñùëõ is the minimum value of the variable in that column and ùëãùëöùëéùë• is the maximum value for that variable We apply the normalization on both the training and test data set on all sensor measurement and setting variables. We will not rescale the engine id, as it should be seen as a categorical variable. Further, for the cycle variable we keep both the original and the scaled values. In the next video, we will go to the core of this tutorial, namely using a deep learning algorithm to train a model that is able to solve this binary classification problem.","tags":"Remaining Useful Life Prediction","url":"/Remaining Useful Life Prediction/2020-01-05-data-preprocessing.html","loc":"/Remaining Useful Life Prediction/2020-01-05-data-preprocessing.html"},{"title":"Outlier Detection","text":"Outlier Detection In the previous video we explored the data mainly in a visual manner. In this fourth video, we will discuss how to detect outliers in the data and how different methods can be used to improve the data quality. There are several different techniques for outlier detection in time-series. Here, we will focus on online outlier detection, that means the detection of an outlier as soon as it occurs, as opposed to an offline detection, which happens retrospectively. We present two different approaches for outlier detection using temperature and wind speed variables. A relatively simple and frequently used approach for outlier detection is based on the boxplot data distribution. For a given attribute, this method computes its interquartile range or IQR, which is the difference between the 25th and 75th percentiles. This value is then multiplied by a constant factor ùõº which determines how stringent the outlier detection is. A typical value for ùõº is 1.5, although this value can be adapted according to the level of stringiness desired. Indeed, larger values will push the outlier boundaries further and thereby reducing the number of detected outliers. The resulting value is subtracted from the 25th and added to the 75th percentiles to obtain the lower and upper fences, respectively, which define the thresholds beyond which a given value is labelled as an outlier. Considering the seasonal nature of the data, we should ensure that the outlier detection approach takes the impact of seasonality into account. It is known that the temperature has significant seasonal variation as it varies between day and night or between winter and summer for example and the same temperature in winter and in summer can be considered as outlier in one case, but not in the other. Therefore, the seasonal trend decomposition described in the former video is applied to the signal and the residuals are used as input for the outlier detection. In doing so, we only take the distance to the seasonal pattern into account for the outlier detection. In the example shown on the right, we identify outlier events based on a given ùõº value. We define outlier events as outliers that are consecutive in time. An additional input parameter allows to merge outlier events that are separated by less than a given amount of time. When using the most stringent alpha value - in our case ‚Äì we detect a single outlier in the dataset: namely one of the turbines measured a temperature of -273 degrees for a period of time. Further, we can visualize the time series around the time of a given outlier event. The Flank duration parameter allows you to control the time window around the outlier for the visualization. The left figure shows the time series with the outlier event highlighted in blue, while the figure on the right shows the distribution of all the temperature residual values using a boxplot. Again, the points in blue indicate the outlier event depicted on the left. When increasing the flank duration, we clearly see that this measurement is an outlier. Of course, to identify an outlier with a temperature of -273 degrees, you don't need data science. Hence, We also have a look at another outlier detected with a smaller value for alpha, namely 1.5. In this case, the outlier is not as evident as before. On the 23rd of August, the temperature was much higher than on a normal day in August and warmer than the days before and after. An important aspect we want to discuss in this respect is the influence of outlier detection and removal on normalization. Normalization is a typical data pre-processing step where the range of a variable is standardized, meaning rescaled in order to make different variables with different ranges comparable. It is an important pre-processing step before the data is presented to a machine learning algorithm, as it ensures all variables have equal importance. Different normalization approaches exist. Examples are rescaling the values in the 0-1 range, known as min-max normalization, or removing the mean and scaling to unit variance, known as z-score or standard score normalization. Most of these approaches are sensitive to outliers: For example, in the case of min-max normalization, the minimum value is mapped to 0 and the maximum to 1 so obviously extreme outliers will have a large impact. In the starterkit, we can test these two normalization approaches on each of the attributes of the dataset. We can enable or disable the outlier removal in order to appreciate how this affects the normalization procedure. This effect is most striking when looking at the temperature attribute. If we do not remove the outliers, the min-max normalization is meaningless as 0 is mapped onto -273 degrees. All remaining values are then in the range between 0.8 and 1. When removing the outliers, the range between 0 and 1 is equally dense and the normalization reflects the seasonality in the data. A second approach is the so-called Fleet-based outlier detection. For detecting outliers of the power attribute, we will use this alternative approach. Note, that this is only for the purpose of demonstration and that we could also use the interquartile range-base outlier detection for the power attribute. The approach we will use is based on exploiting the fleet aspect, which is exemplified by wind turbines, which typically operate as part of a wind park in the same environment under similar environmental conditions. At each point in time, we compute the median power recorded by the fleet and we consider any value that deviates too much from that median value to be an outlier. To determine what constitutes too much deviation, we again consider the boxplot outlier definition. If a value is beyond 5 times the IQR from the 25th or 75th percentile, we consider the observation to be an outlier. To exclude periods during which all the turbines in the fleet were not operational, we only consider time points when at least 3 of the 4 turbines recorded a power production above 0. The starter kit allows us to explore all the detected outliers in the power attribute using this definition. We can change the Flank parameter in order to see a larger time window around the outlier. It can be observed that the outliers often happen in periods of time when the power of a given turbine dropped to 0 without it being the case for the remaining turbines. There are, nonetheless, other instances, when the produced power of a given turbine was - statistically speaking ‚Äì above what would be expected given the behaviour of the remaining turbines. Note that the grey square highlights only the fleet outlier event in question. Other points in the visualization might also be labelled as outliers but they are not part of the same event. The fleet-based approach has the advantage of being able to capture outliers at a specific moment in time only relying on sensor values captured at that time. On the contrary, this approach can only be applied if the dataset contains a fleet of co-located assets, meaning that they are exposed to similar conditions. In the next video, we will discuss how we can impute missing data in the dataset and therewith improve the quality of the data.","tags":"Time Series Preprocessing","url":"/Time Series Preprocessing/2020-01-05-outlier-detection.html","loc":"/Time Series Preprocessing/2020-01-05-outlier-detection.html"},{"title":"Statistical Data Exploration and Feature Engineering","text":"Statistical Data Exploration and Feature Engineering In the former video, we performed a visual data exploration. We could already gain quite some insights from the figures we showed. In this video, we will concentrate stronger on statistics in order to verify our findings. Finally, we will prepare the datasets for modelling purposes by extracting a number of distinguishing features which will serve as input for the machine learning algorithm. In order to find repetitive patterns in time series data, we can use autocorrelation. It provides the correlation between the time series and a delayed copy of itself. In case of a perfect periodicity as shown in the figure, the autocorrelation equals 1 if we delay one copy by the periodicity. In order to investigate the autocorrelation in case of the global active power, we visualize the autocorrelation with time lags of 1 day, 2 days, and so on. Note that we do not show the results of a delay of 0 days, since the correlation with the exact same day would trivially be 1. For bigger delays, a clear peak occurs after 1 day, indicating the consumption of a day is highly correlated with the consumption of the previous day. And more obviously, several peaks occur every 7 days, which confirms the existence of a weekly pattern. That means that the consumption of a day is highly correlated with the consumption of the same day one or several weeks earlier. In the data exploration video, we further saw that the temperature and the global active power show an opposite trend. In order to verify the strength of this relationship, we can compute the correlation between these two variables. For this purpose, we compute Pearson's correlation. The Starter Kit allows to calculate the correlation between the temperature and the global active power for different resampling rates. Why is this important? One reason is the time scale for changes. The temperature typically changes less drastically over time than energy consumption does. In the figure, on the diagonal, we see the correlation of the two time series with each other, resulting in a perfect correlation of 1 as expected. On the opposite, along the antidiagonal, we see the correlation between the global active power and the outside temperature. In case of a sampling rate of 1 hour, the negative correlation is weak with roughly -0.2. With an increased resampling rate of 1 week, the negative correlation is more evident with a value of -0.75. This confirms our insight from the visual inspection. With a larger sampling rate, the short-term fluctuations ‚Äì for example from your washing machine ‚Äì are smoothed out and leaves us with the seasonal pattern. From these insights, we can now start extracting features from the extended dataset that we can use later on for the electricity consumption forecasting. A feature is an individual measurable property of the phenomenon being observed. Choosing informative, discriminating, and independent features is a crucial step for building effective machine learning algorithms. Based on the previous visual and statistical observations, we will consider the following features to predict the energy consumption for the hour ahead: energy consumption of the day before because we saw that there was a peak in the autocorrelation with a delay of one day; energy consumption of the 7 days before due to the weekly pattern; the hour of the day, which will allow to distinguish between different periods of the day (i.e., night, morning, afternoon, evening); the day of the week, which will allow to distinguish between weekdays and weekend days whether the day is a holiday or not; the month of the year, which will allow to distinguish between yearly seasons; and finally, the temperature. That results in the final dataset that will be used as input for the machine learning algorithm to learn the model. Before discussing the modelling step in more detail, in the next video, we will provide a theoretical overview of the approaches chosen for the electricity consumption forecast. Additional information The video material in this website was developed in the context of the SKAIDive project , financially supported by the European Social Fund , the European Union and Flanders. For more information, please contact us at elucidatalab@sirris.be","tags":"Resource demand Forecasting","url":"/Resource demand Forecasting/2020-01-05-statistical-data-exploration-and-feature-engineering.html","loc":"/Resource demand Forecasting/2020-01-05-statistical-data-exploration-and-feature-engineering.html"},{"title":"Deep Learning Theory","text":"Deep Learning Theory Data Science, Machine Learning, Deep Learning, Artificial intelligence‚Ä¶ These words are often used interchangeably in different contexts and not always precisely. Therefore, before we continue with our particular use case, we will provide you with a brief introduction into the difference between \"traditional\" machine learning methods and Deep Learning-based techniques. Subsequently we will introduce you to the main neural network types that are used in deep learning, and explain you we selected the appropriate type for use in the context of remaining useful lifetime prediction. When starting on a data science problem, you usually need to start with finding the answer to a number of questions before starting the hands-on work on the data. Without understanding the use case, even the best data science model will not help. In most cases, a particular business problem can mapped onto one of the following data science tasks. A first question to answer in that respect is which type of outcome is expected from the use case owner. Is the aim to predict a category, such as ‚Äònormal', ‚Äòdegrading' or ‚Äòfailed'? If yes, the next question to answer is whether labelled data is available or not. Labelled data is data for which examples are available that are annotated by a domain expert with one the classes to predict. Put differently, for each data point or set of data points, a class is defined. Usually, the number of unique classes is rather small. This data will be used by the algorithm for training a model. Once trained, it can be evaluated on a test data set, for which the classes are known but will not be visible to the model. Evaluating the ratio of correctly predicted classes gives a measure of the quality of the model. Often used algorithms for classification are k-Nearest Neighbors, Decision Trees or Support Vector Machines. But what can we do if we don't have information available on possible classes? In that case, introducing a similarity between the examples that you have available makes it possible to cluster different data points into groups. These groups can then be used to gain deeper insights into the data, and in some cases can be mapped to particular classes. Partition-base clustering, hierarchical clustering or density-based clustering are often used techniques for this purpose. The situation is different in case a numerical value should be predicted. It is similar to the classification task, but the prediction range is continuous. For these so-called regression tasks, Ordinary Least Squares or Support Vector Regression are often used. If the goal is neither to predict a class nor a numerical value, but rather a future state, one typically turns to graphical modelling algorithms order to predict these states. These techniques also allow one to include the available background knowledge into the modelling process. Examples of graphical modelling techniques are Hidden Markov Models and Bayesian Networks. To make the difference between the single categories a bit clearer, we discuss some examples: In case no labelled data is available, clustering of data points can provide insights in different modes in the data. An example is a performance benchmarking of industrial assets. Let's assume the data to analyze comes from a wind turbine park. When looking at several measurements, like for example of the power curve, the wind speed, and the wind direction, we can identify different modes in which the single wind turbines are operating. In contrast, assume that we are interested in the expected power production of a particular wind turbine in the following days for which we have a weather forecast. We can use this information as input variables for a regression model and therewith predict the power production. If labels are attached to the gathered data, for example on the root cause of particular sensor failures, a classification algorithm can be used to train a model that is able to determine which fault is associated with a certain set of sensor readings. When next to the data also background knowledge from a domain perspective is available, graphical models can for example be used to diagnose the reason behind a particular fault in industrial assets such as wind turbines. How do the above examples differ from Deep Learning and when does it make sense to use the one or the other? One of the major constraints in traditional machine learning is given by the fact that domain knowledge is usually needed in order to perform proper feature extraction. Consequently, feature engineering is typically a manual and time-consuming task, requiring the necessary domain knowledge. There is hardly any case in which a model can be applied to the data as it is and receive good results. In a number of cases, however, the domain knowledge on a particular problem setting is limited or the machine or process generating the data is highly complex, making it hard for an expert to manually come up with useful characteristics that can be used as features for a machine learning model. This is where deep learning comes into play. In deep learning, the feature extraction and selection step is basically performed by the algorithm itself. The technique is inspired by the neural networks in human brains. It should learn by itself how to combine the different neurons in the single layers in order to obtain the best results. The learning is performed by the adjustment of the weights between the neurons in single layers. In the figure, these weights are symbolized by the arrows going from one neuron in one layer to another neuron in the next layer. The final result is given by the output in the output layer, indicated in green in the figure. The textbook example is the classification of images of animals, to discern cats from dogs. Nowadays there is a whole spectrum of different model architectures and flavours of neurons depending on the type of task that is supposed to be solved. The choice for a particular type of network typically depends on the problem you want to solve, the characteristics of your input data and the availability of particular domain knowledge. All these factors influence amongst others the complexity of the network, its interpretability and the learning speed. One of the simplest models is a so-called feedforward neural network as shown in the figure. As the model learns by adjusting the weights between the layers, the weights can be seen as its memory. While such models perform well for quite a number of problems, one downside is that their memory is rather static, due to which they have a hard time in ‚Äòremembering' patterns that happened a long time ago. Suppose that you need to learn a model to predict the sales of Christmas trees. It might be clear that hardly any Christmas tree will be sold in summer, and the main indicator for the sales this year will be the sales figures of the previous years in the month before Christmas. Remembering this long-term information is not easy for a feedforward neural network. One way to overcome this problem is the usage of so-called Long Short-Term Memory - or LSTM in short - networks. As the name already indicates, LSTM networks are able to adapt their memory in a more precise way than feedforward neural networks, and in this way are better able in deciding which information they need to remember for a longer period of time. LSTM networks are often used to solve tasks such as unsegmented, connected handwriting recognition, speech recognition and anomaly detection in network traffic or intrusion detection systems. Also in the field of remaining useful lifetime, the implementation of LSTM networks is reasonable as we deal with data for which the change over time of several variables is decisive. Our interactive starterkit offers the possibility to build your own LSTM model. It allows to gain a deeper understanding of the different parameters and examine how they influence the prediction result.","tags":"Remaining Useful Life Prediction","url":"/Remaining Useful Life Prediction/2020-01-04-deep-learning-theory.html","loc":"/Remaining Useful Life Prediction/2020-01-04-deep-learning-theory.html"},{"title":"Resampling, Smoothing and Seasonal Patterns","text":"Resampling, Smoothing and Seasonal Patterns Welcome to the third video of the tutorial for the AI Starter Kit on time-series pre-processing! In the previous video, we already had a look at the temperature values and saw a clear seasonal pattern. For active power and wind speed however, it's not so easy to answer if particular patterns are present because the data is noisier. In general, visualizing time-series data with a high temporal granularity might make it difficult to interpret and detect underlying patterns. Resampling techniques allow to reduce the temporal granularity, thereby revealing longer-term trends and hiding sharp, fast fluctuations in the signal. In the interactive starter kit we can select the amount of resampling by specifying the time unit and the number of time units to resample to. For example, we can resample the data to weekly values. For this, we set the time unit to \"Week\" and the number of time units to 1. Playing with these two inputs, we see how the level of detail of the time series changes and how that influences the insights that we can derive from it. In a first step, we select a small resampling factor, for example a daily resampling. We observe that the visualization of the temperature shows a seasonal pattern in the data, even though strong stochastic fluctuations dominate the overall picture. At this level of granularity, the visualization of the wind speed is not at all interpretable due to these fluctuations, especially when visualizing over such a long period of time. Specifically, if we are interested to visualize the long-term trend of the time series, we need to reduce the sampling rate. To this end, we need to increase the resampling rate to a weekly resampling. We then see how these high frequency patterns disappear and it becomes possible to analyse some long-term fluctuations in the power and wind speed variables. You can notably observe the strict correlation between the patterns of wind speed and active power, which might be obvious: the more wind, the higher the power that can be generated by the turbine. Note also that by increasing the resampling of the time series too much, most of the information it contains is discarded. It is also important to understand that resampling aggregates the data for the specified period. Hence, we need to specify how we want the data to be aggregated. For the wind speed and temperature data in our example, we can opt to aggregate the data using the median of the values within the selected period. Like this, we level out very high or very low values. Therefore, this statistic is quite robust against outliers that could be present in our dataset. For other quantities, we might consider other statistics. For example, if we were considering power production, the interesting value is the power produced per day or week. Consequently, it makes more sense to sum all the data samples rather than to average them. Does the sum function also make sense for the temperature or wind speed? Think about it and then try different resamplings by changing the aggregation function or resampling units used. As we can see, a simple resampling makes additional details explicit, and the degree of resampling allows drawing different insights. For example, with a weekly sampling rate the temperature plot still shows a clear seasonal pattern. However, we can also see that in each year the weekly temperature evolves in a different manner: the highest and lowest temperatures are located in different weeks. Further, the wind speed also seems to follow a seasonal pattern, albeit a less explicit one. We can notice that the wind speed is generally higher in winter, yet its evolution is much more dynamic than the one of temperature. Finally, the power production closely follows the wind speed profile, consistent with the latter being its main driver. Important to keep in mind when resampling your own data is that it can help to find seasonal patterns but that it can also hide patterns when the scale is too big. To make this more tangible, imagine that you resample the data to an annual scale. In our example, you will not see the seasonality anymore. When you perform resampling on your own data a domain expert typically can support you to find a good scale by reasoning about the underlying phenomenon from which the data originates. In many data sources, stochastic fluctuations can be seen on different temporal scales. Some of those might be small variations that may not be significant for what you want to detect. These types of fluctuations can be real, such as, due to sudden decreases in temperatures between summer nights, but can also be caused by inaccurate measurements of the sensor, for example. If such details are not important to consider for your analysis, you can remove them by smoothing. In our interactive Starter Kit, we explore three different smoothing approaches, namely the rolling window algorithm, Gaussian smoothing and Savgol filters. The easiest approach is the rolling window algorithm: The user defines a small, fixed period ‚Äì the window. The algorithm runs over the data taking into account the consecutive time points covered within the window, and replaces each time point by an aggregated value computed within this window. Typically, the aggregation follows the mean or the median value. This has the effect of reducing short-term fluctuations while preserving longer-term trends. When using Gaussian smoothing, the method summarizes the values over a sliding window, just as the rolling window algorithm. But instead of calculating the mean or median value, a Gaussian function is used. The size of the sliding window is specified by the standard deviation of this Gaussian function, which is called a kernel. With this method, the values closest to the centre of the sliding window will have a stronger impact on the smoothing process. The impact of the remaining values is a function of the distance to the window centre. Finally, the Savgol filter is a popular filter in signal processing and is based on a convolution approach. It fits a low-degree polynomial to successive subsets of adjacent data points via the linear least-squares method. Note that also here a window is defined. In our Starter Kit, we use a polynomial of degree 2. Now that we know the general idea of smoothing, We can experiment with these different approaches in the interactive starter kit. At the top of the graph, we can select one of the explained methods. Using the time unit controls, we can explore how the level of detail resulting from the smoothing varies for different window sizes. Can you strike a good balance between too much and too little detail? This is a difficult question to answer, as it strictly depends on the goal of the analysis. Smoothing using a large time window, for example, several days, can be useful to detect slow, long-term changes to the time series. On the other hand, a shorter time window, let's say 12 hours, can be very efficient to analyse short-lasting changes in the signal by getting rid of fast transients in the signal. There are also some significant differences in the presented methods. If you chose the rolling median method and zoom in, you will be able to see that the signal appears 'shifted'. This is due to the fact that the rolling window implementation of the Python pandas library that was used, uses a look-ahead option, where the value at any given point in time is given by a time window ahead of it. Another interesting aspect is how the Gaussian method creates a signal that is characterized by smooth 'Gaussian-like' hills, a consequence of its Gaussian kernel-based nature. In sum, the Savgol and rolling window approaches create a signal that more closely follows the original signal. This is due to the fact that both operate using a local estimation approach: the latter method aggregates values over a local time window, while the former applies a least-squares regression over the specified time window. We previously discussed the seasonal patterns that can be observed in certain variables such as temperature. One technique that allows analysing the seasonal patterns in a time-series signal and to decompose the signal into different components is called Seasonal Trend Decomposition. This technique identifies cyclical patterns in the signal and decompose it into 3 components: First, the trend, which summarizes the long-term trend of the time-series in the considered time frame. A second component is the seasonal component: this is the part of the signal that can be attributed to repeating patterns in the signal. Finally, the residuals are the \"left-overs\" after subtracting the trend and the seasonal factors from the original signal. This is the component of the time-series that cannot be attributed to either the long-term trend evolution of the signal or the seasonal patterns. In the starter kit, we can analyse the seasonal trend decomposition over two periods: a seasonal period that covers the four yearly seasons and a daily period. If we select a seasonal decomposition over a sufficiently large period, we can clearly observe the yearly temperature pattern, with higher temperatures in the summer months and lower ones in the winter months. If you test a daily decomposition over the same period of time you will need to zoom-in on a short-time range and similarly see a pattern in the seasonal component, namely the 24-hour temperature cycle. You can also test how the seasonal trend decomposition performs on a less cyclical signal, such as power production. Indeed, since power production is mainly driven by the amount of wind and wind speed, each showing a weak seasonal modulation, power production can only be poorly expressed in terms of its seasonal component. In the next video, we will discuss how outliers can be detected in seasonal data with stochastic fluctuations.","tags":"Time Series Preprocessing","url":"/Time Series Preprocessing/2020-01-04-resampling-smoothing-and-seasonal-patterns.html","loc":"/Time Series Preprocessing/2020-01-04-resampling-smoothing-and-seasonal-patterns.html"},{"title":"Visual Data Exploration","text":"Visual Data Exploration Welcome to the third video of the tutorial for the AI Starter Kit on resource demand forecasting! Let us now explore the extended dataset in order to understand which factors influence the household energy consumption. Based on this exploration, we will later on identify which features are useful for training a machine learning model to forecast energy consumption. Time plots are a common way for exploring time series data, such as the energy consumption that we consider here. We explore time plots in different time frames in order to highlight different seasonal patterns in the data. We start with a yearly plot, which allows us to identify global patterns. Then, a monthly plot allows us to zoom in and understand relationships between energy consumption in different weeks and days along the year. And finally, a weekly plot allows us to zoom in even further and identify daily and hourly patterns. We start with the yearly plot. The data ranges from January 2007 to end of 2010. We can easily see the yearly pattern for the global active power with peaks in winter and valleys in summer. With these settings though, we see the temperature values only as a flat line because they are about two magnitudes smaller. When only displaying the temperature, we see a similar pattern with high values in summer and low values in winter, as expected. In order to make the two-time series comparable, we normalize the data, such that the maximal value of each time series corresponds to 1 and the lowest one corresponds to zero. Like this, we can see that the two show an opposite evolution over the year. For the monthly data, we progress analogously. While we can again clearly see the annual effect, there is no clear pattern recurring every month. One interesting thing that can be observed is that strong dips in power consumption tend to happen at the same time in February and November in 2007 and 2008. Most likely, these are holidays which tend to happen at the same time for the household under investigation. This can be best seen when the sampling rate is changed. Instead of showing the data per day, we can have a look at the rolling mean of the active power over three days to see the pattern more clearly. We further explore the evolution of the energy consumption in a shorter time frame. We want to verify whether during a week, the energy consumption exhibits the same patterns. In the following plot, the user can inspect and compare the energy consumption in December of different years. We mark weekends in yellow and Christmas day in orange. What can you notice when you compare weekdays and weekends? And how about the holidays? Exactly, the mean consumption is higher on the weekends. The reason might be that more members of the household are at home during the weekends. Comparing the week around Christmas 2008 and 2009 shows interesting patterns. While the household was obviously at home and maybe even with guests in 2009, it looks like they were not around in 2008 for several days. Finally, we analyze the daily pattern. For this, rather than showing the resampled dataset, we use the original dataset, which provides consumption with a sampling rate of 1 minute and which can show more detailed patterns. Note that this means we are using the original units of kW. In the figure, we see how the power consumption follows a daily recurring pattern, with peaks in the morning and evening and troughs at night and noon ‚Äì on weekdays. The 5th of December was a Saturday, and we can clearly see how the curve of that day deviates from the weekdays. During the night, we still see a low and more or less constant power usage, most likely due to appliances like refrigerators that run continuously. In the next video, we will discuss the statistical significance of the patterns detected and prepare the data for machine learning modelling by extracting meaningful characteristics or features. Additional information The video material in this website was developed in the context of the SKAIDive project , financially supported by the European Social Fund , the European Union and Flanders. For more information, please contact us at elucidatalab@sirris.be","tags":"Resource demand Forecasting","url":"/Resource demand Forecasting/2020-01-04-visual-data-exploration.html","loc":"/Resource demand Forecasting/2020-01-04-visual-data-exploration.html"},{"title":"Data Preprocessing","text":"Data Preprocessing Welcome to the second video of the tutorial for the AI Starter Kit on resource demand forecasting! In this video, we will detail the dataset that we will use and perform the necessary preprocessing steps in order to prepare the data for further analysis. In this AI Starter Kit, we will work with a publicly available energy consumption dataset from the UCI repository. This dataset contains measurements of electric power consumption in one household, located near Paris, with a one-minute sampling rate over a period of almost 4 years. Different electrical quantities and sub-metering values are available, as you can see in the table. Let us first have a look at the single variables given in the table: First of all, and most importantly, the global active and reactive powers are given. The active power specifies the amount of energy the single electric receivers transform into mechanical work and heat. This useful effect is called 'active energy'. On the contrary, several receivers, such as motors, transformer, or inductive cookers need magnetic fields in order to work. This amount is called reactive power as these receivers are generally inductive: This means, they absorb energy from the network in order to create magnetic fields and return it while they disappear. This results in an additional electrical consumption that is not useful for the receivers. Further, the voltage and current intensity are given in Volts and Ampere, respectively. Finally, the submeterings provide us a more detailed insights where the power is consumed. Submetering 1 corresponds to the kitchen, containing mainly a dishwasher, an oven, and a microwave. Submetering 2 corresponds to the laundry room, containing a washing-machine, a tumble-drier, a refrigerator and a light. Lastly, submetering 3 corresponds to an electric water-heater and an air-conditioner. We expect, that the different submeterings obey different patterns: While an oven mainly consumes power during lunch and dinner time, a refrigerator has a more or less constant consumption during the entire day. As mentioned before, we will also take weather measurements into account. For this, we use measurements recorded close to the city of Paris, where the household is located, provided by the Wunderground website. It contains information on climate conditions on a 30-minutes base. The dataset contains the outside temperature and the dew point in degrees Celsius, the air humidity in percentage, the pressure at sea level in hPa, the visibility in km and the wind direction expressed in degrees. The weather data is measured by a set of connected sensors. For these, it is rare that the data availability is uninterrupted for such a long period. For this reason, we first check the general statistics for the data set. For most variables, the minimal value is -9999, which indicates missing values as also specified in the documentation of the data. We will replace these with so-called 'Not a Number' values or NaNs such that they will not interfere too much during data modelling. In a next step, we fuse the electricity data and the weather data. Note, that the temporal resolution of the two is not the same ‚Äì the power data is measured every minute while the weather information is available only every 30 minutes. The integration of these two datasets thus requires careful consideration. Two options are possible: we can either upsample the weather data or downsample the household data. That means that we either linearly interpolate the information from weather data on a one-minute interval or aggregate the power data on bigger intervals. In this use case, the latter is more reasonable, as otherwise, we would need to find a way to impute missing data in the climate dataset. Let us first analyze the effect of downsampling the power data. Therefore, we calculate the rolling mean values of the original power over a given time window. You can choose between a sampling rate of 30 or 60 minutes, or 4 hours. The light blue time series shows the original data, the dark blue one the downsampled one. In all cases, the data looks smoother as the original data, as taking the mean value smooths out the major short-time power peaks but at the same time results in some loss of information. With the larger window size of 60 minutes this effect is stronger than for the smaller one. A window size of 4 hours arguably removes too much variation. Note that the weather data is available every 30 minutes such that in case of an hourly time window for the power data also the weather data has to be resampled. Since the information loss for the 60-minute interval is reasonable, we decide to proceed with this sampling rate. This also allows us to reduce the size of the dataset, which will reduce the computation time required for the analysis. In order to merge the two data sets, we transform the active power from a minutes-based power measurement given in kW to an hourly consumption given in Wh. Therefore, we sum the power over one hour and divide it by 60 to get the actual energy. By additionally multiplying by a factor of 1000, we change units from kWh to Wh. For the weather data, we proceed similarly by taking the mean temperature over one hour. With this, we can join the two data sets. For future analysis, we will only take the global consumption and temperature into account. Now that we have preprocessed the dataset, we will illustrate how to gain deeper insights in the data through both visual and statistical data exploration. In the next video, we will start with the visual exploration by means of time plots. Additional information The video material in this website was developed in the context of the SKAIDive project , financially supported by the European Social Fund , the European Union and Flanders. For more information, please contact us at elucidatalab@sirris.be","tags":"Resource demand Forecasting","url":"/Resource demand Forecasting/2020-01-03-data-preprocessing.html","loc":"/Resource demand Forecasting/2020-01-03-data-preprocessing.html"},{"title":"Data Understanding","text":"Data Understanding Welcome to the second video of the tutorial for the AI Starter Kit on time-series pre-processing! In this video, we will detail the dataset that we will use and perform an initial data exploration. The time series dataset that we study in this Starter Kit is generated by the SCADA system of a set of wind turbines. SCADA stands for supervisory control and data acquisition. In modern turbines, such a data acquisition system can easily contain more than 100 sensors that keep track of amongst others temperature, pressure data, electrical quantities like currents and voltages, and vibrations. It is commonly used for performance monitoring and condition-based maintenance. The dataset originates from 4 wind turbines located in the Northeast of France. It spans a period of 4 years with a sampling rate of 10 minutes. Although the original SCADA system records statistics for more than 30 sensors, we will focus our attention only on wind speed, wind direction, temperature as well as the power that is generated by each of these turbines to illustrate a variety of time series pre-processing techniques. The table on the right-hand side shows an excerpt of the data, with the following attributes: * The column Date_time with the timestamp of the measurement, in 10-minute increments * the identifier of the turbine * Power, which is the active power measurement in kW as effectively produced by the turbine. * the outside temperature measurement in degrees Celsius * the wind speed measurement in meters per second And finally * the wind direction measurement in degrees The top rows show the average values for each turbine for the defined range. Note that in the case of wind direction, this is the circular average, which takes into account the circular nature of the data. That means that values are bound between 0 and 360 degrees, in which 0 degrees is identical to 360 degrees. In our interactive Starter Kit, you can define a range of values for a given attribute and see how the values in the remaining attributes change. For example, we can increase the wind speed to more than 17m/sec and see how that affects power production. Can you already spot any unexpected values in the dataset? Indeed, in the marked area in the figure, the values are significantly below those seen at other times. You can experiment yourself how the other variables influence the active power in the interactive Starter Kit. Now that we know which variables were measured, let's check some statistics. From the table we learn that in total 4 years of data are available, namely from January 2013 to the end of 2016. Further, we see that the number of data points per wind turbine differs. Most data points are available for the first turbine in the column, while the second turbine collected roughly 1000 data points less. This indicates already that for this latter turbine some data points are missing. In the interactive Starter Kit, we can plot the values of a given variable in time to see how their long-term trend looks like. In the graph shown at the right, we adjusted the timeframe by selecting it directly in the graph, such that also shorter-term fluctuations become visible. By clicking on the turbine names at the right-hand side of the graph, we can select a subset of the turbines. In the drop-down menu at the top of the graph, the variable to plot can be selected. What can we learn from this visual exploration? Let's first have a look at the temperature. Here we can easily spot the seasonal pattern due to summer and winter. Is this behaviour similar for the active power? Is there something unusual you can observe in the temperature values? We will come back to the answers to these questions later on in this Starter Kit and introduce you to a number of techniques to automatically detect this. In the next video, we will first concentrate on the smoothening and so-called resampling of the data points. See you there.","tags":"Time Series Preprocessing","url":"/Time Series Preprocessing/2020-01-03-data-understanding.html","loc":"/Time Series Preprocessing/2020-01-03-data-understanding.html"},{"title":"Data Understanding","text":"Data Understanding Welcome to the second video of the tutorial for the AI Starter Kit on remaining useful lifetime prediction! In this video, we will detail the dataset that will use and perform an initial data exploration to extract some first insights. In this AI Starter Kit, we will work with a publicly-available dataset from NASA. The data simulates run-to-failure data from aircraft engines. These engines are assumed to start with varying degrees of wear and manufacturing variation - but this information is unknown to the user. Furthermore, in this simulated data, the engines are assumed to be operating normally at the beginning and start to degrade at some point during operation. The degradation progresses and grows in magnitude with time. When a predefined threshold is reached, the engine is considered unsafe for further operation. In other words, the last operational cycle of the engine can be considered as the failure point of the corresponding engine ‚Äì meaning that the remaining useful lifetime has decreased to zero. Before we can start with learning an actual machine learning model, it is crucial to understand the data itself. The data set consists of multiple time series with the \"cycle\" variable as time unit. For each engine, identified by the variable \"id\", a different number of cycles is captured as not all engines fail at the same time. Per cycle, the following information in gathered: On the one hand, 21 sensor readings given by the data points s1 to s21. On the other hand, additional information about the machine settings, given by setting1 to setting3. In machine learning experiments, a dataset is often split in a training set and a test set. This split allows one to quickly evaluate the performance of an algorithm. The training dataset is used to prepare a model, to train it. For evaluation, the test dataset can be understood as new data that is presented to the algorithm. It was not seen by the algorithm before and therefore the outcome is unknown. In our example, it is data from different engines for which it is unclear when they are going to fail, or put differently, what their remaining useful lifetime is. For the purpose of evaluation, the information about the actual failure of the test data set is collected in the so-called ground truth data. This information will not be visible to the algorithm but will only be used for calculating the quality of the model. Now let's have a look at the single sensor measurements. We see that the value range of the single measurements are quite different, without knowing in detail what they correspond to in a sense of physical measurement. In the graph, we see the first 50 entries of time series data collected from three different sensor channels for engine 18. All three show some fluctuations but no clear deviation from a mean value indicated by the gray dotted lines, that could be a sign of degradation in engine performance, are visible. With increasing observation time, all three time series deviate more or less strongly from the mean values observed in the first 50 data points given by the gray horizontal line, indicating the start of the degradation process of the engine. For different engines, the deviation starts at different times. For engine 18, the deviation starts approximately at time 100. For engine 31 though, hardly any deviation is visible in the same time range. Only when increasing the time range, the clear deviation becomes evident. The different starting points of degradation for the single engines indicate that the simulations are made for engines with different wear. In the next video, we will go into more detail into the data preprocessing phase, explaining how the data needs to be prepared such that it can be served as input for a machine learning learning algorithm. If you are not familiar with deep learning, we recommend you to first watch our introductory video on this topic, in which we discuss the difference between ‚Äòtraditional' Machine Learning algorithms and Deep Learning techniques. We will also provide a brief introduction to the different type of neural networks, amongst others the so-called Long Short-Term Memory networks or LSTMs for short, which is the type of network that we will use to solve this problem.","tags":"Remaining Useful Life Prediction","url":"/Remaining Useful Life Prediction/2020-01-03-data-understanding.html","loc":"/Remaining Useful Life Prediction/2020-01-03-data-understanding.html"},{"title":"Introduction","text":"Introduction Welcome to the tutorial for the AI Starter Kit on time-series pre-processing. In this first video we will provide an overview of the actual challenge we're tackling. We will introduce the most important problems occurring in time series data and explain why it is beneficial to first improve your data before you start any other data-modelling tasks. Time series are characterised as a collection of data points obtained at successive times, most often with equal intervals between them. Since an increasing amount of assets is instrumented with sensors, this type of data is omnipresent, for example when monitoring industrial machinery or resulting from wearables tracking one's health state. Depending on the application domain, time series data is exploited for various purposes. It is used, amongst others, for profiling energy consumption of households predicting imminent failures of a production line, estimating the remaining useful lifetime of a machine, and many more industrial use cases. Typically, time series data cannot be used easily as-is by machine learning algorithms. This can be for example due to data quality issues such as missing values and sensor misreadings. Or because some algorithms are not fit to deal with the continuous nature of this type of data or the associated - often high ‚Äì frequency with which it is gathered. To illustrate the various methods and techniques in this Starter Kit, we will use a publicly available real-world dataset that consists of sensor data measurements from wind turbines. It exhibits typical characteristics of industrial time series datasets as it is very noisy and contains outliers and missing values. Moreover, it exhibits seasonal patterns across multiple years, as the machine behaviour is influenced by the meteorological conditions. We will use this dataset to illustrate * how resampling and smoothing can be applied to gain a better understanding of the behaviour of the time series, * how the quality of the data can be improved through normalization and outlier detection, and * how missing data can be imputed in various ways. In the next video, we will perform some initial exploration on the dataset in order to gain some basic understanding of the data.","tags":"Time Series Preprocessing","url":"/Time Series Preprocessing/2020-01-02-introduction.html","loc":"/Time Series Preprocessing/2020-01-02-introduction.html"},{"title":"Introduction","text":"Introduction Welcome to the tutorial for the AI Starter Kit on remaining useful lifetime prediction! In this first video we will provide an overview of the actual business case we're tackling. We will introduce the most important concepts and explain why it is beneficial for maintenance to know the remaining useful lifetime of a machine, a tool or any other industrial asset. Estimating the remaining useful lifetime of an asset can be beneficial for several purposes in a variety of industrial contexts. For one, it can offer support in the better scheduling of maintenance operations, for example for offshore wind turbines. For such assets, maintenance can only be performed under the right weather conditions. But also for other assets, maintenance is an important part of its lifecycle. Traditionally, maintenance is done in a corrective way, such that it is performed at the moment an asset has failed. In that case, the failing part is identified and rectified or replaced, to ensure that the asset can subsequently resume normal operation. Especially for strongly interdependent production lines or industrial assets operating in critical environments, an unplanned downtime is to be avoided and often costly. For this reason, more recently, for an increasing number of assets preventive maintenance is performed. In that case, maintenance tasks are scheduled at regular intervals, avoiding future asset failure to a maximum extent ‚Äì but with the risk to replace assets that are still working correctly and still could for a while. Hence, the optimal time for replacement is wanted: The time when the asset still works correctly but will probably fail soon. And this is where the remaining useful lifetime comes into play. Nowadays, ever more assets are equipped with different types of sensors gathering data. This started the trend towards predictive maintenance, in which the maintenance moments are decided upon in a data-driven way. Therewith, maintenance is only performed when actually needed. This is a very broad domain which encompasses a variety of topics. Besides the estimation of the asset's remaining useful lifetime ‚Äì the main topic of this AI starter kit ‚Äì further related problems to solve for a fully predictive maintenance approach are failure prediction, detection, and diagnosis for root cause analysis. The remaining useful lifetime of industrial assets is defined as an estimate of the remaining time that an item, component, or system is estimated to function as expected. It is expressed as the number of hours, cycles, batches or any other quantity. In the figure we see datapoints collected from an arbitrary sensor in green. Let's say it measures the tool radius of a milling machine. With increasing time, the radius decreases as the tool wears out. At a certain point, the tool is considered too worn out to still further use it in production, which can influence the quality of the produced parts or lead to an unstable production process and consequent damage to the machine. The main questions thus is for how long the tool will it still be able to function properly? By applying Machine Learning algorithms, the continuation of this time series data can be forecasted, denoted in pink. From this forecast, the remaining useful lifetime can be estimated for a given asset. In this case, the machine should be maintained within the next 10 to 15 cycles. With this information, maintenance tasks can be scheduled accordingly ‚Äì with a tool still functioning and without unexpected downtime. Hence, by estimating the remaining useful lifetime, we minimize the risk of failure and maintenance cost. An accurate prediction of an asset's lifetime can also help to optimise the operational efficiency of an asset, by more optimally planning the use of that asset before its end of life, or adapting its operational use to extend its useful lifetime. In manufacturing settings, where a downtime of the production line results in several operational problems and associated costs, remaining useful lifetime prediction can avoid unplanned downtime. In this AI Starter Kit, we will show its use to avoid safety-critical situations, by illustrating the solution methodology for predicting the remaining useful lifetime of aircrafts engines. To summarize, by applying predictive maintenance it is possible to: - better schedule maintenance operations - optimize operational efficiency - avoid unplanned downtime - anticipate and avoid safety-critical situations. This analysis can be performed for various machines, tools or processes as long as this degradation over time can be measured accordingly, just like resistance, length, temperature or similar measures. Predicting the remaining useful lifetime is typically very challenging for several reasons: First of all, usually a multitude of heterogenous sensor data is measured at several places in the machine. This data is often captured at a high-frequency, consisting of vibration data, acoustic emission, accelerometer data, and many more machine-internal parameters. Secondly, in some settings, also characteristics of the surrounding environment influencing the lifetime of the asset are captured. Further, typically, data that is gathered over a long period of time needs to be available to train the model. Given that industrial assets are often very complex ‚Äì as they consist of both electrical and mechanical components - it generally is a non-trivial exercise to predict their end of life. On top of that, the data typically comes with a lot of variation, due to varying machine types operating in heterogenous operating conditions with different machine configurations. Therefore, the main challenge in this process is to extract meaningful characteristics that can be used to predict the end of life from the gathered data. This is complicated by the fact that usually very little domain expertise on the operating conditions of these assets is available. Additionally, the environment in which they operate is typically very dynamic. In this AI Starter Kit, we will guide you through a data-driven methodology based on deep learning to tackle this challenge. In the next video, we will concentrate on the data itself that we selected to illustrate the approach. We will explain which information is available and what we can learn from it.","tags":"Remaining Useful Life Prediction","url":"/Remaining Useful Life Prediction/2020-01-02-introduction.html","loc":"/Remaining Useful Life Prediction/2020-01-02-introduction.html"},{"title":"Introduction","text":"Introduction Welcome to the tutorial for the AI Starter Kit on resource demand forecasting. In this first video we will provide an overview of the actual business case we are tackling. We will introduce the most important concepts and explain why it is beneficial for resource planning to know the expected demand in the future. Resource demand forecasting concerns accurately predicting the future need for a resource, typically using historical information. It is one of the most essential steps in resource demand management, which tries to ensure that sufficient resources are available to satisfy a fluctuating demand. Resource demand forecasting allows one to plan ahead and guarantee that sufficient resources are available when needed and to avoid costly countermeasures in case of shortage. Precisely forecasting the demand of a given resource can be beneficial for several purposes in a wide range of industrial contexts. When knowing the future demand, only the number of resources actually required need to be allocated to a specific task or process. The field ranges from forecasting the number of products that need to be manufactured based on historical sales figures, over predicting the parking demand or traffic density in a particular neighbourhood in order to control traffic in more intelligent ways, to estimating the amount of energy that will be consumed and produced for a particular city or region. Especially in the later field, the demand is influenced by a mixture of external factors such as the weather conditions. For energy suppliers it is more beneficial to buy electricity on the day-ahead market than on the spot market. Consequently, the more accurate the energy consumption can be predicted, the lower the cost. Given that more and more houses and appliances are enabled with a smart meter, an increasing amount of data becomes available that can enable and improve this prediction. Furthermore, these kinds of predictions also allow energy suppliers to better balance demand and supply and helps them to ensure proper grid operation, but also to avoid negative prices, for example in times of high solar irradiation. In this example we see the active power of a single household near Paris shown in green. In orange, the outside temperature is given. The two are strongly correlated ‚Äì the demand increases with decreasing temperatures since most French households use electrical heating. Therefore, particularly in winter times, the grid has to be stable when facing high demands. In this Starter Kit, we will demonstrate how to effectively forecast the demand for this household for the future by taking external factors into account. In the following videos, we will first start by gaining some first insights in the data in front of us and perform some initial pre-processing steps to prepare the data for the analysis. Subsequently, we will illustrate you how to gain deeper insights by presenting a number of visual and statistical techniques in order to explore the data for identifying interesting patterns. In a next step, useful characteristics or features will be extracted from the dataset that can as serve as input for the forecasting models. Finally, in the last video, we will explain you how to correctly compare the performance of multiple forecasting models. Let us go ahead with the next video on data preparation, in which we will describe how to deal with the typical problems of raw data and how to perform data fusion when multiple sources of data are available. Additional information The video material in this website was developed in the context of the SKAIDive project , financially supported by the European Social Fund , the European Union and Flanders. For more information, please contact us at elucidatalab@sirris.be","tags":"Resource demand Forecasting","url":"/Resource demand Forecasting/2020-01-02-introduction.html","loc":"/Resource demand Forecasting/2020-01-02-introduction.html"},{"title":"Getting Started with the Interactive Experimenting Environment","text":"Getting Started with the Interactive Experimenting Environment In the AI Starter Kits, we mainly work in so-called Jupyter Notebooks . The Jupyter Project is a non-profit, open-source project born in 2004. Jupyter Notebook, that was developed within the project, is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text [1]. If you have never worked with Jupyter Notebooks before, we invite you to read this short tutorial before starting with the actual content of the AI Starter Kits. We provide you some short explanations on how to use the web application to run the python code that was developed. How to get started? For the AI Starterkits, getting started is very easy! Just click at the link provided on the platform and it will bring you directly into Jupyter Hub, which hosts the Jupyter Notebooks for the Starter Kits. For each Starter Kit, we created a so-called environment in which all libraries needed are already installed such that you do not need to take care of this at all. You can get started straight away. How to use the Jupyter Notebooks? Before we have a look at the notebooks themselves, we first have a brief look at the toolbar at the top of each notebook as shown in the following figure. The toolbar provides all buttons to interact with the code in a notebook. We go through the single buttons step by step. Important to understand is that each Jupyter Notebook consists of different cells. In each cell, you can execute a batch of code. In order to create a new cell, you simply click at the button and a new code cell will appear in the notebook. Try it yourself and create a new cell. You notebook should look like this now: In this cell, we can execute a simply command like print(\"Hello World!\") and execute it by pushing the or more easily by pushing the combination SHIFT + Enter . Congratulations, you just executed your first python command in a Jupyter Notebook! Just below the cell, the result in shown: In this case, the words 'Hello World!' are printed. Additionally, a new cell was automatically created just below the former one. Of course, we can also execute several commands within one cell. For this, please enter the following lines in the next cell: a = 5 a b = 3 a + b Before you execute the cell, think about the output you expect. Was the output as expected? Or did you expect to also see the value of a ? Keep in mind that Jupter Notebook will always return the last executed command only. In case you also want to see the value of a , you need to wrap a print(a) command around it. How to add documentation to your notebook? In order to increase the readability of the notebook, it helps to add some documentation around the single commands. This can be helpful in case you want to share the notebooks with others and want to explain why you performed a given command. The easiest way to to this is my changing the cell output in the toolbar: After doing so, you can enter some text in Markdown format around your cells. Test it by entering This was a ** nice ** example ! into a Markdown cell and execute it just as a normal code cell by pushing the button. How to save a Jupyter Notebook? Once you are done with a notebook you have two simple ways to share it with others: * Directly as a Jupyter Notebook. If the recipent has a Jupyter environment, he or she can directly continue working on your code. * Otherwise, you can also export the Notebook in html format: Bibliography [1] https://jupyter.org/","tags":"Time Series Preprocessing","url":"/Time Series Preprocessing/2020-01-01-getting-started.html","loc":"/Time Series Preprocessing/2020-01-01-getting-started.html"},{"title":"Getting Started with the Interactive Experimenting Environment","text":"Getting Started with the Interactive Experimenting Environment In the AI Starter Kits, we mainly work in so-called Jupyter Notebooks . The Jupyter Project is a non-profit, open-source project born in 2004. Jupyter Notebook, that was developed within the project, is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text [1]. If you have never worked with Jupyter Notebooks before, we invite you to read this short tutorial before starting with the actual content of the AI Starter Kits. We provide you some short explanations on how to use the web application to run the python code that was developed. How to get started? For the AI Starterkits, getting started is very easy! Just click at the link provided on the platform and it will bring you directly into Jupyter Hub, which hosts the Jupyter Notebooks for the Starter Kits. For each Starter Kit, we created a so-called environment in which all libraries needed are already installed such that you do not need to take care of this at all. You can get started straight away. How to use the Jupyter Notebooks? Before we have a look at the notebooks themselves, we first have a brief look at the toolbar at the top of each notebook as shown in the following figure. The toolbar provides all buttons to interact with the code in a notebook. We go through the single buttons step by step. Important to understand is that each Jupyter Notebook consists of different cells. In each cell, you can execute a batch of code. In order to create a new cell, you simply click at the button and a new code cell will appear in the notebook. Try it yourself and create a new cell. You notebook should look like this now: In this cell, we can execute a simply command like print(\"Hello World!\") and execute it by pushing the or more easily by pushing the combination SHIFT + Enter . Congratulations, you just executed your first python command in a Jupyter Notebook! Just below the cell, the result in shown: In this case, the words 'Hello World!' are printed. Additionally, a new cell was automatically created just below the former one. Of course, we can also execute several commands within one cell. For this, please enter the following lines in the next cell: a = 5 a b = 3 a + b Before you execute the cell, think about the output you expect. Was the output as expected? Or did you expect to also see the value of a ? Keep in mind that Jupter Notebook will always return the last executed command only. In case you also want to see the value of a , you need to wrap a print(a) command around it. How to add documentation to your notebook? In order to increase the readability of the notebook, it helps to add some documentation around the single commands. This can be helpful in case you want to share the notebooks with others and want to explain why you performed a given command. The easiest way to to this is my changing the cell output in the toolbar: After doing so, you can enter some text in Markdown format around your cells. Test it by entering This was a ** nice ** example ! into a Markdown cell and execute it just as a normal code cell by pushing the button. How to save a Jupyter Notebook? Once you are done with a notebook you have two simple ways to share it with others: * Directly as a Jupyter Notebook. If the recipent has a Jupyter environment, he or she can directly continue working on your code. * Otherwise, you can also export the Notebook in html format: Bibliography [1] https://jupyter.org/","tags":"Remaining Useful Life Prediction","url":"/Remaining Useful Life Prediction/2020-01-01-getting-started.html","loc":"/Remaining Useful Life Prediction/2020-01-01-getting-started.html"},{"title":"Getting Started with the Interactive Experimenting Environment","text":"Getting Started with the Interactive Experimenting Environment In the AI Starter Kits, we mainly work in so-called Jupyter Notebooks . The Jupyter Project is a non-profit, open-source project born in 2004. Jupyter Notebook, that was developed within the project, is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text [1]. If you have never worked with Jupyter Notebooks before, we invite you to read this short tutorial before starting with the actual content of the AI Starter Kits. We provide you some short explanations on how to use the web application to run the python code that was developed. How to get started? For the AI Starterkits, getting started is very easy! Just click at the link provided on the platform and it will bring you directly into Jupyter Hub, which hosts the Jupyter Notebooks for the Starter Kits. For each Starter Kit, we created a so-called environment in which all libraries needed are already installed such that you do not need to take care of this at all. You can get started straight away. How to use the Jupyter Notebooks? Before we have a look at the notebooks themselves, we first have a brief look at the toolbar at the top of each notebook as shown in the following figure. The toolbar provides all buttons to interact with the code in a notebook. We go through the single buttons step by step. Important to understand is that each Jupyter Notebook consists of different cells. In each cell, you can execute a batch of code. In order to create a new cell, you simply click at the button and a new code cell will appear in the notebook. Try it yourself and create a new cell. You notebook should look like this now: In this cell, we can execute a simply command like print(\"Hello World!\") and execute it by pushing the or more easily by pushing the combination SHIFT + Enter . Congratulations, you just executed your first python command in a Jupyter Notebook! Just below the cell, the result in shown: In this case, the words 'Hello World!' are printed. Additionally, a new cell was automatically created just below the former one. Of course, we can also execute several commands within one cell. For this, please enter the following lines in the next cell: a = 5 a b = 3 a + b Before you execute the cell, think about the output you expect. Was the output as expected? Or did you expect to also see the value of a ? Keep in mind that Jupter Notebook will always return the last executed command only. In case you also want to see the value of a , you need to wrap a print(a) command around it. How to add documentation to your notebook? In order to increase the readability of the notebook, it helps to add some documentation around the single commands. This can be helpful in case you want to share the notebooks with others and want to explain why you performed a given command. The easiest way to to this is my changing the cell output in the toolbar: After doing so, you can enter some text in Markdown format around your cells. Test it by entering This was a ** nice ** example ! into a Markdown cell and execute it just as a normal code cell by pushing the button. How to save a Jupyter Notebook? Once you are done with a notebook you have two simple ways to share it with others: * Directly as a Jupyter Notebook. If the recipent has a Jupyter environment, he or she can directly continue working on your code. * Otherwise, you can also export the Notebook in html format: Bibliography [1] https://jupyter.org/","tags":"Resource demand Forecasting","url":"/Resource demand Forecasting/2020-01-01-getting-started.html","loc":"/Resource demand Forecasting/2020-01-01-getting-started.html"},{"title":"Interactive notebook","text":"In [2]: import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn import preprocessing import re import os from ipywidgets import register import cufflinks as cf cf . go_offline ( connected = True ) cf . set_config_file ( colorscale = 'plotly' , world_readable = True ) % matplotlib inline from IPython.core.display import HTML from IPython.display import Markdown , display def printmd ( string ): display ( Markdown ( string )) from starterkits.visualization import vis_plotly_widgets as vpw import visualizations as v import support as sp from support import LstmApp , get_test_scores # Fixing the used seeds for reproducibility of the experiments np . random . seed ( 1234 ) PYTHONHASHSEED = 0 display ( HTML ( \" \" )) /Users/hcab/opt/miniconda3/lib/python3.7/site-packages/dash_bootstrap_components/table.py:1: UserWarning: The dash_html_components package is deprecated. Please replace `import dash_html_components as html` with `from dash import html` Starter Kit 1.2.1: Remaining useful life estimation Business context Maintenance is an important part of an asset's lifecycle. In the past, corrective and preventive maintenance were the norm: corrective maintenance was performed when an asset had failed and involved tasks for identifying the fault and rectifying it so that the asset could resume normal operation preventive maintenance was performed to avoid asset failure and involved executing maintenance tasks at regular intervals, e.g. when an asset was used for a certain period of time or when it executed a pre-determined number of cycles. Nowadays, ever more assets are equipped with sensors and ever more data is gathered. This data can be exploited to realize predictive maintenance, which promises costs saving over corrective or preventive maintenance because maintenance is only performed when warranted. Predictive maintenance encompasses a variety of topics, such as failure prediction, remaining useful lifetime estimation (RUL), failure detection, failure diagnosis (root cause analysis) and recommendation of mitigation or maintenance actions after failure. Business goal The business goal addressed by this Starter Kit is the estimation of the remaining useful lifetime of an asset. 'Lifetime' in this context is expressed in terms of the evolution of a particular quantity, such as the distance travelled, fuel consumed, repetition cycles performed, number of transactions experienced, etc. Estimating the remaining useful lifetime is non-trivial, as it is influenced by a multitude of internal and external factors, e.g. operating conditions, weather conditions, usage scenarios, etc. Application contexts Estimating the remaining useful lifetime of an asset can be beneficial for several purposes in a variety of industrial contexts, for example: to better schedule maintenance operations, e.g. for offshore wind turbines, for which maintenance can only be performed under the right weather conditions. to optimise the operational efficiency of an asset, by more optimally planning the use of that asset before its end of life, or adapting its operational use to extend its useful lifetime. to avoid unplanned downtime, e.g. in manufacturing settings, where a downtime of the production line results in several operational problems and associated costs. to anticipate and avoid safety-critical situations, e.g. for aircrafts or vehicles. ... Data characteristics and requirements Typically, the data to be exploited for remaining useful lifetime estimation has the following characteristics: it consists of time-series data, in particular for the quantity used to express the lifetime of the asset but also for all the factors that influence that quantity (e.g. weather and operating conditions). temporal patterns such as trends, cycles, seasonality, day/night or week/weekend patterns can be present in the data. random fluctuation (noise) can also be present, as a result of random variation or unexplained causes. In a large number of cases the data is measured using sensors, therefore some noise is inherently present on the measured signal. Moreover, the following requirements are imposed on the dataset: a sufficiently large amount of historical data needs to be available. Two aspects are important in this respect: the implicit assumption is that an asset has a continuous degradation pattern, which is reflected in the asset's sensor measurements. Degradation is often slow, however, so the monitoring period should be sufficiently long so as to include a clear presence of the degradation pattern. a sufficient number of examples of assets that reached their end of life needs to be present. the data needs to include explicit information on when an asset has reached its end of life or should allow to easily derive it (e.g. the lifetime quantity has reached a particular threshold). Starter Kit outline The data-driven approach for remaining useful life estimation will be illustrated on the industrially-relevant use case of predicting the remaining useful lifetime of aircraft engines based on run-to-failure data . We will use this dataset to illustrate: How to appropriately preprocess the data and construct training, test and validation sets to learn and evaluate a model able to predict whether an engine will fail within a certain number of operational cycles. How to use deep learning to construct a model for remaining useful lifetime prediction. In particular, we apply a deep learning approach that uses short-term memory ( LSTM ) networks and allows the construction of a predictive model without the time-consuming feature engineering step, which requires one to manually extract the characteristics from which the algorithm can learn. How to experimentally validate the resulting model and objectively compare it with other approaches. The resulting model achieves a high accuracy. The approach is simple and flexible enough to be applied to a wide range of RUL problems. Data understanding The time series data we will use in this Starter Kit is simulated run-to-failure data from aircraft engines. Engines in the data are assumed to start with varying degrees of wear and manufacturing variation. This information is unknown to the user. Furthermore, in this simulated data, engines are assumed to be operating normally at the beginning, and start to degrade at some point during operation. The degradation progresses and grows in magnitude. When a predefined threshold is reached, the engine is considered unsafe for further operation. In other words, the last operational cycle of the engine can be considered as the failure point of the corresponding engine. In machine learning experiments, a dataset is often split into a training set and a test set, which allows one to quickly evaluate the performance of an algorithm on the problem at hand. The training dataset is used to prepare a model, to train it. We pretend the test dataset is new, unseen data where the output values (in this case, whether or not a specific engine is going to fail within a predefined number of cycles) are withheld from the algorithm. We gather predictions from the trained model on the inputs from the test dataset and compare them to the withheld output values of the test set, the so-called ground truth . Comparing the predictions and withheld outputs on the test dataset allows us to compute a performance measure for the model on the test dataset. This is an estimate of the skill of the algorithm trained on the problem when making predictions on unseen data. For this reason, the dataset is divided into three parts, namely the training, test and ground truth datasets: The training data will be used by the algorithm to learn the prediction model. It consists of multiple time series with \"cycle\" as the time unit, and 21 sensor readings for each cycle. Each sequence of cycles can be assumed as being generated from a different engine (identified by the engine ID) of the same type. Since the algorithm needs to be able to learn when the engine will fail, the last time period does represent the failure point . The test data has the same data schema as the training data and is used to test the quality of the resulting model. The only difference is that the data does not indicate when the failure occurs, or put differently, the last time period does not represent the failure point. The ground truth data provides the number of remaining working cycles for the engines in the test data, which will only be used to assess the quality of the results, but will not be given as input to the learned model. More details on this dataset can be found in [1]. In [3]: train_df , test_df , truth_df = sp . load_data () The table below shows an excerpt of the training data, with the following attributes: id: the identifier of the engine cycle: the current cycle of the engine from which the data in the respective row is originating, i.e. the current flight of the aircraft setting 1-3: the values of 3 different operational settings of the engine, e.g. an operational setting could correspond to a cruise setting when the aircraft is flying at cruise speed s1-21: 21 sensor readings monitoring various parameters of the engine, e.g. temperature or vibration Since the input file is space-separated, and two spurious spaces appear at the end of each line, we remove the last two columns. In [4]: train_df . head () Out[4]: id cycle setting1 setting2 setting3 s1 s2 s3 s4 s5 ... s12 s13 s14 s15 s16 s17 s18 s19 s20 s21 0 1 2 0.0019 -0.0003 100.0 518.67 642.15 1591.82 1403.14 14.62 ... 522.28 2388.07 8131.49 8.4318 0.03 392 2388 100.0 39.00 23.4236 1 1 3 -0.0043 0.0003 100.0 518.67 642.35 1587.99 1404.20 14.62 ... 522.42 2388.03 8133.23 8.4178 0.03 390 2388 100.0 38.95 23.3442 2 1 4 0.0007 0.0000 100.0 518.67 642.35 1582.79 1401.87 14.62 ... 522.86 2388.08 8133.83 8.3682 0.03 392 2388 100.0 38.88 23.3739 3 1 5 -0.0019 -0.0002 100.0 518.67 642.37 1582.85 1406.22 14.62 ... 522.19 2388.04 8133.80 8.4294 0.03 393 2388 100.0 38.90 23.4044 4 1 6 -0.0043 -0.0001 100.0 518.67 642.10 1584.47 1398.37 14.62 ... 521.68 2388.03 8132.85 8.4108 0.03 391 2388 100.0 38.98 23.3669 5 rows √ó 26 columns The test data looks similar to the training data, with the difference that it does not indicate when the failure occurs, or put differently, the last time period does not represent the failure point. Taking the sample test data shown in the table below as an example, the engine with id=1 runs from cycle 1 through cycle 31. It is not shown how many more cycles this engine can last before it fails. In [5]: test_df [ 25 : 31 ] Out[5]: id cycle setting1 setting2 setting3 s1 s2 s3 s4 s5 ... s12 s13 s14 s15 s16 s17 s18 s19 s20 s21 25 1 27 -0.0007 0.0001 100.0 518.67 642.08 1586.65 1400.31 14.62 ... 521.82 2388.02 8127.24 8.4494 0.03 392 2388 100.0 38.87 23.3931 26 1 28 0.0022 0.0005 100.0 518.67 641.93 1594.25 1401.29 14.62 ... 521.84 2388.07 8134.89 8.4470 0.03 392 2388 100.0 38.83 23.3502 27 1 29 0.0014 0.0001 100.0 518.67 641.95 1587.15 1398.11 14.62 ... 522.39 2388.07 8133.13 8.4212 0.03 392 2388 100.0 39.02 23.3621 28 1 30 -0.0025 0.0004 100.0 518.67 642.79 1585.72 1400.97 14.62 ... 521.78 2388.10 8134.79 8.4110 0.03 391 2388 100.0 39.09 23.4069 29 1 31 -0.0006 0.0004 100.0 518.67 642.58 1581.22 1398.91 14.62 ... 521.79 2388.06 8130.11 8.4024 0.03 393 2388 100.0 38.81 23.3552 30 2 1 -0.0009 0.0004 100.0 518.67 642.66 1589.30 1407.16 14.62 ... 521.62 2388.14 8129.59 8.4283 0.03 392 2388 100.0 39.00 23.3923 6 rows √ó 26 columns The ground truth data provides the number of remaining working cycles for the engines in the test dataset. Taking the sample ground truth data shown in the following table as an example, the engine with id 1 in the test data can run for another 112 cycles before it fails. In [6]: display ( truth_df . set_index ( truth_df . index + 1 ) . head ( 3 )) 112 1 98 2 69 3 82 Data preprocessing Before we can start training a model to predict the RUL, a number of preparatory steps needs to be taken. As could be seen from the training data sample above, currenly the data does not include a column indicating the RUL, which is the target value we want to predict. As the algorithm needs this information to learn from, the first step will be to create such a column. A second observation that can be made on the training data sample is that the scale of the values differs across columns. This difference could cause problems when the model needs to calculate the similarity between different cycles (i.e., the rows in the table) during modeling. To address this problem, we will normalise the data. The goal of normalisation is to change the values of the numeric columns in the dataset, in order to have a common scale and avoid distorting differences in the ranges of values or losing information. The normalised values maintain the general distribution and ratios in the source data, while keeping values within a scale applied across all numeric columns used in the model. RUL label creation The first preprocessing step is to generate a label indicating the Remaining Useful Lifetime. In this Starter Kit, we will not predict the exact number of remaining cycles the engine still has left before failure, yet rather try answering the question 'Is a specific engine going to fail within w1 cycles?', where w1 is a predefined threshold used during the label creation process. Training data In order to determine the remaining number of cycles at each time point, the maximum number of cycles per engine is first determined. Subsequently, the current cycle number is substracted from the max number of cycles to obtain the number of cycles remaining at a particular time point. In the following sample, the column RUL indicates the remaining number of cycles for that particular point in time (i.e. the row). In [7]: rul = pd . DataFrame ( train_df . groupby ( 'id' )[ 'cycle' ] . max ()) . reset_index () rul . columns = [ 'id' , 'max' ] train_df = train_df . merge ( rul , on = [ 'id' ], how = 'left' ) train_df [ 'RUL' ] = train_df [ 'max' ] - train_df [ 'cycle' ] train_df . drop ( 'max' , axis = 1 , inplace = True ) Xtrain = train_df . copy () train_df [[ 'id' , 'cycle' , 'RUL' ]] . head () Out[7]: id cycle RUL 0 1 2 190 1 1 3 189 2 1 4 188 3 1 5 187 4 1 6 186 In order to arrive to a binary label indicating whether an engine is going to fail within w1 cycles, the next step is to test whether the number of remaining cycles in the RUL column is smaller than or equal to the predefined threshold w1 , currently set to 30 cycles. This is illustrated in the sample below, where for the first row, the value in the column RUL_label is 1, because the engine will fail within 4 cycles. In [8]: w1 = 30 train_df [ 'RUL_label' ] = train_df . RUL . apply ( lambda x : x <= w1 ) . astype ( int ) train_df [[ 'id' , 'cycle' , 'RUL' , 'RUL_label' ]] . tail () Out[8]: id cycle RUL RUL_label 20625 100 196 4 1 20626 100 197 3 1 20627 100 198 2 1 20628 100 199 1 1 20629 100 200 0 1 Test data A similar approach is adopted for the test set. Note that, in order to generate the labels for the test data, we need to use the ground truth dataset, because the test data does not indicate when the failure occurs, or put differently, the last time period does not represent the failure point. In [9]: # Generate column max for test data rul = pd . DataFrame ( test_df . groupby ( 'id' )[ 'cycle' ] . max ()) . reset_index () rul . columns = [ 'id' , 'max' ] truth_df . columns = [ 'more' ] truth_df [ 'id' ] = truth_df . index + 1 truth_df [ 'max' ] = rul [ 'max' ] + truth_df [ 'more' ] truth_df . drop ( 'more' , axis = 1 , inplace = True ) # Generate RUL for test data test_df = test_df . merge ( truth_df , on = [ 'id' ], how = 'left' ) test_df [ 'RUL' ] = test_df [ 'max' ] - test_df [ 'cycle' ] test_df . drop ( 'max' , axis = 1 , inplace = True ) # Generate label columns RUL and RUL_label for test data test_df [ 'RUL_label' ] = np . where ( test_df [ 'RUL' ] <= w1 , 1 , 0 ) test_df [[ 'id' , 'cycle' , 'RUL' , 'RUL_label' ]]; Normalisation using Min-Max scaling As explained before, the goal of normalisation is to change the values of numeric columns in the dataset to a common scale. In this notebook, to put it simply, normalisation is performed to avoid variables on a higher scale affecting the outcome in bigger measure only because they are on a higher scale. One of the most popular normalisation technique is Min-Max normalisation , which we also use here. It scales the values of a variable to a range between 0 and 1 using the following formula, where $X$ represents the value to be normalised, $X_{min}$ is the minimum value of the variable in that column and $X_{max}$ is the maximum value for that variable: $X_{norm} = \\frac{X - X_{min}}{X_{max} - X_{min}}$ Note: an alternative approach to Min-Max scaling is Z-score normalisation (or standardisation ) in which the variables will be rescaled so that they have the properties of a standard normal distribution with zero mean and unit variance. For both the training set and the test set, we normalise all columns except id , cycle , and RUL , as we want to keep the original values for these columns. The following sample shows a portion of the normalised training data. In [10]: train_df [ 'cycle_norm' ] = train_df [ 'cycle' ] cols_normalize = train_df . columns . difference ([ 'id' , 'cycle' , 'RUL' ]) min_max_scaler = preprocessing . MinMaxScaler () norm_train_df = pd . DataFrame ( min_max_scaler . fit_transform ( train_df [ cols_normalize ]), columns = cols_normalize , index = train_df . index ) join_df = train_df [ train_df . columns . difference ( cols_normalize )] . join ( norm_train_df ) train_df = join_df . reindex ( columns = train_df . columns ) train_df . describe () . loc [[ 'mean' , 'min' , 'max' ]] Out[10]: id cycle setting1 setting2 setting3 s1 s2 s3 s4 s5 ... s15 s16 s17 s18 s19 s20 s21 RUL RUL_label cycle_norm mean 51.509016 108.813088 0.499492 0.501975 0.0 0.0 0.443065 0.424747 0.450442 0.0 ... 0.451122 0.0 0.434226 0.0 0.0 0.524232 0.546119 107.803829 0.150267 0.298651 min 1.000000 1.000000 0.000000 0.000000 0.0 0.0 0.000000 0.000000 0.000000 0.0 ... 0.000000 0.0 0.000000 0.0 0.0 0.000000 0.000000 0.000000 0.000000 0.000000 max 100.000000 362.000000 1.000000 1.000000 0.0 0.0 1.000000 1.000000 1.000000 0.0 ... 1.000000 0.0 1.000000 0.0 0.0 1.000000 1.000000 361.000000 1.000000 1.000000 3 rows √ó 29 columns As can be seen, both the settings variables and the sensor variables now only contain values between 0 and 1. In [11]: test_df [ 'cycle_norm' ] = test_df [ 'cycle' ] norm_test_df = pd . DataFrame ( min_max_scaler . transform ( test_df [ cols_normalize ]), columns = cols_normalize , index = test_df . index ) test_join_df = test_df [ test_df . columns . difference ( cols_normalize )] . join ( norm_test_df ) test_df = test_join_df . reindex ( columns = test_df . columns ) test_df = test_df . reset_index ( drop = True ); Data Modeling and Analysis Traditional machine learning models are usually based on manually extracting distinguishing factors that characterise the phenomenon to be learned using domain expertise (the so-called feature engineering step). This usually makes these models hard to reuse since feature engineering is specific to the problem scenario and the available data, and therefore a labour-intensive process. In contrast, deep learning algorithms can automatically extract the right features from the data, eliminating the need for manual feature engineering. Deep learning refers to machine learning approaches involving artificial neural networks (ANNs) that contain a multitude of so-called hidden layers. One particular type of ANNs are the Long Short-Term Memory networks, or LSTMs. One critical advantage of LSTMs is their ability to remember information from long-term sequences, which is hard to achieve with traditional feature engineering. When manually engineering features for this problem, one would for example compute the rolling average per windows of 50 cycles. This averaging may however lead to a loss of information due to the smoothing and abstracting of values over such a long period. Instead, using all 50 values as input may provide better results. While performing feature engineering over large window sizes may complicate things for traditional machine learning methods, LSTMs are naturally able to handle larger window sizes and use all the information in the window as input. Algorithm-specific data preparation As a first step in the modeling phase, we will prepare the data to serve as input for the LSTM network. When using LSTMs in the time-series domain, one important parameter to pick is the sequence length, which is the size of the window within which the LSTMs needs to remember information. This is similar to picking a window size when calculating time series features using a rolling window. The idea of using LSTMs is to let the model extract abstract features from the sequence of sensor values in the window rather than engineering those manually. The expectation is that if there is a pattern in these sensor values within the window prior to failure, the pattern should be extracted and learned by the LSTM. The window size chosen for the training data set strongly influences the prediction results. In the animation below, you can see how differently the single variables for a given engine behave. Furthermore, the time at which the degradation becomes visible is different for different engines. When using the initial settings in the animation below, i.e. Variable equals s11 , ID equals 1 and period equals 50, we see a comparably short line fluctuating around a constant value. When selecting the IDs 3 and 13 instead and increasing the period to 130 at the same time, one sees a clear deviation from the median value indicated by the horizontal gray dashed line. The deviation for both starts at around index 110. When then additionally selecting the engine with ID 36, the deviation starts already at approximately index 80 and therewith significantly earlier. Further looking at Variable s12 , the deviation from the median starts at the same times as for s11 for the engines under inspection. When selecting the training period for the model, we need to take the different start of the deviation into account. Here we need to find a good balance between the two different influences: on the one hand, the longer the training data sequence, the better the results; on the other hand, when selecting a too long training period, it is possible that the model learns that a slight deviation is fine in several engines and will therewith fail in predicting the remaining useful lifetime. The period length will be used in the remainder of the notebook to calculate the model sequences. In [12]: date_range_slider = v . interactive_plot ( test_df ) Based on the figures above, it is very hard to visually spot any trends that are influencial for the remaining useful life, let alone to manually extract features from it. As indicated above, the advantage of deep learning algorithms is that they can automatically extract the right features from the data, eliminating the need for manual feature engineering. In the following, we pick the feature columns ( sensor_cols ) to use for modelling as well as the columns identifying the single sequences ( sequence_cols ). In [13]: sensor_cols = [ x for x in test_df if re . findall ( string = x , pattern = 's[0-9]' )] sequence_cols = [ 'setting1' , 'setting2' , 'setting3' , 'cycle_norm' ] sequence_cols . extend ( sensor_cols ) Keras is a deep learning library written in Python, which naturally interfaces with TensorFlow , an open-source software library for machine learning. Its LSTM layer expects an input in the shape of a 3-dimensional matrix of (samples, time steps, features) (more specifically a 3-dimensional NumPy array) where 'samples' is the number of training sequences, 'time steps' is the look back window or sequence length and 'features' is the number of features of each sequence at each time step. During this transformation, we also select the columns from the input data that can serve as features. In this case, we selected all sensor values, as well as the 3 settings parameters and the (normalised) current cycle the machine is in at this point. In [14]: def gen_sequence ( id_df , seq_length , seq_cols ): \"\"\" Function to reshape the data into a 3-dimensional (samples, time steps, features) matrix \"\"\" data_array = id_df [ seq_cols ] . values num_elements = data_array . shape [ 0 ] for start , stop in zip ( range ( 0 , num_elements - seq_length ), range ( seq_length , num_elements )): yield data_array [ start : stop , :] For each id in the data set, a data sequence will be created. As initial sequence length, we will use 50 time steps. In [15]: register ( date_range_slider ) sequence_length = 50 seq_gen = ( list ( gen_sequence ( train_df [ train_df [ 'id' ] == id ], sequence_length , sequence_cols )) for id in train_df [ 'id' ] . unique ()) seq_array = np . concatenate ( list ( seq_gen )) . astype ( np . float32 ) printmd ( f '''This results in a 3-dimensional input matrix with { seq_array . shape [ 0 ] } samples, { seq_array . shape [ 1 ] } time steps and { seq_array . shape [ 2 ] } features (including 21 sensor values, the 3 settings parameters and the normalised current cycle).''' ) This results in a 3-dimensional input matrix with 15630 samples, 50 time steps and 25 features (including 21 sensor values, the 3 settings parameters and the normalised current cycle). For the training data, the labels are also added to this data structure, so that the LSTM can learn from it. In [16]: def gen_labels ( id_df , seq_length , label ): data_array = id_df [ label ] . values num_elements = data_array . shape [ 0 ] return data_array [ seq_length : num_elements , :] def handler ( x ): pass sequence_length = date_range_slider . get_interact_value () label_gen = [ gen_labels ( train_df [ train_df [ 'id' ] == id_ ], sequence_length , [ 'RUL_label' ]) for id_ in train_df [ 'id' ] . unique ()] label_array = np . concatenate ( label_gen ) . astype ( np . float32 ) v . plot_label_frequency ( label_array ) The majority of data points, roughly 80 %, are labeled as zero, indicating that no failure was observed. LSTM network Note: we refer the interested reader to Appendix A for a brief introduction to LSTM networks. In this section, we guide you through the general process of building, training, evaluating and testing a LSTM model. This will be done by means of an interactive interface. In order to see the code used for these steps, please expand the fields below. Exercise 1 Network construction Let us start by building an LSTM network. In our setup, we will experiment with a stacked structure. In order to learn from different architectures of the LSTM model, it is possible to change the model variables in the animation below. We ask the reader to test different model scenarios by applying the following steps: Select the number of intermediate layers. For the first experiment, please select a single intermediate layer only. Set the size of the intermediate layer to 30. This corresponds to a rather small network. Set the number of epochs for training to 15. Epochs define the number of times the model iterates over the training data arrays, the aim being for the model to improve during each training epoch. In general, the more epochs the better the results, until we reach the given model's limit. Similarly as before, the training sequence length defines the number of data points to take into account for training per engine. Please select a sequence length of 50. Furthermore, we add a dropout layer after each LSTM layer. The dropout consists in randomly setting a fraction rate of input units to 0 at each update during training time, which helps preventing overfitting. Overfitting means that a model fits too closely a limited set of data points. This typically results in an overly complex model that fits all little details in the data under study. Please choose a value of 0.2 for this first experiment. Model training Now that we have fixed the network design, we can fit the network to the training data by pushing the button Train model . After a successful training, a green badge will appear. The fitting step is influenced by the following parameters, which we will keep fixed during our experiments: Batch size: The number of samples that are used per gradient update (while fixing the best parameters for the model). We will use a batch size of 200. Validation split: fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. We will use a validation split of 5%. Furthermore, the fitting function will stop early if the validation loss (which is the quality criterion that is used to evaluate the model performance) is not decreasing anymore. To parametrise the early stopping function, the following parameters can be handled by TensorFlow: Minimal delta: minimum change in the monitored quantity as given by the validation loss to qualify as an improvement, i.e. an absolute change of less than min_delta will count as no improvement. Patience: number of epochs with no improvement after which training will be stopped. In the experiments defined in this Starter Kit, we keep both minimal delta and patience at zero. Model evaluation and testing Now that we have a fitted model, we can evaluate its performance. We will first evaluate the model performance on the training data and subsequently on the test data. If both evaluations result in approximately the same score, this gives an indication that the model is generalisable to unseen datasets and thus is not overfitting on the training data. In the interface, please switch to the tab Evaluation at the top and press the button Evaluate model . This will lead to the evaluation of the last trained model. The model can be evaluated using different measures. We will on the one hand evaluate the model in function of accuracy , which measures the fraction of all instances that are correctly categorized; it is the ratio of the number of correct classifications to the total number of instances to classify. On the other hand, we will show the so-called confusion matrix . The confusion matrix for the model that we just trained shows that the model is able to correctly classify that a specific engine is not going to fail within w1 cycles for almost all of the 12531 samples. Vice versa, the model is able to correctly classify for almost all of the 3100 cases that a specific engine is going to fail within w1 cycles. \\ Based on the confusion matrix, several other measures such as precision and recall can be calculated. Intuitively, precision is the ability of the classifier not to label a sample that is negative as positive. When instantiated for the RUL prediction problem we are currently considering, precision indicates how many of the engines that the model predicted to reach their end of life within 30 cycles also actually reach their end of life within this amount of cycles. Recall is the ability of the classifier to find all the positive samples, i.e., all engines that reach their end of life within 30 cycles. The respective results are given in the model evaluation summary table. A unique id will be assigned to each evaluated model in order to compare the results for different models. Note that the evaluation is performed on the training data. In order to evaluate it on the test data (that is unknown to the model), please continue to the tab Testing . By pushing the button Evaluate model , the model selected in the dropdown menu is evaluated on the test data. Questions: Is there a significant difference in accuracy when evaluating the model on the validation and test data? Do you think the model is overfitting? Exercise 2: The LSTM network built in Exercise 1 performs well on the evaluation data set with an accuracy of about 95% but with a lower accuracy on the test data. This gives us a hint that the model might be overfitting on the training data. Therefore, in this second experiment, we add a second LSTM layer to the network. Please increase the number of layers to 2 and set the following layer sizes: Intermediate Layer 1: 200 Intermediate Layer 2: 50 Keep the remaining parameters as they are. Subsequently repeat the steps taken in the previous exercise: Train the model Evaluate it on the validation data Evaluate it on the test data Questions: Which difference do you observe? Compare the results in the tables in both tabs - evaluation and testing When comparing the two models, which one would you use and why? Exercise 3: Use the interface to experiment with alternative architectures and see how the parameters influence the quality of the model. You can try to answer questions such as: Will the model improve when the number of epochs is increased? Is it possible to reach an accuracy of 98 % on both data sets (validation and test)? Does adding a third LSTM layer improve the results even further? LSTM interface Note: if the current port is already in use, change it to any port between 8050 and 8060 In [17]: PORT = 8050 v . start_dashboard ( Xtrain , sequence_cols , test_df , PORT ) Dash app running on http://127.0.0.1:8050 Code In what follows, you can find the most prominent functions that are called in the interactive interface to construct and train the LSTM model, and subsequently evaluate it. Model building and compilation def compile_model ( num_layers = 2 , sizes ): model = Sequential ( name = 'Sequential_Layer' ) for i in range ( num_layers ): if sizes [ i ]: if i == 0 : model . add ( LSTM ( input_shape = ( seq_len , nb_features ), units = sizes [ i ], return_sequences = bool ( sizes [ i + 1 ] is not None ))) else : model . add ( LSTM ( units = sizes [ i ], return_sequences = bool ( sizes [ i + 1 ] is not None ))) model . add ( Dropout ( dropout )) model . add ( Dense ( units = nb_out , activation = 'sigmoid' )) model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ]) return ( model ) Model training model . fit ( seq_array , label_array , epochs = epochs , batch_size = 200 , validation_split = 0.05 , verbose = 0 , callbacks = [ keras . callbacks . EarlyStopping ( monitor = 'val_loss' , min_delta = 0 , patience = 0 , verbose = 0 , mode = 'auto' )]) Make predictions and compute confusion matrix y_pred = model . predict_classes ( seq_array , verbose = 1 , batch_size = 200 ) y_true = label_array cm = confusion_matrix ( y_true , y_pred ) Comparison of performance against other methods In order to get an idea of the quality of the LSTM model, we will compare it with the best performing model from [1]. In this experiment, four binary classification models are compared, namely a Two-Class Logistic Regression model, a Two-Class Boosted Decision Tree, a Two-Class Decision Forest, and a Two-Class Neural Network. Note that for these models, a manual feature engineering step was needed, whereas in the case of the LSTM model, the algorithm automatically decides on the most appropriate features. The table below compares the best performing model, namely a Two-Class Neural Network, with the LSTM approach, in terms of accuracy, precision, recall, and F1-score. In [18]: results_df = get_test_scores () results_df Out[18]: model id Accuracy Precision Recall F1-score model_type Two-Class Neural Network (best performing model from [1]) - 94.0 0.952381 0.8 0.869565 Conclusion In this Starter Kit, we covered the basics of using deep learning for remaining useful life prediction, which is one of the central topics in predictive maintenance. We illustrated the approach on the industrially-relevant use case of predicting the remaining useful lifetime of aircraft engines based on run-to-failure data. In doing so, we have shown: How to appropriately preprocess the data and construct training, validation and test sets to learn and evaluate a model that predicts whether an engine will fail within a certain number of operational cycles. How to use deep learning to construct a model for remaining useful life prediction. In particular, we applied a deep learning approach called long short-term memory (LSTM) networks which allows the construction of a predictive model without the time-consuming feature engineering step, which requires one to manually extract the characteristics from which the algorithm can learn. How to experimentally validate the resulting model and objectively compare it with other approaches. Comparing the above results on the test set, we see that the LSTM results are comparable to the results of the best performing model in [1] , with the advantage of not requiring a manual feature engineering step. Furthermore, it should be noted that the data set used here is very small and deep learning models are known to perform better with large datasets, so for a more fair comparison larger datasets should be used. The approach is simple and flexible enough to be applied to a wide range of RUL problems. Possible next steps One of the attention points when working with deep learning techniques is that it is important to tune the models to the right parameters, such as the window size. While the presented approach already gives promising results, quite some improvements are still possible, such as: Trying different window sizes. Trying different architectures with a different number of layers and nodes. Trying to tune the hyperparameters of the network. Trying to cast the RUL prediction problem as a regression or multi-class classification task instead of as a binary classification task. Trying on a larger dataset with more instances. Appendix A: A brief introduction to LSTM networks In the following, we will briefly explain the intuition behind an LSTM network, yet for a more detailed explanation we refer the interested reader to the original publication [2] or more intuitive descriptions (e.g. [3] , [4] ). A single layer LSTM consists of several blocks . Each block takes three inputs: 1) the input of the current time step, 2) the output from the previous LSTM block (of the previous time step) and 3) the \"memory\" of the previous block. Each block also provides output for the current timestep, and produces an updated memory of the current block. Therefore, a single block makes a decision by considering the current input, previous output and previous memory, generates a new output and alters its memory. This information flow is graphically represented in the following figure (taken from [4] ): The way the internal memory changes is controlled by different gates that control the information flow. Each gate computes a value between 0 and 1 using a logistic function to compute a value. The data is multiplied by this value to partially allow or deny information to flow into or out of the memory. An input gate controls the extent to which a new value flows into the memory. A forget gate controls the extent to which a value remains in memory. An output gate controls the extent to which the value in memory is used to compute the output activation of the block. An LSTM thus keeps two pieces of information as it propagates through time: A hidden state ; which is the memory the LSTM accumulates using its (forget, input, and output) gates through time, The previous time-step output . An important parameter when constructing LSTM networks is the size of the LSTM's hidden state. In the underlying computation framework TensorFlow that is used in this notebook, this parameter is set by the variable num_units . To make the name num_units more intuitive, you can think of it as the number of hidden units in the LSTM block, or the number of memory units in the block. The explaination above describes the structure of a single LSTM layer. However, an LSTM network usually consists of multiple layers , that can be stacked or merged. In case the layers are stacked, the output of the previous layer serves as input for the next layer. References [Predictive Maintenance Template](https://gallery.cortanaintelligence.com/Collection/Predictive-Maintenance-Template-3) Hochreiter, Sepp, and Jurgen Schmidhuber. \"Long short-term memory.\" Neural computation 9.8 (1997): 1735-1780. Olah, Christopher. [\"Understanding LSTM Networks\"](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) Yan, Shi. [\"Understanding LSTM and its diagrams\"](https://medium.com/@shiyan/understanding-lstm-and-its-diagrams-37e2f46f1714) MathWorks, [\"Models for Predicting Remaining Useful Life\"](https://nl.mathworks.com/help/predmaint/ug/models-for-predicting-remaining-useful-life.html) Additional information This notebook is based on the 'Deep Learning for Predictive Maintenance' notebook by Fidan Boylu Uz. MIT License Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.","tags":"Remaining Useful Life Prediction","url":"/Remaining Useful Life Prediction/2020-01-01-interactive-notebook.html","loc":"/Remaining Useful Life Prediction/2020-01-01-interactive-notebook.html"},{"title":"Interactive notebook","text":"In [1]: % load_ext autoreload % autoreload 2 # from IPython.core.display import display, HTML # deprecated from IPython.display import display , HTML display ( HTML ( \" \" )) In [2]: import numpy as np import pandas as pd import matplotlib import warnings warnings . filterwarnings ( 'ignore' ) pd . set_option ( 'display.float_format' , ' {:.3f} ' . format ) from support import read_consumption_data , read_climate_data , printmd from visualisations import run_app , plot_effects_resampling , plot_temperature_power_one_year , plot_yearly_data , \\ plot_weekly_data , plot_correlation , plot_consumption_with_holidays_weekends , plot_auto_correlation import holidays In [3]: data = read_consumption_data () ext_data = read_climate_data () 2022-06-14 16:26:14,045 [INFO ] Generating /Users/hcab/Documents/projects/elucidatalab.github/elucidatalab.starterkits/data/SK_1_3/household_power_consumption.txt Downloading file /Users/hcab/Documents/projects/elucidatalab.github/elucidatalab.starterkits/data/SK_1_3/household_power_consumption.txt.zip: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19.7M/19.7M [00:01<00:00, 17.9MiB/s] 2022-06-14 16:26:17,601 [INFO ] Using file: /Users/hcab/Documents/projects/elucidatalab.github/elucidatalab.starterkits/data/SK_1_3/household_power_consumption.txt 2022-06-14 16:26:37,612 [INFO ] Generating /Users/hcab/Documents/projects/elucidatalab.github/elucidatalab.starterkits/data/SK_1_3/hourly_temperature.csv Downloading file /Users/hcab/Documents/projects/elucidatalab.github/elucidatalab.starterkits/data/SK_1_3/hourly_temperature.csv.zip: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 144k/144k [00:00<00:00, 4.20MiB/s] 2022-06-14 16:26:38,469 [INFO ] Using file: /Users/hcab/Documents/projects/elucidatalab.github/elucidatalab.starterkits/data/SK_1_3/hourly_temperature.csv Starter Kit 1.3: Resource Demand Forecasting Business context Resource demand forecasting concerns accurately predicting the future need for a resource, typically using historical information. It is one of the most essential steps in resource demand management, i.e. ensuring sufficient resources are available to satisfy a fluctuating demand. Resource demand forecasting allows one to plan ahead and guarantee that sufficient resources are available when needed and to avoid costly countermeasures in case of shortage. A typical example of resource demand forecasting is provided by the energy sector. For energy suppliers it is more beneficial to buy electricity on the day-ahead market than on the spot market. Consequently, the more accurate the energy consumption can be predicted, the lower the cost. Given that more and more houses and appliances are enabled with a smart meter, an increasing amount of data becomes available that can enable and improve this prediction. Furthermore, these kinds of predictions also allow energy suppliers to better balance demand and supply, and helps them to ensure proper grid operation (and e.g., prepare for possible peak loads in winter time). This Starter Kit will focus energy consumption forecasting as a use case for illustrating how resource demand forecasting using historical data can be realized. Business goal The business goal related to this Starter Kit is forecasting the demand for a particular resource at a particular point in the future based on usage data of that resource that is available from the past. More specifically, we will illustrate a data-driven approach for forecasting the energy consumption of households. Application contexts Forecasting the demand of a particular resource can be useful for several purposes in a variety of industrial contexts: Forecast parking demand or traffic density in a particular neighbourhood in order to control traffic in intelligent ways (e.g. divert to alternative routes or parking spots) Predict battery consumption based on operating conditions in order to intelligently suggest recharging moments or switch to low-power mode Estimate the usage of consumables (e.g., ink, paper) based on product usage data in order to deliver new consumables exactly on time before stock breakage or avoid expensive storage of superfluous stock (e.g. in a manufacturing environment) ... Data requirements In order to realize the business goal, we need a dataset (time series) that includes: a value indicating the demand for the resource at each moment in time. the factors that influence the demand of the resource. These can be internal or external, for example: the energy consumption of a building is influenced by the outside temperature, the amount of people present in the building, etc. the consumption of raw materials is influenced by the type of product that is being produced, the production line settings, etc. the traffic density is influenced by the time of day, the presence of road works, the weather, etc. 'representative' historical data. The amount of data that is needed to make an accurate prediction is typically determined by the length of the temporal patterns in the data. For example, if the data expresses a yearly seasonality, historical data of multiple years is needed. data sampled with the 'appropriate' frequency (i.e., how many times per minute/hour/day you gather data points). This frequency is related to the required forecast frequency, e.g. it is impossible to forecast every 10 minutes when only 15-minute data is available. In some use cases, where resource demand forecasting concerns particular processes, (expert) knowledge on these processes is required (i.e. as labeled data) Starter Kit outline In this Starter Kit, we will illustrate a data-driven approach for forecasting the energy consumption of a household. As a dataset we will use the energy consumption of the household and climate data collected by a nearby weather station. We will use this dataset to illustrate: How to prepare your data for the analysis. We will describe how to handle the typical problems of raw data and how to perform data fusion when multiple sources of data are available. How to get insights from your dataset. We will present visual and numerical techniques in order to explore your data for identifying interesting patterns. How to transform the obtained insights into useful features for building forecasting models. How to compare correctly the performance of multiple forecasting models. Data loading and preprocessing For the household energy consumption use case we consider here, two types of data are relevant: Household data, which refers to the total household energy consumption along with the consumption of the individual energy-consuming appliances Climate data, which contains information of climate factors (e.g. temperature) that can influence on the energy consumption of households In the following subsections, we will load these datasets and perform some basic preprocessing, e.g. correctly handling missing values and fusing both datasets. Household data As a dataset we use the public energy consumption data set from the UCI repository . This dataset contains measurements of electric power consumption in one household, located near Paris, with a one-minute sampling rate over a period of almost 4 years. Different electrical quantities and sub-metering values are available, as can be seen in the table below: Date_Time : the date and time of day of the measurement Global_active_power : household global minute-averaged active power (in kilowatt) Global_reactive_power : household global minute-averaged reactive power (in kilowatt) Voltage : minute-averaged voltage (in volt) Global_intensity : household global minute-averaged current intensity (in ampere) Sub_metering_1 : energy sub-metering No. 1 (in watt-hour of active energy). It corresponds to the kitchen, containing mainly a dishwasher, an oven and a microwave (hot plates are not electric but gas powered). Sub_metering_2 : energy sub-metering No. 2 (in watt-hour of active energy). It corresponds to the laundry room, containing a washing-machine, a tumble-drier, a refrigerator and a light. Sub_metering_3 : energy sub-metering No. 3 (in watt-hour of active energy). It corresponds to an electric water-heater and an air-conditioner. In [4]: start_date = data . index . min () . date () end_date = data . index . max () . date () printmd ( f \"The dataset contains data from { start_date } to { end_date } .\" ) data . head () The dataset contains data from 2007-01-01 to 2010-11-26. Out[4]: Global_active_power Global_reactive_power Voltage Global_intensity Sub_metering_1 Sub_metering_2 Sub_metering_3 Date_Time 2007-01-01 00:00:00 2.580 0.136 241.970 10.600 0.000 0.000 0.000 2007-01-01 00:01:00 2.552 0.100 241.750 10.400 0.000 0.000 0.000 2007-01-01 00:02:00 2.550 0.100 241.640 10.400 0.000 0.000 0.000 2007-01-01 00:03:00 2.550 0.100 241.710 10.400 0.000 0.000 0.000 2007-01-01 00:04:00 2.554 0.100 241.980 10.400 0.000 0.000 0.000 From the table below, we can see that for all sensors we have 2.049.280 instances. This already indicates that there are no missing values. Besides this information, we do not spot immediately any strange insights, e.g. an anomaly in sensor data like negative consumed power. In [5]: data . describe () Out[5]: Global_active_power Global_reactive_power Voltage Global_intensity Sub_metering_1 Sub_metering_2 Sub_metering_3 count 2027288.000 2027288.000 2027288.000 2027288.000 2027288.000 2027288.000 2027288.000 mean 1.083 0.124 240.833 4.591 1.121 1.289 6.448 std 1.049 0.113 3.231 4.411 6.147 5.786 8.434 min 0.076 0.000 223.200 0.200 0.000 0.000 0.000 25% 0.308 0.048 238.990 1.400 0.000 0.000 0.000 50% 0.594 0.100 241.000 2.600 0.000 0.000 1.000 75% 1.520 0.194 242.870 6.400 0.000 1.000 17.000 max 11.122 1.390 254.150 48.400 88.000 80.000 31.000 For the purpose of this Starter Kit, only the Date_Time and the Global_active_power attributes will be considered from now onwards. In [6]: data = data [[ 'Global_active_power' ]] Climate data As an additional dataset, we use the climate information recorded close to the city of Paris, where the household is located, provided by the Wunderground website. It contains information on climate conditions for every 30 minutes, via the following attributes shown in the table below: time_UTC : the date and hour of the measurement temperature : temperature (degrees Celsius) dew_point : dew point (degrees Celsius) humidity : air humidity (percentage) sea_lvl_pressure : pressure at sea level (hPa) visibility : visibility (km) wind_dir_degrees : wind direction expressed in degrees (degrees) In [7]: ext_data . head ( 5 ) Out[7]: datetime 2007-01-01 00:00:00 14.210 2007-01-01 01:00:00 14.110 2007-01-01 02:00:00 13.910 2007-01-01 03:00:00 13.010 2007-01-01 04:00:00 11.910 Name: temperature, dtype: float64 From the statistics in the table below, we can see the data contains minimum values of -9999 for some variables. According to the documentation , values of -9999 or -999 signify missing values. We will replace these with the proper missing value NaN, as otherwise our algorithms would get confused. In [8]: ext_data . describe () Out[8]: count 34164.000 mean 12.407 std 13.868 min -8.710 25% 7.210 50% 12.310 75% 17.310 max 999.990 Name: temperature, dtype: float64 In [9]: ext_data . replace ( to_replace =- 9999 , value = np . nan , inplace = True ) ext_data . describe () Out[9]: count 34164.000 mean 12.407 std 13.868 min -8.710 25% 7.210 50% 12.310 75% 17.310 max 999.990 Name: temperature, dtype: float64 We can now notice that the temperature values seem to be more plausible. As expected, we also see that not all sensors have the same number of instances, meaning there are missing values. For the purpose of this Starter Kit, only the time_UTC and temperature attributes will be considered from now on. Data fusion The evolution of household energy consumption may be influenced by climatological factors, e.g. lower or higher temperatures can lead to a higher energy consumption for some appliances such as water-heaters and air-conditioners. To understand and analyze this relationship, we need to fuse the household data with the climate data. However, these datasets have different sampling time: the household data is sampled every minute, whereas the climate data is sampled every 30 minutes. The integration of these 2 datasets thus requires careful consideration. Two options are possible: we can either upsample the climate data or downsample the household data. We select the second option, as otherwise, we would need to find a way to impute missing data in the climate dataset, and it allows us to reduce the size of the dataset, which will reduce compute time required for the analysis. The effect of downsampling the household data can be seen below. The user can experiment how the window sizes of 30 minutes, 1 hour, or 4 hours affect the evolution of the global active power. In [10]: plot_effects_resampling ( data ) As we can see from the plot, resampling reduces the (small) peaks of the original signal. The effect becomes more pronounced for larger window sizes. A window size of 4 hours arguably removes too much variation. A window of 30 minutes or 1 hours on the other hand still captures the general evolution of the active power, which is exactly what is needed for characterizing the typical household consumption. Between 30 or 60min time windows, we decide to use a window size of 60min since this reduces the dataset and speeds up the computation time required later on for the analysis. To downsample the climate dataset, the mean of the values of the respective hour is used. To downsample the household dataset, the sum of each of the values for the corresponding hour is taken An excerpt of the downsampling of household dataset is shown in the table below. In [11]: ext_data_h = ext_data . resample ( '1H' ) . apply ( np . mean ) data_h = data . resample ( '1H' ) . agg ({ 'Global_active_power' : np . sum , }) data_h [: 3 ] Out[11]: Global_active_power Date_Time 2007-01-01 00:00:00 153.038 2007-01-01 01:00:00 151.404 2007-01-01 02:00:00 154.940 Since Global_reactive_power and Global_active_power are expressed in minute-averaged kW, we will change their units to Wh over an hour, such that this is in line with the resampling frequency. To this end, the following transformations are applied (the summing was already done in the previous step): \\begin{align*} \\mathit{globalactive}\\ Wh &= \\sum&#94;{60}_{min=1} 1000\\cdot\\mathit{globalactivepower}_{\\mathit{min}}\\ kW\\cdot \\frac{1}{60}\\ h\\\\ \\end{align*} The result of this transformation can be seen in the table below. In [12]: data_h [ 'Global_active_power' ] = data_h [ 'Global_active_power' ] . apply ( lambda x : x * 1000 / 60 ) data_h [: 3 ] Out[12]: Global_active_power Date_Time 2007-01-01 00:00:00 2550.633 2007-01-01 01:00:00 2523.400 2007-01-01 02:00:00 2582.333 Now we can finally merge the two datasets into an extended dataset that will be explored in the next section. In [13]: new_data = pd . concat (( data_h , ext_data_h ), axis = 1 , join = 'inner' ) new_data [: 3 ] Out[13]: Global_active_power temperature 2007-01-01 00:00:00 2550.633 14.210 2007-01-01 01:00:00 2523.400 14.110 2007-01-01 02:00:00 2582.333 13.910 Before proceeding, we inspect to what extent this extended dataset contains missing data. In [14]: new_data . isna () . mean () Out[14]: Global_active_power 0.000 temperature 0.025 dtype: float64 In case of too many missing values, we could consider data imputation. However, in this case, only the temperature has 0.01% of msising values, so there is no need for imputation. Data Exploration In this section, we explore the extended dataset in order to understand which factors influence the household energy consumption. Based on this exploration, we will later on identify which features are useful for training a machine learning algorithm to forecast energy consumption. There are several approaches to data exploration. In this section we will use: visual exploration by means of time plots statistical data exploration Visual Data Exploration Time plots are a common way for exploring time series data, such as the energy consumption that we consider here. We explore time plots in different time frames in order to highlight different aspects in the data: we start with a yearly plot, which allows us to identify global patterns. a monthly plot allows us to zoom in and understand relationships between energy consumption in different weeks and days a weekly plot allows us to zoom in even further and identify daily and hourly patterns Yearly patterns It is expected that energy consumption depends on the outdoor temperature, since in winter time heaters are used more frequently than in summer time for example. For this reason, we plot the evolution of the energy consumption along with the temperature. In the plot below, we resample the hourly data of both datasets to daily data, so that we don't get lost in details but can observe more global patterns. The user can toggle on or off the normalization of the data. The normalization that is applied is called min-max normalization and is explained in detail in SK 3.4. It applies the following formulate to the data in order to make sure all values are contained within the [0, 1] range: \\begin{align*} \\mathit{X_{scaled}} = \\frac{X - X_{min}}{X_{max} - {X_{min}}}\\\\ \\end{align*} The user is invited to verify, without selecting the option normalization, whether of temperature and global active power have the same trend. The reader can then repeat the analysis using the option normalization. In [15]: plot_temperature_power_one_year ( new_data ) It should be clear that without normalization, it is difficult to compare the temperature with the global active power because both use different ranges, and the range of the global active power dominates the range of the temperature. With normalization however, the plot above reveals interesting insights. Both the temperature and the global active power follow a seasonal trend. Obviously, the temperature is lower in winter and higher in summer, whereas the reverse is true for the global active power. This is to be expected: energy consumption is higher during winter time. In more formal terms, we can observe that the temperate and global active power exhibit an inverse correlation: when one increases the other decreases, and vice versa. Monthly patterns Let's now see if there is a monthly recurring pattern as well. In the following figure, the active power of the selected year(s) is shown. The user can change the resampling rate to make the insights more clear to them. To make the plot more clear, the user can (de)selected the years they want on the right of the figure. In [16]: plot_yearly_data ( new_data ) While we can again clearly see the seasonal effect, there is no clear pattern recurring every month. One interesting thing that can be seen is that strong dips in power consumption tend to happen in the same time in February and November in 2007 and 2008. Most likely, these are holidays which tend to happen at the same time for a fixed household. However, these dips are not present at the same time for 2009. Weekly patterns In this section we further explore the evolution of the energy consumption in a more fine grained time frame. We want to verify whether during a week, the energy consumption exhibits the same patterns. In the plot below, the user can inspect and compare the energy consumption in December of different years. We mark weekends in yellow and Christmas day in orange. We invite the user to see for each year if they can notice a difference between week days and weekends and if there are any changes around the Holidays. In [17]: plot_consumption_with_holidays_weekends ( new_data ) What should be obvious from exploring this plot is the existence of a daily pattern for energy consumption dependent on the day of the week: in most weekend days of the three years, we can observe slightly higher energy consumption than during weekdays. This can depend on the common habits of working at the office or children being at school. The effect of the holidays can also be seen. On Christmas day in 2007, the power consumption is slightly higher than other typical weekdays, but similar values can be seen e.g. exactly one week before. So the holidays don't have a strong impact here. In 2008 on the other hand, the power consumption is lower, indicating that perhaps the household celebrated outside their house. In 2009, there is a peak in consumption on Christmas eve. We can conclude that whether the day is a weekday or in the weekend or a holiday has an influence on the expected power consumption. Daily patterns Next to yearly and monthly patterns, we are also interested in daily patterns. Below, we plot the evolution of the global active power for one week (The first week of December 2009). We can see how the power consumption follows a daily recurring pattern, with peaks in the morning and evening and troughs at night and midday. In fact, this pattern is also already present in the previous plot, where daily morning and evening peaks can also be observed. During the night and the day, we still see low amounts of power usage, most likely due to appliances like refrigerators and heating. In [18]: plot_weekly_data ( new_data ) Statistical Data Exploration The visualizations above highlighted interesting patterns in the energy consumption of the household that can be exploited for feature extraction. In this section we use statistical methods to verify, and eventually quantify those observations. Autocorrelation In order to find repetitive patterns in time series data, we can use autocorrelation , which provides the correlation between the time series and a delayed copy of itself. In order to investigate this autocorrelation, the plot below visualizes the autocorrelation of the global active power with time lags of 1 day, 2 days, etc. Note that we do not show the results of a delay of 0 days, since the correlation with the exact same day would trivially be 1. In [19]: plot_auto_correlation ( new_data ) The plot above reveals that a clear peak occurs after 1 day, indicating the consumption of a day is highly correlated with the consumption of the previous day another clear peak occurs every 7 days, which confirms the existence of a weekly pattern, i.e. the consumption of a day is highly correlated with the consumption of the same day a week earlier Correlation As visually observed above, the temperature influences the energy consumption of a household. In order to verify the strength of this relationship, we can compute the correlation between these two variables. For this purpose, we compute Pearson's correlation. The plot below shows again both temperature and consumption and allows to investigate their relationship. This relationship is influenced by the frequency of the dataset, e.g. temperature changes are typically less drastic than energy consumption changes. We invite the user to experiment with the different resampling periods, to understand how that changes the correlation and what frequency leads to the highest correlation. In [20]: plot_correlation ( new_data ) What should be obvious from the above is that there is a negative correlation between temperature and energy consumption, meaning the consumption rises when temperature drops and vice versa. The larger the resampling window is chosen, the higher is the correlation. This is because with larger windows, we smooth out more the short-term fluctuations, leaving only the seasonal pattern described above. Feature engineering In this section, we will extract several features from the extended dataset that can be used for forecasting energy consumption. A feature is an individual measurable property of the phenomenon being observed. Choosing informative, discriminating and independent features is a crucial step for building effective machine learning algorithms. Based on the previous visual and statistical observation, we will consider the following features to predict the energy consumption for the hour ahead: energy consumption of the day before energy consumption of the 7 days before the hour of the day, which will allow to distinguish between different periods of the day (i.e. night, morning, afternoon, evening) the day of the week, which will allow to distinguish between weekdays and weekend days whether the day is a holiday or not the month of the year, which will allow to distinguish between yearly seasons the temperature In [21]: new_feats = new_data [[ 'Global_active_power' ]] . rename ( columns = ({ 'Global_active_power' : 'consumption' })) new_feats [ 'Pastday' ] = new_feats [ 'consumption' ] . shift ( 1 , freq = 'D' ) new_feats [ 'Pastweek' ] = new_feats [ 'consumption' ] . shift ( 7 , freq = 'D' ) new_feats [ 'Hour' ] = new_feats . index . hour new_feats [ 'Weekday' ] = new_feats . index . dayofweek new_feats [ 'Month' ] = new_feats . index . month new_feats [ 'Holiday' ] = [ 1 if ( date in holidays . France ()) else 0 for date in new_feats . index ] new_feats [ 'Temperature' ] = new_data [ 'temperature' ] new_feats = new_feats . dropna () Modeling This section allows the user to discover the most important factors for training a machine learning model. More specifically, the user will get insights on the influence of the training strategy , type of machine learning model and effect of data normalization on model performance. To evaluate the performance of the models the mean absolute error (MAE) is used (a metric commonly used in literature for this purpose). This metric quantifies to what extent the model forecasts are close to the real values. As the name suggests, the mean absolute error is an average of the absolute errors. It holds that the lower the MAE of the model, the better its performance. This section ends with an interactive application where the user can test all options in order to find the one that leads the model to achieve the best performance. Training strategies The training data influences models performances. For this experiment, the following options are available: Use 1 month before the test month as training data. In this experiment, several models are trained for the predictions of each individual month. Each model is trained on one month before the month we are making predictions for. One month gap is introduced between the training and the test month (this is done to avoid that the last day of the training set is also included in the first day of the test set). For example, the models that will make the predictions for April 2008 will be trained on the data from February 2008. Use 6 months before the test month as training data. Similar to the above experiment, but with 6 months of training data. It still includes a 1 month gap. Use 1 year before the test month as training data. Similar to the above experiment, but with 1 year of training data, still include a 1 month gap. Use 1 month the year before as training data. Similar to strategy number 1, but with an 11 months gap. This way, the training data and test data are taken from the same month, one year apart. Use all months before the test month as training data. For each test month, train a model using all the data prior to this (including a 1 month gap). This strategy simulates the scenario where the model is retrained as new data comes in. Since this requires training several models based on potentially large training sets, this might require a certain computational time. Use train-test split . Train on the first year of data and make predictions on the rest. This strategy is commonly used for training these kinds of models. Notice that for the benchmark model, there is no training phase. Machine learning models There is a plethora of models that can be used for forecasting. In this notebook, we decide to use two models that are commonly used in literature for the task at hand: Random Forest Regression (RFR) and Support Vector Regression (SVR) . Besides these models, we will also use a simple benchmark model that we use to compare the performance of the other models against. For the prediction of the energy of a certain day, this benchmark model simply takes the energy consumption of the previous day at the same time. We will use these models to forecast the energy consumption based on the past energy measurements (see training strategies). Normalization Before being used for training, data may need to be normalized, i.e. rescaled so that all input and output values are between 0 and 1. This is indeed a requirement for the correct training of some machine learning models. It is left to the user the exercise of discovering which model requires the normalization of the data. In [22]: from IPython.utils import io PORT = '8095' # in AWS use ports 8050-8060 mode = 'external' #external if mode == 'inline' : run_app ( new_feats , mode = 'inline' , port = PORT ) else : with io . capture_output () as captured : run_app ( new_feats , mode = 'external' , port = PORT , host = '0.0.0.0' ) HOST = '0.0.0.0' # in AWS replace here by public IP print ( f 'Dash app running on http:// { HOST } : { PORT } ' ) Dash app running on http://0.0.0.0:8095 The mean absolute error of each experiment that was run is reported in the table above. Insights from the experiments These are the basic insights the user could have noticed when trying different models, parameters and training strategies. Effect of the training strategy For the SVR, we see that the standard train-test split has the best performance, although using all months and using a one year window before the test month as training data both have a similar performance. For the RFR on the other hand, we clearly see that using a one year window before the test month as training data leads to better performance than the other training strategies. We note that for both this \"one year window\" strategy and the standard train-test split, the training data is one year. The difference however, is that for the former the training set changes, while for the latter it is fixed (using all the data from 2017). A general trend that can be seen is that the performance increases as a larger training set is used. We therefore urge the user to look at the predictions made using the training strategy where all months before the test month are used as training data: we can expect that the predictions for the later months will be better than for the first months since more training data was used. However, this is hard to see by eye in the prediction plot directly. Effect of the chosen model We can see that, for most parameter choices, both the SVR and RFR can outperform the simple benchmark model we set up. The RFR outperforms the SVR for most (if not all) parameter choices and training strategies. This shows the importance of knowing which model is best suited for your problem and testing different ones. Effect of the chosen parameters The search for the optimal set of hyperparameters for your machine learning model is a non-trivial task, since there might be a strong interplay between the parameters so we can not do the optimal parameter search one by one. Here, we let the user try a pre-set number of choices, but more automatic and smart search algorithms exist to tackle this problem. We found that for the RFR, the higher the number of estimators, the better the results, but at a greater computational cost and with diminishing returns. We see that 50 estimators and leaving the max depth at 10 is a good choice. For the SVR, $C=10$ and $\\gamma=0.01$ work well. Effect of normalization Normalizing the data, rescaling it so that the input and output variables all have values within a similar range (in this case, between 0 and 1) is a common step when setting up machine learning models. This is because, depending on the model, they work much better with values of the order 1. We see that normalization indeed greatly improves the predictions for the SVR, but has very little influence on the RFR. Indeed, algorithms such as random forest and decision tree are not influenced by the scale of the input variables. Conclusion In this Starter Kit we demonstrated forecasting the demand for a particular resource at a particular point in the future, illustrated in the case of energy demand forecasting. To this end, we first analysed the business problem, preprocessed the data, visually and numerically explored it to eventually illustrate the use of different models ( Random Forest Regression, Support Vector Regression and benchmark) to perform the actual forecasting with different amounts of available historical data. From the analysis, it may be clear that achieving good prediction results depend on different factors (appropriate data preprocessing, the amount of available training data, algorithm parametrisation, etc.). Additional Information Contributions by Mathias Verbeke, Wannes Meert, Vincent Vercruyssen, Alessandro Murgia, Tom Tourw√©, and Robbert Verbeke. This Starter Kit was developed in the context of the EluciDATA project ( http://www.elucidata.be ). For more information, please contact info@elucidata.be .","tags":"Resource demand Forecasting","url":"/Resource demand Forecasting/2020-01-01-interactive-notebook.html","loc":"/Resource demand Forecasting/2020-01-01-interactive-notebook.html"},{"title":"Interactive notebook","text":"In [1]: % load_ext autoreload % autoreload 2 # from IPython.core.display import display, HTML # deprecated from IPython.display import display , HTML display ( HTML ( \" \" )) In [2]: import numpy as np import pandas as pd import matplotlib import warnings warnings . filterwarnings ( 'ignore' ) pd . set_option ( 'display.float_format' , ' {:.3f} ' . format ) from support import read_consumption_data , read_climate_data , printmd from visualisations import run_app , plot_effects_resampling , plot_temperature_power_one_year , plot_yearly_data , \\ plot_weekly_data , plot_correlation , plot_consumption_with_holidays_weekends , plot_auto_correlation import holidays In [3]: data = read_consumption_data () ext_data = read_climate_data () 2022-06-14 16:26:14,045 [INFO ] Generating /Users/hcab/Documents/projects/elucidatalab.github/elucidatalab.starterkits/data/SK_1_3/household_power_consumption.txt Downloading file /Users/hcab/Documents/projects/elucidatalab.github/elucidatalab.starterkits/data/SK_1_3/household_power_consumption.txt.zip: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19.7M/19.7M [00:01<00:00, 17.9MiB/s] 2022-06-14 16:26:17,601 [INFO ] Using file: /Users/hcab/Documents/projects/elucidatalab.github/elucidatalab.starterkits/data/SK_1_3/household_power_consumption.txt 2022-06-14 16:26:37,612 [INFO ] Generating /Users/hcab/Documents/projects/elucidatalab.github/elucidatalab.starterkits/data/SK_1_3/hourly_temperature.csv Downloading file /Users/hcab/Documents/projects/elucidatalab.github/elucidatalab.starterkits/data/SK_1_3/hourly_temperature.csv.zip: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 144k/144k [00:00<00:00, 4.20MiB/s] 2022-06-14 16:26:38,469 [INFO ] Using file: /Users/hcab/Documents/projects/elucidatalab.github/elucidatalab.starterkits/data/SK_1_3/hourly_temperature.csv Starter Kit 1.3: Resource Demand Forecasting Business context Resource demand forecasting concerns accurately predicting the future need for a resource, typically using historical information. It is one of the most essential steps in resource demand management, i.e. ensuring sufficient resources are available to satisfy a fluctuating demand. Resource demand forecasting allows one to plan ahead and guarantee that sufficient resources are available when needed and to avoid costly countermeasures in case of shortage. A typical example of resource demand forecasting is provided by the energy sector. For energy suppliers it is more beneficial to buy electricity on the day-ahead market than on the spot market. Consequently, the more accurate the energy consumption can be predicted, the lower the cost. Given that more and more houses and appliances are enabled with a smart meter, an increasing amount of data becomes available that can enable and improve this prediction. Furthermore, these kinds of predictions also allow energy suppliers to better balance demand and supply, and helps them to ensure proper grid operation (and e.g., prepare for possible peak loads in winter time). This Starter Kit will focus energy consumption forecasting as a use case for illustrating how resource demand forecasting using historical data can be realized. Business goal The business goal related to this Starter Kit is forecasting the demand for a particular resource at a particular point in the future based on usage data of that resource that is available from the past. More specifically, we will illustrate a data-driven approach for forecasting the energy consumption of households. Application contexts Forecasting the demand of a particular resource can be useful for several purposes in a variety of industrial contexts: Forecast parking demand or traffic density in a particular neighbourhood in order to control traffic in intelligent ways (e.g. divert to alternative routes or parking spots) Predict battery consumption based on operating conditions in order to intelligently suggest recharging moments or switch to low-power mode Estimate the usage of consumables (e.g., ink, paper) based on product usage data in order to deliver new consumables exactly on time before stock breakage or avoid expensive storage of superfluous stock (e.g. in a manufacturing environment) ... Data requirements In order to realize the business goal, we need a dataset (time series) that includes: a value indicating the demand for the resource at each moment in time. the factors that influence the demand of the resource. These can be internal or external, for example: the energy consumption of a building is influenced by the outside temperature, the amount of people present in the building, etc. the consumption of raw materials is influenced by the type of product that is being produced, the production line settings, etc. the traffic density is influenced by the time of day, the presence of road works, the weather, etc. 'representative' historical data. The amount of data that is needed to make an accurate prediction is typically determined by the length of the temporal patterns in the data. For example, if the data expresses a yearly seasonality, historical data of multiple years is needed. data sampled with the 'appropriate' frequency (i.e., how many times per minute/hour/day you gather data points). This frequency is related to the required forecast frequency, e.g. it is impossible to forecast every 10 minutes when only 15-minute data is available. In some use cases, where resource demand forecasting concerns particular processes, (expert) knowledge on these processes is required (i.e. as labeled data) Starter Kit outline In this Starter Kit, we will illustrate a data-driven approach for forecasting the energy consumption of a household. As a dataset we will use the energy consumption of the household and climate data collected by a nearby weather station. We will use this dataset to illustrate: How to prepare your data for the analysis. We will describe how to handle the typical problems of raw data and how to perform data fusion when multiple sources of data are available. How to get insights from your dataset. We will present visual and numerical techniques in order to explore your data for identifying interesting patterns. How to transform the obtained insights into useful features for building forecasting models. How to compare correctly the performance of multiple forecasting models. Data loading and preprocessing For the household energy consumption use case we consider here, two types of data are relevant: Household data, which refers to the total household energy consumption along with the consumption of the individual energy-consuming appliances Climate data, which contains information of climate factors (e.g. temperature) that can influence on the energy consumption of households In the following subsections, we will load these datasets and perform some basic preprocessing, e.g. correctly handling missing values and fusing both datasets. Household data As a dataset we use the public energy consumption data set from the UCI repository . This dataset contains measurements of electric power consumption in one household, located near Paris, with a one-minute sampling rate over a period of almost 4 years. Different electrical quantities and sub-metering values are available, as can be seen in the table below: Date_Time : the date and time of day of the measurement Global_active_power : household global minute-averaged active power (in kilowatt) Global_reactive_power : household global minute-averaged reactive power (in kilowatt) Voltage : minute-averaged voltage (in volt) Global_intensity : household global minute-averaged current intensity (in ampere) Sub_metering_1 : energy sub-metering No. 1 (in watt-hour of active energy). It corresponds to the kitchen, containing mainly a dishwasher, an oven and a microwave (hot plates are not electric but gas powered). Sub_metering_2 : energy sub-metering No. 2 (in watt-hour of active energy). It corresponds to the laundry room, containing a washing-machine, a tumble-drier, a refrigerator and a light. Sub_metering_3 : energy sub-metering No. 3 (in watt-hour of active energy). It corresponds to an electric water-heater and an air-conditioner. In [4]: start_date = data . index . min () . date () end_date = data . index . max () . date () printmd ( f \"The dataset contains data from { start_date } to { end_date } .\" ) data . head () The dataset contains data from 2007-01-01 to 2010-11-26. Out[4]: Global_active_power Global_reactive_power Voltage Global_intensity Sub_metering_1 Sub_metering_2 Sub_metering_3 Date_Time 2007-01-01 00:00:00 2.580 0.136 241.970 10.600 0.000 0.000 0.000 2007-01-01 00:01:00 2.552 0.100 241.750 10.400 0.000 0.000 0.000 2007-01-01 00:02:00 2.550 0.100 241.640 10.400 0.000 0.000 0.000 2007-01-01 00:03:00 2.550 0.100 241.710 10.400 0.000 0.000 0.000 2007-01-01 00:04:00 2.554 0.100 241.980 10.400 0.000 0.000 0.000 From the table below, we can see that for all sensors we have 2.049.280 instances. This already indicates that there are no missing values. Besides this information, we do not spot immediately any strange insights, e.g. an anomaly in sensor data like negative consumed power. In [5]: data . describe () Out[5]: Global_active_power Global_reactive_power Voltage Global_intensity Sub_metering_1 Sub_metering_2 Sub_metering_3 count 2027288.000 2027288.000 2027288.000 2027288.000 2027288.000 2027288.000 2027288.000 mean 1.083 0.124 240.833 4.591 1.121 1.289 6.448 std 1.049 0.113 3.231 4.411 6.147 5.786 8.434 min 0.076 0.000 223.200 0.200 0.000 0.000 0.000 25% 0.308 0.048 238.990 1.400 0.000 0.000 0.000 50% 0.594 0.100 241.000 2.600 0.000 0.000 1.000 75% 1.520 0.194 242.870 6.400 0.000 1.000 17.000 max 11.122 1.390 254.150 48.400 88.000 80.000 31.000 For the purpose of this Starter Kit, only the Date_Time and the Global_active_power attributes will be considered from now onwards. In [6]: data = data [[ 'Global_active_power' ]] Climate data As an additional dataset, we use the climate information recorded close to the city of Paris, where the household is located, provided by the Wunderground website. It contains information on climate conditions for every 30 minutes, via the following attributes shown in the table below: time_UTC : the date and hour of the measurement temperature : temperature (degrees Celsius) dew_point : dew point (degrees Celsius) humidity : air humidity (percentage) sea_lvl_pressure : pressure at sea level (hPa) visibility : visibility (km) wind_dir_degrees : wind direction expressed in degrees (degrees) In [7]: ext_data . head ( 5 ) Out[7]: datetime 2007-01-01 00:00:00 14.210 2007-01-01 01:00:00 14.110 2007-01-01 02:00:00 13.910 2007-01-01 03:00:00 13.010 2007-01-01 04:00:00 11.910 Name: temperature, dtype: float64 From the statistics in the table below, we can see the data contains minimum values of -9999 for some variables. According to the documentation , values of -9999 or -999 signify missing values. We will replace these with the proper missing value NaN, as otherwise our algorithms would get confused. In [8]: ext_data . describe () Out[8]: count 34164.000 mean 12.407 std 13.868 min -8.710 25% 7.210 50% 12.310 75% 17.310 max 999.990 Name: temperature, dtype: float64 In [9]: ext_data . replace ( to_replace =- 9999 , value = np . nan , inplace = True ) ext_data . describe () Out[9]: count 34164.000 mean 12.407 std 13.868 min -8.710 25% 7.210 50% 12.310 75% 17.310 max 999.990 Name: temperature, dtype: float64 We can now notice that the temperature values seem to be more plausible. As expected, we also see that not all sensors have the same number of instances, meaning there are missing values. For the purpose of this Starter Kit, only the time_UTC and temperature attributes will be considered from now on. Data fusion The evolution of household energy consumption may be influenced by climatological factors, e.g. lower or higher temperatures can lead to a higher energy consumption for some appliances such as water-heaters and air-conditioners. To understand and analyze this relationship, we need to fuse the household data with the climate data. However, these datasets have different sampling time: the household data is sampled every minute, whereas the climate data is sampled every 30 minutes. The integration of these 2 datasets thus requires careful consideration. Two options are possible: we can either upsample the climate data or downsample the household data. We select the second option, as otherwise, we would need to find a way to impute missing data in the climate dataset, and it allows us to reduce the size of the dataset, which will reduce compute time required for the analysis. The effect of downsampling the household data can be seen below. The user can experiment how the window sizes of 30 minutes, 1 hour, or 4 hours affect the evolution of the global active power. In [10]: plot_effects_resampling ( data ) As we can see from the plot, resampling reduces the (small) peaks of the original signal. The effect becomes more pronounced for larger window sizes. A window size of 4 hours arguably removes too much variation. A window of 30 minutes or 1 hours on the other hand still captures the general evolution of the active power, which is exactly what is needed for characterizing the typical household consumption. Between 30 or 60min time windows, we decide to use a window size of 60min since this reduces the dataset and speeds up the computation time required later on for the analysis. To downsample the climate dataset, the mean of the values of the respective hour is used. To downsample the household dataset, the sum of each of the values for the corresponding hour is taken An excerpt of the downsampling of household dataset is shown in the table below. In [11]: ext_data_h = ext_data . resample ( '1H' ) . apply ( np . mean ) data_h = data . resample ( '1H' ) . agg ({ 'Global_active_power' : np . sum , }) data_h [: 3 ] Out[11]: Global_active_power Date_Time 2007-01-01 00:00:00 153.038 2007-01-01 01:00:00 151.404 2007-01-01 02:00:00 154.940 Since Global_reactive_power and Global_active_power are expressed in minute-averaged kW, we will change their units to Wh over an hour, such that this is in line with the resampling frequency. To this end, the following transformations are applied (the summing was already done in the previous step): \\begin{align*} \\mathit{globalactive}\\ Wh &= \\sum&#94;{60}_{min=1} 1000\\cdot\\mathit{globalactivepower}_{\\mathit{min}}\\ kW\\cdot \\frac{1}{60}\\ h\\\\ \\end{align*} The result of this transformation can be seen in the table below. In [12]: data_h [ 'Global_active_power' ] = data_h [ 'Global_active_power' ] . apply ( lambda x : x * 1000 / 60 ) data_h [: 3 ] Out[12]: Global_active_power Date_Time 2007-01-01 00:00:00 2550.633 2007-01-01 01:00:00 2523.400 2007-01-01 02:00:00 2582.333 Now we can finally merge the two datasets into an extended dataset that will be explored in the next section. In [13]: new_data = pd . concat (( data_h , ext_data_h ), axis = 1 , join = 'inner' ) new_data [: 3 ] Out[13]: Global_active_power temperature 2007-01-01 00:00:00 2550.633 14.210 2007-01-01 01:00:00 2523.400 14.110 2007-01-01 02:00:00 2582.333 13.910 Before proceeding, we inspect to what extent this extended dataset contains missing data. In [14]: new_data . isna () . mean () Out[14]: Global_active_power 0.000 temperature 0.025 dtype: float64 In case of too many missing values, we could consider data imputation. However, in this case, only the temperature has 0.01% of msising values, so there is no need for imputation. Data Exploration In this section, we explore the extended dataset in order to understand which factors influence the household energy consumption. Based on this exploration, we will later on identify which features are useful for training a machine learning algorithm to forecast energy consumption. There are several approaches to data exploration. In this section we will use: visual exploration by means of time plots statistical data exploration Visual Data Exploration Time plots are a common way for exploring time series data, such as the energy consumption that we consider here. We explore time plots in different time frames in order to highlight different aspects in the data: we start with a yearly plot, which allows us to identify global patterns. a monthly plot allows us to zoom in and understand relationships between energy consumption in different weeks and days a weekly plot allows us to zoom in even further and identify daily and hourly patterns Yearly patterns It is expected that energy consumption depends on the outdoor temperature, since in winter time heaters are used more frequently than in summer time for example. For this reason, we plot the evolution of the energy consumption along with the temperature. In the plot below, we resample the hourly data of both datasets to daily data, so that we don't get lost in details but can observe more global patterns. The user can toggle on or off the normalization of the data. The normalization that is applied is called min-max normalization and is explained in detail in SK 3.4. It applies the following formulate to the data in order to make sure all values are contained within the [0, 1] range: \\begin{align*} \\mathit{X_{scaled}} = \\frac{X - X_{min}}{X_{max} - {X_{min}}}\\\\ \\end{align*} The user is invited to verify, without selecting the option normalization, whether of temperature and global active power have the same trend. The reader can then repeat the analysis using the option normalization. In [15]: plot_temperature_power_one_year ( new_data ) It should be clear that without normalization, it is difficult to compare the temperature with the global active power because both use different ranges, and the range of the global active power dominates the range of the temperature. With normalization however, the plot above reveals interesting insights. Both the temperature and the global active power follow a seasonal trend. Obviously, the temperature is lower in winter and higher in summer, whereas the reverse is true for the global active power. This is to be expected: energy consumption is higher during winter time. In more formal terms, we can observe that the temperate and global active power exhibit an inverse correlation: when one increases the other decreases, and vice versa. Monthly patterns Let's now see if there is a monthly recurring pattern as well. In the following figure, the active power of the selected year(s) is shown. The user can change the resampling rate to make the insights more clear to them. To make the plot more clear, the user can (de)selected the years they want on the right of the figure. In [16]: plot_yearly_data ( new_data ) While we can again clearly see the seasonal effect, there is no clear pattern recurring every month. One interesting thing that can be seen is that strong dips in power consumption tend to happen in the same time in February and November in 2007 and 2008. Most likely, these are holidays which tend to happen at the same time for a fixed household. However, these dips are not present at the same time for 2009. Weekly patterns In this section we further explore the evolution of the energy consumption in a more fine grained time frame. We want to verify whether during a week, the energy consumption exhibits the same patterns. In the plot below, the user can inspect and compare the energy consumption in December of different years. We mark weekends in yellow and Christmas day in orange. We invite the user to see for each year if they can notice a difference between week days and weekends and if there are any changes around the Holidays. In [17]: plot_consumption_with_holidays_weekends ( new_data ) What should be obvious from exploring this plot is the existence of a daily pattern for energy consumption dependent on the day of the week: in most weekend days of the three years, we can observe slightly higher energy consumption than during weekdays. This can depend on the common habits of working at the office or children being at school. The effect of the holidays can also be seen. On Christmas day in 2007, the power consumption is slightly higher than other typical weekdays, but similar values can be seen e.g. exactly one week before. So the holidays don't have a strong impact here. In 2008 on the other hand, the power consumption is lower, indicating that perhaps the household celebrated outside their house. In 2009, there is a peak in consumption on Christmas eve. We can conclude that whether the day is a weekday or in the weekend or a holiday has an influence on the expected power consumption. Daily patterns Next to yearly and monthly patterns, we are also interested in daily patterns. Below, we plot the evolution of the global active power for one week (The first week of December 2009). We can see how the power consumption follows a daily recurring pattern, with peaks in the morning and evening and troughs at night and midday. In fact, this pattern is also already present in the previous plot, where daily morning and evening peaks can also be observed. During the night and the day, we still see low amounts of power usage, most likely due to appliances like refrigerators and heating. In [18]: plot_weekly_data ( new_data ) Statistical Data Exploration The visualizations above highlighted interesting patterns in the energy consumption of the household that can be exploited for feature extraction. In this section we use statistical methods to verify, and eventually quantify those observations. Autocorrelation In order to find repetitive patterns in time series data, we can use autocorrelation , which provides the correlation between the time series and a delayed copy of itself. In order to investigate this autocorrelation, the plot below visualizes the autocorrelation of the global active power with time lags of 1 day, 2 days, etc. Note that we do not show the results of a delay of 0 days, since the correlation with the exact same day would trivially be 1. In [19]: plot_auto_correlation ( new_data ) The plot above reveals that a clear peak occurs after 1 day, indicating the consumption of a day is highly correlated with the consumption of the previous day another clear peak occurs every 7 days, which confirms the existence of a weekly pattern, i.e. the consumption of a day is highly correlated with the consumption of the same day a week earlier Correlation As visually observed above, the temperature influences the energy consumption of a household. In order to verify the strength of this relationship, we can compute the correlation between these two variables. For this purpose, we compute Pearson's correlation. The plot below shows again both temperature and consumption and allows to investigate their relationship. This relationship is influenced by the frequency of the dataset, e.g. temperature changes are typically less drastic than energy consumption changes. We invite the user to experiment with the different resampling periods, to understand how that changes the correlation and what frequency leads to the highest correlation. In [20]: plot_correlation ( new_data ) What should be obvious from the above is that there is a negative correlation between temperature and energy consumption, meaning the consumption rises when temperature drops and vice versa. The larger the resampling window is chosen, the higher is the correlation. This is because with larger windows, we smooth out more the short-term fluctuations, leaving only the seasonal pattern described above. Feature engineering In this section, we will extract several features from the extended dataset that can be used for forecasting energy consumption. A feature is an individual measurable property of the phenomenon being observed. Choosing informative, discriminating and independent features is a crucial step for building effective machine learning algorithms. Based on the previous visual and statistical observation, we will consider the following features to predict the energy consumption for the hour ahead: energy consumption of the day before energy consumption of the 7 days before the hour of the day, which will allow to distinguish between different periods of the day (i.e. night, morning, afternoon, evening) the day of the week, which will allow to distinguish between weekdays and weekend days whether the day is a holiday or not the month of the year, which will allow to distinguish between yearly seasons the temperature In [21]: new_feats = new_data [[ 'Global_active_power' ]] . rename ( columns = ({ 'Global_active_power' : 'consumption' })) new_feats [ 'Pastday' ] = new_feats [ 'consumption' ] . shift ( 1 , freq = 'D' ) new_feats [ 'Pastweek' ] = new_feats [ 'consumption' ] . shift ( 7 , freq = 'D' ) new_feats [ 'Hour' ] = new_feats . index . hour new_feats [ 'Weekday' ] = new_feats . index . dayofweek new_feats [ 'Month' ] = new_feats . index . month new_feats [ 'Holiday' ] = [ 1 if ( date in holidays . France ()) else 0 for date in new_feats . index ] new_feats [ 'Temperature' ] = new_data [ 'temperature' ] new_feats = new_feats . dropna () Modeling This section allows the user to discover the most important factors for training a machine learning model. More specifically, the user will get insights on the influence of the training strategy , type of machine learning model and effect of data normalization on model performance. To evaluate the performance of the models the mean absolute error (MAE) is used (a metric commonly used in literature for this purpose). This metric quantifies to what extent the model forecasts are close to the real values. As the name suggests, the mean absolute error is an average of the absolute errors. It holds that the lower the MAE of the model, the better its performance. This section ends with an interactive application where the user can test all options in order to find the one that leads the model to achieve the best performance. Training strategies The training data influences models performances. For this experiment, the following options are available: Use 1 month before the test month as training data. In this experiment, several models are trained for the predictions of each individual month. Each model is trained on one month before the month we are making predictions for. One month gap is introduced between the training and the test month (this is done to avoid that the last day of the training set is also included in the first day of the test set). For example, the models that will make the predictions for April 2008 will be trained on the data from February 2008. Use 6 months before the test month as training data. Similar to the above experiment, but with 6 months of training data. It still includes a 1 month gap. Use 1 year before the test month as training data. Similar to the above experiment, but with 1 year of training data, still include a 1 month gap. Use 1 month the year before as training data. Similar to strategy number 1, but with an 11 months gap. This way, the training data and test data are taken from the same month, one year apart. Use all months before the test month as training data. For each test month, train a model using all the data prior to this (including a 1 month gap). This strategy simulates the scenario where the model is retrained as new data comes in. Since this requires training several models based on potentially large training sets, this might require a certain computational time. Use train-test split . Train on the first year of data and make predictions on the rest. This strategy is commonly used for training these kinds of models. Notice that for the benchmark model, there is no training phase. Machine learning models There is a plethora of models that can be used for forecasting. In this notebook, we decide to use two models that are commonly used in literature for the task at hand: Random Forest Regression (RFR) and Support Vector Regression (SVR) . Besides these models, we will also use a simple benchmark model that we use to compare the performance of the other models against. For the prediction of the energy of a certain day, this benchmark model simply takes the energy consumption of the previous day at the same time. We will use these models to forecast the energy consumption based on the past energy measurements (see training strategies). Normalization Before being used for training, data may need to be normalized, i.e. rescaled so that all input and output values are between 0 and 1. This is indeed a requirement for the correct training of some machine learning models. It is left to the user the exercise of discovering which model requires the normalization of the data. In [22]: from IPython.utils import io PORT = '8095' # in AWS use ports 8050-8060 mode = 'external' #external if mode == 'inline' : run_app ( new_feats , mode = 'inline' , port = PORT ) else : with io . capture_output () as captured : run_app ( new_feats , mode = 'external' , port = PORT , host = '0.0.0.0' ) HOST = '0.0.0.0' # in AWS replace here by public IP print ( f 'Dash app running on http:// { HOST } : { PORT } ' ) Dash app running on http://0.0.0.0:8095 The mean absolute error of each experiment that was run is reported in the table above. Insights from the experiments These are the basic insights the user could have noticed when trying different models, parameters and training strategies. Effect of the training strategy For the SVR, we see that the standard train-test split has the best performance, although using all months and using a one year window before the test month as training data both have a similar performance. For the RFR on the other hand, we clearly see that using a one year window before the test month as training data leads to better performance than the other training strategies. We note that for both this \"one year window\" strategy and the standard train-test split, the training data is one year. The difference however, is that for the former the training set changes, while for the latter it is fixed (using all the data from 2017). A general trend that can be seen is that the performance increases as a larger training set is used. We therefore urge the user to look at the predictions made using the training strategy where all months before the test month are used as training data: we can expect that the predictions for the later months will be better than for the first months since more training data was used. However, this is hard to see by eye in the prediction plot directly. Effect of the chosen model We can see that, for most parameter choices, both the SVR and RFR can outperform the simple benchmark model we set up. The RFR outperforms the SVR for most (if not all) parameter choices and training strategies. This shows the importance of knowing which model is best suited for your problem and testing different ones. Effect of the chosen parameters The search for the optimal set of hyperparameters for your machine learning model is a non-trivial task, since there might be a strong interplay between the parameters so we can not do the optimal parameter search one by one. Here, we let the user try a pre-set number of choices, but more automatic and smart search algorithms exist to tackle this problem. We found that for the RFR, the higher the number of estimators, the better the results, but at a greater computational cost and with diminishing returns. We see that 50 estimators and leaving the max depth at 10 is a good choice. For the SVR, $C=10$ and $\\gamma=0.01$ work well. Effect of normalization Normalizing the data, rescaling it so that the input and output variables all have values within a similar range (in this case, between 0 and 1) is a common step when setting up machine learning models. This is because, depending on the model, they work much better with values of the order 1. We see that normalization indeed greatly improves the predictions for the SVR, but has very little influence on the RFR. Indeed, algorithms such as random forest and decision tree are not influenced by the scale of the input variables. Conclusion In this Starter Kit we demonstrated forecasting the demand for a particular resource at a particular point in the future, illustrated in the case of energy demand forecasting. To this end, we first analysed the business problem, preprocessed the data, visually and numerically explored it to eventually illustrate the use of different models ( Random Forest Regression, Support Vector Regression and benchmark) to perform the actual forecasting with different amounts of available historical data. From the analysis, it may be clear that achieving good prediction results depend on different factors (appropriate data preprocessing, the amount of available training data, algorithm parametrisation, etc.). Additional Information Contributions by Mathias Verbeke, Wannes Meert, Vincent Vercruyssen, Alessandro Murgia, Tom Tourw√©, and Robbert Verbeke. This Starter Kit was developed in the context of the EluciDATA project ( http://www.elucidata.be ). For more information, please contact info@elucidata.be .","tags":"Time Series Preprocessing","url":"/Time Series Preprocessing/2020-01-01-interactive-notebook.html","loc":"/Time Series Preprocessing/2020-01-01-interactive-notebook.html"},{"title":"Key Take Away Messages","text":"Key Take Away Messages In the video tutorial for this AI Starter Kit, we have demonstrated techniques that can be used to preprocess time series data in order to improve its quality for further exploitation. To illustrate the respective methods and techniques, we have used a real-world dataset containing wind turbine data that exhibits the typical characteristics of industrial datasets, including noise, outliers, missing values, and seasonal patterns. First, we demonstrated how resampling and smoothing can be applied to better understand the behaviour of the time series. As we have shown, resampling techniques allow to reduce the temporal granularity, thereby revealing longer-term trends and hiding sharp, fast fluctuations in the signal. Smoothing on the other hand modifies the time series data such that individual points that are higher than the adjacent points - presumably caused by noise - are reduced, and points that are lower than the adjacent points are increased leading to a smoother signal. Subsequently, we showed how the quality of the data can be improved through normalization and outlier detection. Normalization rescales the range of a particular variable in order to make different variables with different ranges comparable. Outlier detection tried to identify the extreme values in the time series that deviate from other observations in the data. The detected outliers can subsequently be examined in more detail to investigate whether these were caused by measurement errors, in which case they can be removed, or if it concerns unexpected phenomena which are interesting to look into in more detail. Finally, another frequently occurring problem with time series data is that it suffers from missing data, caused by for example senor malfunctioning, communication errors, or due to outliers that were removed. We illustrated how to impute this missing data by means of 3 different techniques: linear interpolation, fleet-based imputation and pattern-based interpolation, and outlined what are the advantages and disadvantages of these techniques. The use of these methods allows to improve the quality of the data and to prepare it for further exploration, analysis and modelling purposes. This is a crucial step, as it will improve the completeness and reliability of the input data, and consequently the accuracy of your results. We thank you for completing this video series and hope to welcome you in another AI Starter Kit tutorial.","tags":"Time Series Preprocessing","url":"/Time Series Preprocessing/2020-01-07-key-take-away-messages.html","loc":"/Time Series Preprocessing/2020-01-07-key-take-away-messages.html"},{"title":"Key Take Away Messages","text":"Key Take Away Messages In the video tutorial for this AI Starter Kit, we covered the basics of using deep learning for remaining useful lifetime prediction, which is one of the central topics in predictive maintenance. We illustrated the approach on the use case of predicting the remaining useful life of aircraft engines based on run-to-failure data. In doing so, we have taken you through the different steps in the data science workflow. We started with the business and data understanding, which allowed us to gain more insights into the specific problem to solve and the structure of the data. Based on this information, we continued with the data preprocessing, in which we explained how to appropriately prepare the data. This included the construction of a training, test and validation set to learn and evaluate a model to predict whether an engine will fail within a certain number of operational cycles. In the video on data modelling and analysis, we showed you how to use deep learning to construct a model for remaining useful life prediction. In particular, we applied a deep learning approach called long short-term memory (LSTM) networks which allows the construction of a predictive model without the time-consuming feature engineering step. This voided the need to manually extract the characteristics from which the algorithm can learn. Finally, we also explained you how to experimentally validate the resulting model and compare the results of different models. While the use of deep learning in some cases might eliminate the need for manual feature engineering, it remains important to tune the models with the appropriate parameter settings, which requires the necessary effort. While the presented approach already gives promising results, there are still quite some improvements possible. Therefore, we recommend you to further try tuning the different parameters, and study the effect on the results. You can also continue to experiment with different network architectures, for example by altering the number of layers and nodes. If you want to gain deeper insights in remaining useful lifetime prediction, a good exercise is trying to cast the prediction problem as a regression or multi-class classification task instead of as a binary classification task. It might also be worth to experiment with the methodology on a larger dataset with a higher number of assets or to try to adapt the different steps to the context of your own dataset. While the details of each of the steps might differ, the methodological steps we presented are typically the required phases you need to go through when solving a remaining useful lifetime prediction model. We thank you for completing this video series and hope to welcome you in another AI Starter Kit tutorial.","tags":"Remaining Useful Life Prediction","url":"/Remaining Useful Life Prediction/2020-01-07-key-take-away-messages.html","loc":"/Remaining Useful Life Prediction/2020-01-07-key-take-away-messages.html"},{"title":"Key Take Away Messages","text":"Key Take Away Messages In the video tutorial for this AI Starter Kit, we demonstrated forecasting the demand for a particular resource at a particular point in the future, illustrated on the case of energy demand forecasting. In doing so, we have taken you through the different steps in the data science workflow. We first performed a number of required data preprocessing steps in order to prepare the data for further analysis. This included fusing the electricity and weather data, requiring to resampling the former data to a lower sampling rate. Subsequently, we explored the fused dataset in order to better understand which factors influence the household energy consumption. To this end, we resorted to both visual and statistical exploration methods. Based on this, we could identify a number of yearly, weekly, and daily patterns, including particular behavior during weekends and some holiday periods. This information formed the basis for the feature extraction that followed next, such as the energy consumption a week before, the day of the week and the hour of the day. Finally, we served the extracted features to two different models, namely Random Forest Regression and Support Vector Regression to perform the actual forecasting. Through different experiments, we analyzed the influence of the training strategy, type of machine learning model and effect of data normalization on the model performance. Based on this, it became clear that achieving good prediction results depends on different factors, including the appropriate data preprocessing, the amount of available training data and the algorithm parametrisation. As next steps, we invite you to further explore each of these influences in more detail. Especially with respect to the hyperparameter tuning, it proves worth to further explore the different parameter settings and study how these changes impact the model performance. Furthermore, you can also try to adapt the different steps to the context of your own dataset. While the details of each of the steps might differ, the methodological steps we presented are typically the required phases you need to go through when solving a resource demand forecasting problem. We thank you for completing this video series and hope to welcome you in another AI Starter Kit tutorial. Additional information The video material in this website was developed in the context of the SKAIDive project , financially supported by the European Social Fund , the European Union and Flanders. For more information, please contact us at elucidatalab@sirris.be","tags":"Resource demand Forecasting","url":"/Resource demand Forecasting/2020-01-08-key-take-away-messages.html","loc":"/Resource demand Forecasting/2020-01-08-key-take-away-messages.html"},{"title":"Key Take Away Messages","text":"Key Take Away Messages In the video tutorial for this AI Starter Kit, we covered the basics of using deep learning for remaining useful lifetime prediction, which is one of the central topics in predictive maintenance. We illustrated the approach on the use case of predicting the remaining useful life of aircraft engines based on run-to-failure data. In doing so, we have taken you through the different steps in the data science workflow. We started with the business and data understanding, which allowed us to gain more insights into the specific problem to solve and the structure of the data. Based on this information, we continued with the data preprocessing, in which we explained how to appropriately prepare the data. This included the construction of a training, test and validation set to learn and evaluate a model to predict whether an engine will fail within a certain number of operational cycles. In the video on data modelling and analysis, we showed you how to use deep learning to construct a model for remaining useful life prediction. In particular, we applied a deep learning approach called long short-term memory (LSTM) networks which allows the construction of a predictive model without the time-consuming feature engineering step. This voided the need to manually extract the characteristics from which the algorithm can learn. Finally, we also explained you how to experimentally validate the resulting model and compare the results of different models. While the use of deep learning in some cases might eliminate the need for manual feature engineering, it remains important to tune the models with the appropriate parameter settings, which requires the necessary effort. While the presented approach already gives promising results, there are still quite some improvements possible. Therefore, we recommend you to further try tuning the different parameters, and study the effect on the results. You can also continue to experiment with different network architectures, for example by altering the number of layers and nodes. If you want to gain deeper insights in remaining useful lifetime prediction, a good exercise is trying to cast the prediction problem as a regression or multi-class classification task instead of as a binary classification task. It might also be worth to experiment with the methodology on a larger dataset with a higher number of assets or to try to adapt the different steps to the context of your own dataset. While the details of each of the steps might differ, the methodological steps we presented are typically the required phases you need to go through when solving a remaining useful lifetime prediction model. We thank you for completing this video series and hope to welcome you in another AI Starter Kit tutorial.","tags":"Remaining Useful Life Prediction","url":"/Remaining Useful Life Prediction/2020-01-07-key-take-away-messages.html","loc":"/Remaining Useful Life Prediction/2020-01-07-key-take-away-messages.html"},{"title":"Key Take Away Messages","text":"Key Take Away Messages In the video tutorial for this AI Starter Kit, we demonstrated forecasting the demand for a particular resource at a particular point in the future, illustrated on the case of energy demand forecasting. In doing so, we have taken you through the different steps in the data science workflow. We first performed a number of required data preprocessing steps in order to prepare the data for further analysis. This included fusing the electricity and weather data, requiring to resampling the former data to a lower sampling rate. Subsequently, we explored the fused dataset in order to better understand which factors influence the household energy consumption. To this end, we resorted to both visual and statistical exploration methods. Based on this, we could identify a number of yearly, weekly, and daily patterns, including particular behavior during weekends and some holiday periods. This information formed the basis for the feature extraction that followed next, such as the energy consumption a week before, the day of the week and the hour of the day. Finally, we served the extracted features to two different models, namely Random Forest Regression and Support Vector Regression to perform the actual forecasting. Through different experiments, we analyzed the influence of the training strategy, type of machine learning model and effect of data normalization on the model performance. Based on this, it became clear that achieving good prediction results depends on different factors, including the appropriate data preprocessing, the amount of available training data and the algorithm parametrisation. As next steps, we invite you to further explore each of these influences in more detail. Especially with respect to the hyperparameter tuning, it proves worth to further explore the different parameter settings and study how these changes impact the model performance. Furthermore, you can also try to adapt the different steps to the context of your own dataset. While the details of each of the steps might differ, the methodological steps we presented are typically the required phases you need to go through when solving a resource demand forecasting problem. We thank you for completing this video series and hope to welcome you in another AI Starter Kit tutorial. Additional information The video material in this website was developed in the context of the SKAIDive project , financially supported by the European Social Fund , the European Union and Flanders. For more information, please contact us at elucidatalab@sirris.be","tags":"Resource demand Forecasting","url":"/Resource demand Forecasting/2020-01-08-key-take-away-messages.html","loc":"/Resource demand Forecasting/2020-01-08-key-take-away-messages.html"},{"title":"Key Take Away Messages","text":"Key Take Away Messages In the video tutorial for this AI Starter Kit, we have demonstrated techniques that can be used to preprocess time series data in order to improve its quality for further exploitation. To illustrate the respective methods and techniques, we have used a real-world dataset containing wind turbine data that exhibits the typical characteristics of industrial datasets, including noise, outliers, missing values, and seasonal patterns. First, we demonstrated how resampling and smoothing can be applied to better understand the behaviour of the time series. As we have shown, resampling techniques allow to reduce the temporal granularity, thereby revealing longer-term trends and hiding sharp, fast fluctuations in the signal. Smoothing on the other hand modifies the time series data such that individual points that are higher than the adjacent points - presumably caused by noise - are reduced, and points that are lower than the adjacent points are increased leading to a smoother signal. Subsequently, we showed how the quality of the data can be improved through normalization and outlier detection. Normalization rescales the range of a particular variable in order to make different variables with different ranges comparable. Outlier detection tried to identify the extreme values in the time series that deviate from other observations in the data. The detected outliers can subsequently be examined in more detail to investigate whether these were caused by measurement errors, in which case they can be removed, or if it concerns unexpected phenomena which are interesting to look into in more detail. Finally, another frequently occurring problem with time series data is that it suffers from missing data, caused by for example senor malfunctioning, communication errors, or due to outliers that were removed. We illustrated how to impute this missing data by means of 3 different techniques: linear interpolation, fleet-based imputation and pattern-based interpolation, and outlined what are the advantages and disadvantages of these techniques. The use of these methods allows to improve the quality of the data and to prepare it for further exploration, analysis and modelling purposes. This is a crucial step, as it will improve the completeness and reliability of the input data, and consequently the accuracy of your results. We thank you for completing this video series and hope to welcome you in another AI Starter Kit tutorial.","tags":"Time Series Preprocessing","url":"/Time Series Preprocessing/2020-01-07-key-take-away-messages.html","loc":"/Time Series Preprocessing/2020-01-07-key-take-away-messages.html"},{"title":"Data Preprocessing","text":"Data Preprocessing Welcome to the second video of the tutorial for the AI Starter Kit on resource demand forecasting! In this video, we will detail the dataset that we will use and perform the necessary preprocessing steps in order to prepare the data for further analysis. In this AI Starter Kit, we will work with a publicly available energy consumption dataset from the UCI repository. This dataset contains measurements of electric power consumption in one household, located near Paris, with a one-minute sampling rate over a period of almost 4 years. Different electrical quantities and sub-metering values are available, as you can see in the table. Let us first have a look at the single variables given in the table: First of all, and most importantly, the global active and reactive powers are given. The active power specifies the amount of energy the single electric receivers transform into mechanical work and heat. This useful effect is called 'active energy'. On the contrary, several receivers, such as motors, transformer, or inductive cookers need magnetic fields in order to work. This amount is called reactive power as these receivers are generally inductive: This means, they absorb energy from the network in order to create magnetic fields and return it while they disappear. This results in an additional electrical consumption that is not useful for the receivers. Further, the voltage and current intensity are given in Volts and Ampere, respectively. Finally, the submeterings provide us a more detailed insights where the power is consumed. Submetering 1 corresponds to the kitchen, containing mainly a dishwasher, an oven, and a microwave. Submetering 2 corresponds to the laundry room, containing a washing-machine, a tumble-drier, a refrigerator and a light. Lastly, submetering 3 corresponds to an electric water-heater and an air-conditioner. We expect, that the different submeterings obey different patterns: While an oven mainly consumes power during lunch and dinner time, a refrigerator has a more or less constant consumption during the entire day. As mentioned before, we will also take weather measurements into account. For this, we use measurements recorded close to the city of Paris, where the household is located, provided by the Wunderground website. It contains information on climate conditions on a 30-minutes base. The dataset contains the outside temperature and the dew point in degrees Celsius, the air humidity in percentage, the pressure at sea level in hPa, the visibility in km and the wind direction expressed in degrees. The weather data is measured by a set of connected sensors. For these, it is rare that the data availability is uninterrupted for such a long period. For this reason, we first check the general statistics for the data set. For most variables, the minimal value is -9999, which indicates missing values as also specified in the documentation of the data. We will replace these with so-called 'Not a Number' values or NaNs such that they will not interfere too much during data modelling. In a next step, we fuse the electricity data and the weather data. Note, that the temporal resolution of the two is not the same ‚Äì the power data is measured every minute while the weather information is available only every 30 minutes. The integration of these two datasets thus requires careful consideration. Two options are possible: we can either upsample the weather data or downsample the household data. That means that we either linearly interpolate the information from weather data on a one-minute interval or aggregate the power data on bigger intervals. In this use case, the latter is more reasonable, as otherwise, we would need to find a way to impute missing data in the climate dataset. Let us first analyze the effect of downsampling the power data. Therefore, we calculate the rolling mean values of the original power over a given time window. You can choose between a sampling rate of 30 or 60 minutes, or 4 hours. The light blue time series shows the original data, the dark blue one the downsampled one. In all cases, the data looks smoother as the original data, as taking the mean value smooths out the major short-time power peaks but at the same time results in some loss of information. With the larger window size of 60 minutes this effect is stronger than for the smaller one. A window size of 4 hours arguably removes too much variation. Note that the weather data is available every 30 minutes such that in case of an hourly time window for the power data also the weather data has to be resampled. Since the information loss for the 60-minute interval is reasonable, we decide to proceed with this sampling rate. This also allows us to reduce the size of the dataset, which will reduce the computation time required for the analysis. In order to merge the two data sets, we transform the active power from a minutes-based power measurement given in kW to an hourly consumption given in Wh. Therefore, we sum the power over one hour and divide it by 60 to get the actual energy. By additionally multiplying by a factor of 1000, we change units from kWh to Wh. For the weather data, we proceed similarly by taking the mean temperature over one hour. With this, we can join the two data sets. For future analysis, we will only take the global consumption and temperature into account. Now that we have preprocessed the dataset, we will illustrate how to gain deeper insights in the data through both visual and statistical data exploration. In the next video, we will start with the visual exploration by means of time plots. Additional information The video material in this website was developed in the context of the SKAIDive project , financially supported by the European Social Fund , the European Union and Flanders. For more information, please contact us at elucidatalab@sirris.be","tags":"Resource demand Forecasting","url":"/Resource demand Forecasting/2020-01-03-data-preprocessing.html","loc":"/Resource demand Forecasting/2020-01-03-data-preprocessing.html"},{"title":"Data Preprocessing","text":"Data Preprocessing Welcome to the fourth video of the tutorial for the AI Starter Kit on remaining useful lifetime prediction! In this video, we will explain you how the data needs to be preprocessed such that it can be served as input for a machine learning algorithm. Let's get back to the NASA Engine data and our use case. Before we can start with training a model to predict the remaining useful lifetime, a number of preparatory steps need to be taken. First of all, let us assign which type of problems we are dealing with. What precisely are we interested in? Is it about predicting a categorical value, which can take only a limited set of possible values, or a number? Indeed, it's not so easy to answer. It depends on how we define our use case: We can either ask: How long is the remaining useful lifetime? Then, a real number is looked for, and we deal with a so-called regression task. Alternatively, we can ask: Will the engine fail within the next 50 cycles? Then this boils down to a so-called binary classification task, in which the prediction is ‚Äòyes' or ‚Äòno'. The choice for one or the other task mainly depends on the business question you want to solve. In most cases, a binary classification problem is easier to handle for a Machine Learning algorithm and therefore, this is the question we will answer in this AI Starter Kit. However, the solution methodology is largely similar when you would opt for predicting the remaining useful lifetime using regression. In the previous video, we explained that for classification tasks, labelled data is essential. Therefore, as a first step, we need to create the binary labels: Has the machine failed within a given period of time or not? For the training data, we know that the final cycle per engine id is the time of failure. In order to determine the remaining number of cycles at each time point, first the maximum number of cycles per engine is determined. Subsequently, the current cycle number is subtracted from the maximal number of cycles to arrive at the number of cycles remaining at a particular time point. [Can we visualize this nicely on a screen on an example? √† Show a timeline next to the dataframe, annotated with max/number of cycles/labels] We will add this value as a new variable to the data. However, this is not yet a binary label that can be used as input for the classification model we want to train. This is created by determining whether the calculated remaining useful lifetime is smaller than or equal to the threshold N ‚Äì the period of time we want to predict the failure in. In this tutorial, we will use 30 cycles as a threshold, meaning that we aim to answer the question whether or not the engine will fail within the next 30 cycles at a particular point in time. This binary label can now be used as input value for the classification model. In order to decide on the goodness of the model, the test data needs the same type of label. In that case though, the final cycles per engine does not automatically determine the time of failure. How can we proceed instead? The ground truth data helps in this case. By joining the unlabelled test data and the ground truth data, we know the time of failure. Once done, we can analogously to the training data calculate the binary label for the test data. Don't forget, these labels will only be used for evaluating the model and are not shown to the algorithm beforehand. A second observation that can be made from the training data sample is that the scales of the values differ significantly across columns ‚Äì both for sensor values and machine settings. This difference in the scale of the numbers could cause problems when the model needs to calculate the similarity between different cycles - namely the rows in the table - during modeling. To address this problem, we will normalize the data. The goal of normalization is to change the values of numeric columns in the dataset to use a common scale, without distorting differences in the ranges of values or losing information. The normalized values maintain the general distribution and ratios in the source data, while keeping values within a scale applied across all numeric columns used in the model. One of the most popular normalization techniques is so-called Min-Max normalization and that is also what we will use here. It scales the values of a variable to a range between 0 and 1 using this formula where ùëã represents the value to be normalized, ùëãùëöùëñùëõ is the minimum value of the variable in that column and ùëãùëöùëéùë• is the maximum value for that variable We apply the normalization on both the training and test data set on all sensor measurement and setting variables. We will not rescale the engine id, as it should be seen as a categorical variable. Further, for the cycle variable we keep both the original and the scaled values. In the next video, we will go to the core of this tutorial, namely using a deep learning algorithm to train a model that is able to solve this binary classification problem.","tags":"Remaining Useful Life Prediction","url":"/Remaining Useful Life Prediction/2020-01-05-data-preprocessing.html","loc":"/Remaining Useful Life Prediction/2020-01-05-data-preprocessing.html"},{"title":"Data Understanding","text":"Data Understanding Welcome to the second video of the tutorial for the AI Starter Kit on remaining useful lifetime prediction! In this video, we will detail the dataset that will use and perform an initial data exploration to extract some first insights. In this AI Starter Kit, we will work with a publicly-available dataset from NASA. The data simulates run-to-failure data from aircraft engines. These engines are assumed to start with varying degrees of wear and manufacturing variation - but this information is unknown to the user. Furthermore, in this simulated data, the engines are assumed to be operating normally at the beginning and start to degrade at some point during operation. The degradation progresses and grows in magnitude with time. When a predefined threshold is reached, the engine is considered unsafe for further operation. In other words, the last operational cycle of the engine can be considered as the failure point of the corresponding engine ‚Äì meaning that the remaining useful lifetime has decreased to zero. Before we can start with learning an actual machine learning model, it is crucial to understand the data itself. The data set consists of multiple time series with the \"cycle\" variable as time unit. For each engine, identified by the variable \"id\", a different number of cycles is captured as not all engines fail at the same time. Per cycle, the following information in gathered: On the one hand, 21 sensor readings given by the data points s1 to s21. On the other hand, additional information about the machine settings, given by setting1 to setting3. In machine learning experiments, a dataset is often split in a training set and a test set. This split allows one to quickly evaluate the performance of an algorithm. The training dataset is used to prepare a model, to train it. For evaluation, the test dataset can be understood as new data that is presented to the algorithm. It was not seen by the algorithm before and therefore the outcome is unknown. In our example, it is data from different engines for which it is unclear when they are going to fail, or put differently, what their remaining useful lifetime is. For the purpose of evaluation, the information about the actual failure of the test data set is collected in the so-called ground truth data. This information will not be visible to the algorithm but will only be used for calculating the quality of the model. Now let's have a look at the single sensor measurements. We see that the value range of the single measurements are quite different, without knowing in detail what they correspond to in a sense of physical measurement. In the graph, we see the first 50 entries of time series data collected from three different sensor channels for engine 18. All three show some fluctuations but no clear deviation from a mean value indicated by the gray dotted lines, that could be a sign of degradation in engine performance, are visible. With increasing observation time, all three time series deviate more or less strongly from the mean values observed in the first 50 data points given by the gray horizontal line, indicating the start of the degradation process of the engine. For different engines, the deviation starts at different times. For engine 18, the deviation starts approximately at time 100. For engine 31 though, hardly any deviation is visible in the same time range. Only when increasing the time range, the clear deviation becomes evident. The different starting points of degradation for the single engines indicate that the simulations are made for engines with different wear. In the next video, we will go into more detail into the data preprocessing phase, explaining how the data needs to be prepared such that it can be served as input for a machine learning learning algorithm. If you are not familiar with deep learning, we recommend you to first watch our introductory video on this topic, in which we discuss the difference between ‚Äòtraditional' Machine Learning algorithms and Deep Learning techniques. We will also provide a brief introduction to the different type of neural networks, amongst others the so-called Long Short-Term Memory networks or LSTMs for short, which is the type of network that we will use to solve this problem.","tags":"Remaining Useful Life Prediction","url":"/Remaining Useful Life Prediction/2020-01-03-data-understanding.html","loc":"/Remaining Useful Life Prediction/2020-01-03-data-understanding.html"},{"title":"Data Understanding","text":"Data Understanding Welcome to the second video of the tutorial for the AI Starter Kit on time-series pre-processing! In this video, we will detail the dataset that we will use and perform an initial data exploration. The time series dataset that we study in this Starter Kit is generated by the SCADA system of a set of wind turbines. SCADA stands for supervisory control and data acquisition. In modern turbines, such a data acquisition system can easily contain more than 100 sensors that keep track of amongst others temperature, pressure data, electrical quantities like currents and voltages, and vibrations. It is commonly used for performance monitoring and condition-based maintenance. The dataset originates from 4 wind turbines located in the Northeast of France. It spans a period of 4 years with a sampling rate of 10 minutes. Although the original SCADA system records statistics for more than 30 sensors, we will focus our attention only on wind speed, wind direction, temperature as well as the power that is generated by each of these turbines to illustrate a variety of time series pre-processing techniques. The table on the right-hand side shows an excerpt of the data, with the following attributes: * The column Date_time with the timestamp of the measurement, in 10-minute increments * the identifier of the turbine * Power, which is the active power measurement in kW as effectively produced by the turbine. * the outside temperature measurement in degrees Celsius * the wind speed measurement in meters per second And finally * the wind direction measurement in degrees The top rows show the average values for each turbine for the defined range. Note that in the case of wind direction, this is the circular average, which takes into account the circular nature of the data. That means that values are bound between 0 and 360 degrees, in which 0 degrees is identical to 360 degrees. In our interactive Starter Kit, you can define a range of values for a given attribute and see how the values in the remaining attributes change. For example, we can increase the wind speed to more than 17m/sec and see how that affects power production. Can you already spot any unexpected values in the dataset? Indeed, in the marked area in the figure, the values are significantly below those seen at other times. You can experiment yourself how the other variables influence the active power in the interactive Starter Kit. Now that we know which variables were measured, let's check some statistics. From the table we learn that in total 4 years of data are available, namely from January 2013 to the end of 2016. Further, we see that the number of data points per wind turbine differs. Most data points are available for the first turbine in the column, while the second turbine collected roughly 1000 data points less. This indicates already that for this latter turbine some data points are missing. In the interactive Starter Kit, we can plot the values of a given variable in time to see how their long-term trend looks like. In the graph shown at the right, we adjusted the timeframe by selecting it directly in the graph, such that also shorter-term fluctuations become visible. By clicking on the turbine names at the right-hand side of the graph, we can select a subset of the turbines. In the drop-down menu at the top of the graph, the variable to plot can be selected. What can we learn from this visual exploration? Let's first have a look at the temperature. Here we can easily spot the seasonal pattern due to summer and winter. Is this behaviour similar for the active power? Is there something unusual you can observe in the temperature values? We will come back to the answers to these questions later on in this Starter Kit and introduce you to a number of techniques to automatically detect this. In the next video, we will first concentrate on the smoothening and so-called resampling of the data points. See you there.","tags":"Time Series Preprocessing","url":"/Time Series Preprocessing/2020-01-03-data-understanding.html","loc":"/Time Series Preprocessing/2020-01-03-data-understanding.html"},{"title":"Introduction","text":"Introduction Welcome to the tutorial for the AI Starter Kit on remaining useful lifetime prediction! In this first video we will provide an overview of the actual business case we're tackling. We will introduce the most important concepts and explain why it is beneficial for maintenance to know the remaining useful lifetime of a machine, a tool or any other industrial asset. Estimating the remaining useful lifetime of an asset can be beneficial for several purposes in a variety of industrial contexts. For one, it can offer support in the better scheduling of maintenance operations, for example for offshore wind turbines. For such assets, maintenance can only be performed under the right weather conditions. But also for other assets, maintenance is an important part of its lifecycle. Traditionally, maintenance is done in a corrective way, such that it is performed at the moment an asset has failed. In that case, the failing part is identified and rectified or replaced, to ensure that the asset can subsequently resume normal operation. Especially for strongly interdependent production lines or industrial assets operating in critical environments, an unplanned downtime is to be avoided and often costly. For this reason, more recently, for an increasing number of assets preventive maintenance is performed. In that case, maintenance tasks are scheduled at regular intervals, avoiding future asset failure to a maximum extent ‚Äì but with the risk to replace assets that are still working correctly and still could for a while. Hence, the optimal time for replacement is wanted: The time when the asset still works correctly but will probably fail soon. And this is where the remaining useful lifetime comes into play. Nowadays, ever more assets are equipped with different types of sensors gathering data. This started the trend towards predictive maintenance, in which the maintenance moments are decided upon in a data-driven way. Therewith, maintenance is only performed when actually needed. This is a very broad domain which encompasses a variety of topics. Besides the estimation of the asset's remaining useful lifetime ‚Äì the main topic of this AI starter kit ‚Äì further related problems to solve for a fully predictive maintenance approach are failure prediction, detection, and diagnosis for root cause analysis. The remaining useful lifetime of industrial assets is defined as an estimate of the remaining time that an item, component, or system is estimated to function as expected. It is expressed as the number of hours, cycles, batches or any other quantity. In the figure we see datapoints collected from an arbitrary sensor in green. Let's say it measures the tool radius of a milling machine. With increasing time, the radius decreases as the tool wears out. At a certain point, the tool is considered too worn out to still further use it in production, which can influence the quality of the produced parts or lead to an unstable production process and consequent damage to the machine. The main questions thus is for how long the tool will it still be able to function properly? By applying Machine Learning algorithms, the continuation of this time series data can be forecasted, denoted in pink. From this forecast, the remaining useful lifetime can be estimated for a given asset. In this case, the machine should be maintained within the next 10 to 15 cycles. With this information, maintenance tasks can be scheduled accordingly ‚Äì with a tool still functioning and without unexpected downtime. Hence, by estimating the remaining useful lifetime, we minimize the risk of failure and maintenance cost. An accurate prediction of an asset's lifetime can also help to optimise the operational efficiency of an asset, by more optimally planning the use of that asset before its end of life, or adapting its operational use to extend its useful lifetime. In manufacturing settings, where a downtime of the production line results in several operational problems and associated costs, remaining useful lifetime prediction can avoid unplanned downtime. In this AI Starter Kit, we will show its use to avoid safety-critical situations, by illustrating the solution methodology for predicting the remaining useful lifetime of aircrafts engines. To summarize, by applying predictive maintenance it is possible to: - better schedule maintenance operations - optimize operational efficiency - avoid unplanned downtime - anticipate and avoid safety-critical situations. This analysis can be performed for various machines, tools or processes as long as this degradation over time can be measured accordingly, just like resistance, length, temperature or similar measures. Predicting the remaining useful lifetime is typically very challenging for several reasons: First of all, usually a multitude of heterogenous sensor data is measured at several places in the machine. This data is often captured at a high-frequency, consisting of vibration data, acoustic emission, accelerometer data, and many more machine-internal parameters. Secondly, in some settings, also characteristics of the surrounding environment influencing the lifetime of the asset are captured. Further, typically, data that is gathered over a long period of time needs to be available to train the model. Given that industrial assets are often very complex ‚Äì as they consist of both electrical and mechanical components - it generally is a non-trivial exercise to predict their end of life. On top of that, the data typically comes with a lot of variation, due to varying machine types operating in heterogenous operating conditions with different machine configurations. Therefore, the main challenge in this process is to extract meaningful characteristics that can be used to predict the end of life from the gathered data. This is complicated by the fact that usually very little domain expertise on the operating conditions of these assets is available. Additionally, the environment in which they operate is typically very dynamic. In this AI Starter Kit, we will guide you through a data-driven methodology based on deep learning to tackle this challenge. In the next video, we will concentrate on the data itself that we selected to illustrate the approach. We will explain which information is available and what we can learn from it.","tags":"Remaining Useful Life Prediction","url":"/Remaining Useful Life Prediction/2020-01-02-introduction.html","loc":"/Remaining Useful Life Prediction/2020-01-02-introduction.html"},{"title":"Introduction","text":"Introduction Welcome to the tutorial for the AI Starter Kit on resource demand forecasting. In this first video we will provide an overview of the actual business case we are tackling. We will introduce the most important concepts and explain why it is beneficial for resource planning to know the expected demand in the future. Resource demand forecasting concerns accurately predicting the future need for a resource, typically using historical information. It is one of the most essential steps in resource demand management, which tries to ensure that sufficient resources are available to satisfy a fluctuating demand. Resource demand forecasting allows one to plan ahead and guarantee that sufficient resources are available when needed and to avoid costly countermeasures in case of shortage. Precisely forecasting the demand of a given resource can be beneficial for several purposes in a wide range of industrial contexts. When knowing the future demand, only the number of resources actually required need to be allocated to a specific task or process. The field ranges from forecasting the number of products that need to be manufactured based on historical sales figures, over predicting the parking demand or traffic density in a particular neighbourhood in order to control traffic in more intelligent ways, to estimating the amount of energy that will be consumed and produced for a particular city or region. Especially in the later field, the demand is influenced by a mixture of external factors such as the weather conditions. For energy suppliers it is more beneficial to buy electricity on the day-ahead market than on the spot market. Consequently, the more accurate the energy consumption can be predicted, the lower the cost. Given that more and more houses and appliances are enabled with a smart meter, an increasing amount of data becomes available that can enable and improve this prediction. Furthermore, these kinds of predictions also allow energy suppliers to better balance demand and supply and helps them to ensure proper grid operation, but also to avoid negative prices, for example in times of high solar irradiation. In this example we see the active power of a single household near Paris shown in green. In orange, the outside temperature is given. The two are strongly correlated ‚Äì the demand increases with decreasing temperatures since most French households use electrical heating. Therefore, particularly in winter times, the grid has to be stable when facing high demands. In this Starter Kit, we will demonstrate how to effectively forecast the demand for this household for the future by taking external factors into account. In the following videos, we will first start by gaining some first insights in the data in front of us and perform some initial pre-processing steps to prepare the data for the analysis. Subsequently, we will illustrate you how to gain deeper insights by presenting a number of visual and statistical techniques in order to explore the data for identifying interesting patterns. In a next step, useful characteristics or features will be extracted from the dataset that can as serve as input for the forecasting models. Finally, in the last video, we will explain you how to correctly compare the performance of multiple forecasting models. Let us go ahead with the next video on data preparation, in which we will describe how to deal with the typical problems of raw data and how to perform data fusion when multiple sources of data are available. Additional information The video material in this website was developed in the context of the SKAIDive project , financially supported by the European Social Fund , the European Union and Flanders. For more information, please contact us at elucidatalab@sirris.be","tags":"Resource demand Forecasting","url":"/Resource demand Forecasting/2020-01-02-introduction.html","loc":"/Resource demand Forecasting/2020-01-02-introduction.html"},{"title":"Introduction","text":"Introduction Welcome to the tutorial for the AI Starter Kit on time-series pre-processing. In this first video we will provide an overview of the actual challenge we're tackling. We will introduce the most important problems occurring in time series data and explain why it is beneficial to first improve your data before you start any other data-modelling tasks. Time series are characterised as a collection of data points obtained at successive times, most often with equal intervals between them. Since an increasing amount of assets is instrumented with sensors, this type of data is omnipresent, for example when monitoring industrial machinery or resulting from wearables tracking one's health state. Depending on the application domain, time series data is exploited for various purposes. It is used, amongst others, for profiling energy consumption of households predicting imminent failures of a production line, estimating the remaining useful lifetime of a machine, and many more industrial use cases. Typically, time series data cannot be used easily as-is by machine learning algorithms. This can be for example due to data quality issues such as missing values and sensor misreadings. Or because some algorithms are not fit to deal with the continuous nature of this type of data or the associated - often high ‚Äì frequency with which it is gathered. To illustrate the various methods and techniques in this Starter Kit, we will use a publicly available real-world dataset that consists of sensor data measurements from wind turbines. It exhibits typical characteristics of industrial time series datasets as it is very noisy and contains outliers and missing values. Moreover, it exhibits seasonal patterns across multiple years, as the machine behaviour is influenced by the meteorological conditions. We will use this dataset to illustrate * how resampling and smoothing can be applied to gain a better understanding of the behaviour of the time series, * how the quality of the data can be improved through normalization and outlier detection, and * how missing data can be imputed in various ways. In the next video, we will perform some initial exploration on the dataset in order to gain some basic understanding of the data.","tags":"Time Series Preprocessing","url":"/Time Series Preprocessing/2020-01-02-introduction.html","loc":"/Time Series Preprocessing/2020-01-02-introduction.html"},{"title":"Introduction","text":"Introduction Welcome to the tutorial for the AI Starter Kit on resource demand forecasting. In this first video we will provide an overview of the actual business case we are tackling. We will introduce the most important concepts and explain why it is beneficial for resource planning to know the expected demand in the future. Resource demand forecasting concerns accurately predicting the future need for a resource, typically using historical information. It is one of the most essential steps in resource demand management, which tries to ensure that sufficient resources are available to satisfy a fluctuating demand. Resource demand forecasting allows one to plan ahead and guarantee that sufficient resources are available when needed and to avoid costly countermeasures in case of shortage. Precisely forecasting the demand of a given resource can be beneficial for several purposes in a wide range of industrial contexts. When knowing the future demand, only the number of resources actually required need to be allocated to a specific task or process. The field ranges from forecasting the number of products that need to be manufactured based on historical sales figures, over predicting the parking demand or traffic density in a particular neighbourhood in order to control traffic in more intelligent ways, to estimating the amount of energy that will be consumed and produced for a particular city or region. Especially in the later field, the demand is influenced by a mixture of external factors such as the weather conditions. For energy suppliers it is more beneficial to buy electricity on the day-ahead market than on the spot market. Consequently, the more accurate the energy consumption can be predicted, the lower the cost. Given that more and more houses and appliances are enabled with a smart meter, an increasing amount of data becomes available that can enable and improve this prediction. Furthermore, these kinds of predictions also allow energy suppliers to better balance demand and supply and helps them to ensure proper grid operation, but also to avoid negative prices, for example in times of high solar irradiation. In this example we see the active power of a single household near Paris shown in green. In orange, the outside temperature is given. The two are strongly correlated ‚Äì the demand increases with decreasing temperatures since most French households use electrical heating. Therefore, particularly in winter times, the grid has to be stable when facing high demands. In this Starter Kit, we will demonstrate how to effectively forecast the demand for this household for the future by taking external factors into account. In the following videos, we will first start by gaining some first insights in the data in front of us and perform some initial pre-processing steps to prepare the data for the analysis. Subsequently, we will illustrate you how to gain deeper insights by presenting a number of visual and statistical techniques in order to explore the data for identifying interesting patterns. In a next step, useful characteristics or features will be extracted from the dataset that can as serve as input for the forecasting models. Finally, in the last video, we will explain you how to correctly compare the performance of multiple forecasting models. Let us go ahead with the next video on data preparation, in which we will describe how to deal with the typical problems of raw data and how to perform data fusion when multiple sources of data are available. Additional information The video material in this website was developed in the context of the SKAIDive project , financially supported by the European Social Fund , the European Union and Flanders. For more information, please contact us at elucidatalab@sirris.be","tags":"Resource demand Forecasting","url":"/Resource demand Forecasting/2020-01-02-introduction.html","loc":"/Resource demand Forecasting/2020-01-02-introduction.html"},{"title":"Introduction","text":"Introduction Welcome to the tutorial for the AI Starter Kit on time-series pre-processing. In this first video we will provide an overview of the actual challenge we're tackling. We will introduce the most important problems occurring in time series data and explain why it is beneficial to first improve your data before you start any other data-modelling tasks. Time series are characterised as a collection of data points obtained at successive times, most often with equal intervals between them. Since an increasing amount of assets is instrumented with sensors, this type of data is omnipresent, for example when monitoring industrial machinery or resulting from wearables tracking one's health state. Depending on the application domain, time series data is exploited for various purposes. It is used, amongst others, for profiling energy consumption of households predicting imminent failures of a production line, estimating the remaining useful lifetime of a machine, and many more industrial use cases. Typically, time series data cannot be used easily as-is by machine learning algorithms. This can be for example due to data quality issues such as missing values and sensor misreadings. Or because some algorithms are not fit to deal with the continuous nature of this type of data or the associated - often high ‚Äì frequency with which it is gathered. To illustrate the various methods and techniques in this Starter Kit, we will use a publicly available real-world dataset that consists of sensor data measurements from wind turbines. It exhibits typical characteristics of industrial time series datasets as it is very noisy and contains outliers and missing values. Moreover, it exhibits seasonal patterns across multiple years, as the machine behaviour is influenced by the meteorological conditions. We will use this dataset to illustrate * how resampling and smoothing can be applied to gain a better understanding of the behaviour of the time series, * how the quality of the data can be improved through normalization and outlier detection, and * how missing data can be imputed in various ways. In the next video, we will perform some initial exploration on the dataset in order to gain some basic understanding of the data.","tags":"Time Series Preprocessing","url":"/Time Series Preprocessing/2020-01-02-introduction.html","loc":"/Time Series Preprocessing/2020-01-02-introduction.html"},{"title":"Introduction","text":"Introduction Welcome to the tutorial for the AI Starter Kit on remaining useful lifetime prediction! In this first video we will provide an overview of the actual business case we're tackling. We will introduce the most important concepts and explain why it is beneficial for maintenance to know the remaining useful lifetime of a machine, a tool or any other industrial asset. Estimating the remaining useful lifetime of an asset can be beneficial for several purposes in a variety of industrial contexts. For one, it can offer support in the better scheduling of maintenance operations, for example for offshore wind turbines. For such assets, maintenance can only be performed under the right weather conditions. But also for other assets, maintenance is an important part of its lifecycle. Traditionally, maintenance is done in a corrective way, such that it is performed at the moment an asset has failed. In that case, the failing part is identified and rectified or replaced, to ensure that the asset can subsequently resume normal operation. Especially for strongly interdependent production lines or industrial assets operating in critical environments, an unplanned downtime is to be avoided and often costly. For this reason, more recently, for an increasing number of assets preventive maintenance is performed. In that case, maintenance tasks are scheduled at regular intervals, avoiding future asset failure to a maximum extent ‚Äì but with the risk to replace assets that are still working correctly and still could for a while. Hence, the optimal time for replacement is wanted: The time when the asset still works correctly but will probably fail soon. And this is where the remaining useful lifetime comes into play. Nowadays, ever more assets are equipped with different types of sensors gathering data. This started the trend towards predictive maintenance, in which the maintenance moments are decided upon in a data-driven way. Therewith, maintenance is only performed when actually needed. This is a very broad domain which encompasses a variety of topics. Besides the estimation of the asset's remaining useful lifetime ‚Äì the main topic of this AI starter kit ‚Äì further related problems to solve for a fully predictive maintenance approach are failure prediction, detection, and diagnosis for root cause analysis. The remaining useful lifetime of industrial assets is defined as an estimate of the remaining time that an item, component, or system is estimated to function as expected. It is expressed as the number of hours, cycles, batches or any other quantity. In the figure we see datapoints collected from an arbitrary sensor in green. Let's say it measures the tool radius of a milling machine. With increasing time, the radius decreases as the tool wears out. At a certain point, the tool is considered too worn out to still further use it in production, which can influence the quality of the produced parts or lead to an unstable production process and consequent damage to the machine. The main questions thus is for how long the tool will it still be able to function properly? By applying Machine Learning algorithms, the continuation of this time series data can be forecasted, denoted in pink. From this forecast, the remaining useful lifetime can be estimated for a given asset. In this case, the machine should be maintained within the next 10 to 15 cycles. With this information, maintenance tasks can be scheduled accordingly ‚Äì with a tool still functioning and without unexpected downtime. Hence, by estimating the remaining useful lifetime, we minimize the risk of failure and maintenance cost. An accurate prediction of an asset's lifetime can also help to optimise the operational efficiency of an asset, by more optimally planning the use of that asset before its end of life, or adapting its operational use to extend its useful lifetime. In manufacturing settings, where a downtime of the production line results in several operational problems and associated costs, remaining useful lifetime prediction can avoid unplanned downtime. In this AI Starter Kit, we will show its use to avoid safety-critical situations, by illustrating the solution methodology for predicting the remaining useful lifetime of aircrafts engines. To summarize, by applying predictive maintenance it is possible to: - better schedule maintenance operations - optimize operational efficiency - avoid unplanned downtime - anticipate and avoid safety-critical situations. This analysis can be performed for various machines, tools or processes as long as this degradation over time can be measured accordingly, just like resistance, length, temperature or similar measures. Predicting the remaining useful lifetime is typically very challenging for several reasons: First of all, usually a multitude of heterogenous sensor data is measured at several places in the machine. This data is often captured at a high-frequency, consisting of vibration data, acoustic emission, accelerometer data, and many more machine-internal parameters. Secondly, in some settings, also characteristics of the surrounding environment influencing the lifetime of the asset are captured. Further, typically, data that is gathered over a long period of time needs to be available to train the model. Given that industrial assets are often very complex ‚Äì as they consist of both electrical and mechanical components - it generally is a non-trivial exercise to predict their end of life. On top of that, the data typically comes with a lot of variation, due to varying machine types operating in heterogenous operating conditions with different machine configurations. Therefore, the main challenge in this process is to extract meaningful characteristics that can be used to predict the end of life from the gathered data. This is complicated by the fact that usually very little domain expertise on the operating conditions of these assets is available. Additionally, the environment in which they operate is typically very dynamic. In this AI Starter Kit, we will guide you through a data-driven methodology based on deep learning to tackle this challenge. In the next video, we will concentrate on the data itself that we selected to illustrate the approach. We will explain which information is available and what we can learn from it.","tags":"Remaining Useful Life Prediction","url":"/Remaining Useful Life Prediction/2020-01-02-introduction.html","loc":"/Remaining Useful Life Prediction/2020-01-02-introduction.html"},{"title":"Getting Started with the Interactive Experimenting Environment","text":"Getting Started with the Interactive Experimenting Environment In the AI Starter Kits, we mainly work in so-called Jupyter Notebooks . The Jupyter Project is a non-profit, open-source project born in 2004. Jupyter Notebook, that was developed within the project, is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text [1]. If you have never worked with Jupyter Notebooks before, we invite you to read this short tutorial before starting with the actual content of the AI Starter Kits. We provide you some short explanations on how to use the web application to run the python code that was developed. How to get started? For the AI Starterkits, getting started is very easy! Just click at the link provided on the platform and it will bring you directly into Jupyter Hub, which hosts the Jupyter Notebooks for the Starter Kits. For each Starter Kit, we created a so-called environment in which all libraries needed are already installed such that you do not need to take care of this at all. You can get started straight away. How to use the Jupyter Notebooks? Before we have a look at the notebooks themselves, we first have a brief look at the toolbar at the top of each notebook as shown in the following figure. The toolbar provides all buttons to interact with the code in a notebook. We go through the single buttons step by step. Important to understand is that each Jupyter Notebook consists of different cells. In each cell, you can execute a batch of code. In order to create a new cell, you simply click at the button and a new code cell will appear in the notebook. Try it yourself and create a new cell. You notebook should look like this now: In this cell, we can execute a simply command like print(\"Hello World!\") and execute it by pushing the or more easily by pushing the combination SHIFT + Enter . Congratulations, you just executed your first python command in a Jupyter Notebook! Just below the cell, the result in shown: In this case, the words 'Hello World!' are printed. Additionally, a new cell was automatically created just below the former one. Of course, we can also execute several commands within one cell. For this, please enter the following lines in the next cell: a = 5 a b = 3 a + b Before you execute the cell, think about the output you expect. Was the output as expected? Or did you expect to also see the value of a ? Keep in mind that Jupter Notebook will always return the last executed command only. In case you also want to see the value of a , you need to wrap a print(a) command around it. How to add documentation to your notebook? In order to increase the readability of the notebook, it helps to add some documentation around the single commands. This can be helpful in case you want to share the notebooks with others and want to explain why you performed a given command. The easiest way to to this is my changing the cell output in the toolbar: After doing so, you can enter some text in Markdown format around your cells. Test it by entering This was a ** nice ** example ! into a Markdown cell and execute it just as a normal code cell by pushing the button. How to save a Jupyter Notebook? Once you are done with a notebook you have two simple ways to share it with others: * Directly as a Jupyter Notebook. If the recipent has a Jupyter environment, he or she can directly continue working on your code. * Otherwise, you can also export the Notebook in html format: Bibliography [1] https://jupyter.org/","tags":"Remaining Useful Life Prediction","url":"/Remaining Useful Life Prediction/2020-01-01-getting-started.html","loc":"/Remaining Useful Life Prediction/2020-01-01-getting-started.html"},{"title":"Getting Started with the Interactive Experimenting Environment","text":"Getting Started with the Interactive Experimenting Environment In the AI Starter Kits, we mainly work in so-called Jupyter Notebooks . The Jupyter Project is a non-profit, open-source project born in 2004. Jupyter Notebook, that was developed within the project, is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text [1]. If you have never worked with Jupyter Notebooks before, we invite you to read this short tutorial before starting with the actual content of the AI Starter Kits. We provide you some short explanations on how to use the web application to run the python code that was developed. How to get started? For the AI Starterkits, getting started is very easy! Just click at the link provided on the platform and it will bring you directly into Jupyter Hub, which hosts the Jupyter Notebooks for the Starter Kits. For each Starter Kit, we created a so-called environment in which all libraries needed are already installed such that you do not need to take care of this at all. You can get started straight away. How to use the Jupyter Notebooks? Before we have a look at the notebooks themselves, we first have a brief look at the toolbar at the top of each notebook as shown in the following figure. The toolbar provides all buttons to interact with the code in a notebook. We go through the single buttons step by step. Important to understand is that each Jupyter Notebook consists of different cells. In each cell, you can execute a batch of code. In order to create a new cell, you simply click at the button and a new code cell will appear in the notebook. Try it yourself and create a new cell. You notebook should look like this now: In this cell, we can execute a simply command like print(\"Hello World!\") and execute it by pushing the or more easily by pushing the combination SHIFT + Enter . Congratulations, you just executed your first python command in a Jupyter Notebook! Just below the cell, the result in shown: In this case, the words 'Hello World!' are printed. Additionally, a new cell was automatically created just below the former one. Of course, we can also execute several commands within one cell. For this, please enter the following lines in the next cell: a = 5 a b = 3 a + b Before you execute the cell, think about the output you expect. Was the output as expected? Or did you expect to also see the value of a ? Keep in mind that Jupter Notebook will always return the last executed command only. In case you also want to see the value of a , you need to wrap a print(a) command around it. How to add documentation to your notebook? In order to increase the readability of the notebook, it helps to add some documentation around the single commands. This can be helpful in case you want to share the notebooks with others and want to explain why you performed a given command. The easiest way to to this is my changing the cell output in the toolbar: After doing so, you can enter some text in Markdown format around your cells. Test it by entering This was a ** nice ** example ! into a Markdown cell and execute it just as a normal code cell by pushing the button. How to save a Jupyter Notebook? Once you are done with a notebook you have two simple ways to share it with others: * Directly as a Jupyter Notebook. If the recipent has a Jupyter environment, he or she can directly continue working on your code. * Otherwise, you can also export the Notebook in html format: Bibliography [1] https://jupyter.org/","tags":"Resource demand Forecasting","url":"/Resource demand Forecasting/2020-01-01-getting-started.html","loc":"/Resource demand Forecasting/2020-01-01-getting-started.html"},{"title":"Getting Started with the Interactive Experimenting Environment","text":"Getting Started with the Interactive Experimenting Environment In the AI Starter Kits, we mainly work in so-called Jupyter Notebooks . The Jupyter Project is a non-profit, open-source project born in 2004. Jupyter Notebook, that was developed within the project, is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text [1]. If you have never worked with Jupyter Notebooks before, we invite you to read this short tutorial before starting with the actual content of the AI Starter Kits. We provide you some short explanations on how to use the web application to run the python code that was developed. How to get started? For the AI Starterkits, getting started is very easy! Just click at the link provided on the platform and it will bring you directly into Jupyter Hub, which hosts the Jupyter Notebooks for the Starter Kits. For each Starter Kit, we created a so-called environment in which all libraries needed are already installed such that you do not need to take care of this at all. You can get started straight away. How to use the Jupyter Notebooks? Before we have a look at the notebooks themselves, we first have a brief look at the toolbar at the top of each notebook as shown in the following figure. The toolbar provides all buttons to interact with the code in a notebook. We go through the single buttons step by step. Important to understand is that each Jupyter Notebook consists of different cells. In each cell, you can execute a batch of code. In order to create a new cell, you simply click at the button and a new code cell will appear in the notebook. Try it yourself and create a new cell. You notebook should look like this now: In this cell, we can execute a simply command like print(\"Hello World!\") and execute it by pushing the or more easily by pushing the combination SHIFT + Enter . Congratulations, you just executed your first python command in a Jupyter Notebook! Just below the cell, the result in shown: In this case, the words 'Hello World!' are printed. Additionally, a new cell was automatically created just below the former one. Of course, we can also execute several commands within one cell. For this, please enter the following lines in the next cell: a = 5 a b = 3 a + b Before you execute the cell, think about the output you expect. Was the output as expected? Or did you expect to also see the value of a ? Keep in mind that Jupter Notebook will always return the last executed command only. In case you also want to see the value of a , you need to wrap a print(a) command around it. How to add documentation to your notebook? In order to increase the readability of the notebook, it helps to add some documentation around the single commands. This can be helpful in case you want to share the notebooks with others and want to explain why you performed a given command. The easiest way to to this is my changing the cell output in the toolbar: After doing so, you can enter some text in Markdown format around your cells. Test it by entering This was a ** nice ** example ! into a Markdown cell and execute it just as a normal code cell by pushing the button. How to save a Jupyter Notebook? Once you are done with a notebook you have two simple ways to share it with others: * Directly as a Jupyter Notebook. If the recipent has a Jupyter environment, he or she can directly continue working on your code. * Otherwise, you can also export the Notebook in html format: Bibliography [1] https://jupyter.org/","tags":"Time Series Preprocessing","url":"/Time Series Preprocessing/2020-01-01-getting-started.html","loc":"/Time Series Preprocessing/2020-01-01-getting-started.html"},{"title":"Getting Started with the Interactive Experimenting Environment","text":"Getting Started with the Interactive Experimenting Environment In the AI Starter Kits, we mainly work in so-called Jupyter Notebooks . The Jupyter Project is a non-profit, open-source project born in 2004. Jupyter Notebook, that was developed within the project, is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text [1]. If you have never worked with Jupyter Notebooks before, we invite you to read this short tutorial before starting with the actual content of the AI Starter Kits. We provide you some short explanations on how to use the web application to run the python code that was developed. How to get started? For the AI Starterkits, getting started is very easy! Just click at the link provided on the platform and it will bring you directly into Jupyter Hub, which hosts the Jupyter Notebooks for the Starter Kits. For each Starter Kit, we created a so-called environment in which all libraries needed are already installed such that you do not need to take care of this at all. You can get started straight away. How to use the Jupyter Notebooks? Before we have a look at the notebooks themselves, we first have a brief look at the toolbar at the top of each notebook as shown in the following figure. The toolbar provides all buttons to interact with the code in a notebook. We go through the single buttons step by step. Important to understand is that each Jupyter Notebook consists of different cells. In each cell, you can execute a batch of code. In order to create a new cell, you simply click at the button and a new code cell will appear in the notebook. Try it yourself and create a new cell. You notebook should look like this now: In this cell, we can execute a simply command like print(\"Hello World!\") and execute it by pushing the or more easily by pushing the combination SHIFT + Enter . Congratulations, you just executed your first python command in a Jupyter Notebook! Just below the cell, the result in shown: In this case, the words 'Hello World!' are printed. Additionally, a new cell was automatically created just below the former one. Of course, we can also execute several commands within one cell. For this, please enter the following lines in the next cell: a = 5 a b = 3 a + b Before you execute the cell, think about the output you expect. Was the output as expected? Or did you expect to also see the value of a ? Keep in mind that Jupter Notebook will always return the last executed command only. In case you also want to see the value of a , you need to wrap a print(a) command around it. How to add documentation to your notebook? In order to increase the readability of the notebook, it helps to add some documentation around the single commands. This can be helpful in case you want to share the notebooks with others and want to explain why you performed a given command. The easiest way to to this is my changing the cell output in the toolbar: After doing so, you can enter some text in Markdown format around your cells. Test it by entering This was a ** nice ** example ! into a Markdown cell and execute it just as a normal code cell by pushing the button. How to save a Jupyter Notebook? Once you are done with a notebook you have two simple ways to share it with others: * Directly as a Jupyter Notebook. If the recipent has a Jupyter environment, he or she can directly continue working on your code. * Otherwise, you can also export the Notebook in html format: Bibliography [1] https://jupyter.org/","tags":"Resource demand Forecasting","url":"/Resource demand Forecasting/2020-01-01-getting-started.html","loc":"/Resource demand Forecasting/2020-01-01-getting-started.html"},{"title":"Getting Started with the Interactive Experimenting Environment","text":"Getting Started with the Interactive Experimenting Environment In the AI Starter Kits, we mainly work in so-called Jupyter Notebooks . The Jupyter Project is a non-profit, open-source project born in 2004. Jupyter Notebook, that was developed within the project, is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text [1]. If you have never worked with Jupyter Notebooks before, we invite you to read this short tutorial before starting with the actual content of the AI Starter Kits. We provide you some short explanations on how to use the web application to run the python code that was developed. How to get started? For the AI Starterkits, getting started is very easy! Just click at the link provided on the platform and it will bring you directly into Jupyter Hub, which hosts the Jupyter Notebooks for the Starter Kits. For each Starter Kit, we created a so-called environment in which all libraries needed are already installed such that you do not need to take care of this at all. You can get started straight away. How to use the Jupyter Notebooks? Before we have a look at the notebooks themselves, we first have a brief look at the toolbar at the top of each notebook as shown in the following figure. The toolbar provides all buttons to interact with the code in a notebook. We go through the single buttons step by step. Important to understand is that each Jupyter Notebook consists of different cells. In each cell, you can execute a batch of code. In order to create a new cell, you simply click at the button and a new code cell will appear in the notebook. Try it yourself and create a new cell. You notebook should look like this now: In this cell, we can execute a simply command like print(\"Hello World!\") and execute it by pushing the or more easily by pushing the combination SHIFT + Enter . Congratulations, you just executed your first python command in a Jupyter Notebook! Just below the cell, the result in shown: In this case, the words 'Hello World!' are printed. Additionally, a new cell was automatically created just below the former one. Of course, we can also execute several commands within one cell. For this, please enter the following lines in the next cell: a = 5 a b = 3 a + b Before you execute the cell, think about the output you expect. Was the output as expected? Or did you expect to also see the value of a ? Keep in mind that Jupter Notebook will always return the last executed command only. In case you also want to see the value of a , you need to wrap a print(a) command around it. How to add documentation to your notebook? In order to increase the readability of the notebook, it helps to add some documentation around the single commands. This can be helpful in case you want to share the notebooks with others and want to explain why you performed a given command. The easiest way to to this is my changing the cell output in the toolbar: After doing so, you can enter some text in Markdown format around your cells. Test it by entering This was a ** nice ** example ! into a Markdown cell and execute it just as a normal code cell by pushing the button. How to save a Jupyter Notebook? Once you are done with a notebook you have two simple ways to share it with others: * Directly as a Jupyter Notebook. If the recipent has a Jupyter environment, he or she can directly continue working on your code. * Otherwise, you can also export the Notebook in html format: Bibliography [1] https://jupyter.org/","tags":"Time Series Preprocessing","url":"/Time Series Preprocessing/2020-01-01-getting-started.html","loc":"/Time Series Preprocessing/2020-01-01-getting-started.html"},{"title":"Getting Started with the Interactive Experimenting Environment","text":"Getting Started with the Interactive Experimenting Environment In the AI Starter Kits, we mainly work in so-called Jupyter Notebooks . The Jupyter Project is a non-profit, open-source project born in 2004. Jupyter Notebook, that was developed within the project, is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text [1]. If you have never worked with Jupyter Notebooks before, we invite you to read this short tutorial before starting with the actual content of the AI Starter Kits. We provide you some short explanations on how to use the web application to run the python code that was developed. How to get started? For the AI Starterkits, getting started is very easy! Just click at the link provided on the platform and it will bring you directly into Jupyter Hub, which hosts the Jupyter Notebooks for the Starter Kits. For each Starter Kit, we created a so-called environment in which all libraries needed are already installed such that you do not need to take care of this at all. You can get started straight away. How to use the Jupyter Notebooks? Before we have a look at the notebooks themselves, we first have a brief look at the toolbar at the top of each notebook as shown in the following figure. The toolbar provides all buttons to interact with the code in a notebook. We go through the single buttons step by step. Important to understand is that each Jupyter Notebook consists of different cells. In each cell, you can execute a batch of code. In order to create a new cell, you simply click at the button and a new code cell will appear in the notebook. Try it yourself and create a new cell. You notebook should look like this now: In this cell, we can execute a simply command like print(\"Hello World!\") and execute it by pushing the or more easily by pushing the combination SHIFT + Enter . Congratulations, you just executed your first python command in a Jupyter Notebook! Just below the cell, the result in shown: In this case, the words 'Hello World!' are printed. Additionally, a new cell was automatically created just below the former one. Of course, we can also execute several commands within one cell. For this, please enter the following lines in the next cell: a = 5 a b = 3 a + b Before you execute the cell, think about the output you expect. Was the output as expected? Or did you expect to also see the value of a ? Keep in mind that Jupter Notebook will always return the last executed command only. In case you also want to see the value of a , you need to wrap a print(a) command around it. How to add documentation to your notebook? In order to increase the readability of the notebook, it helps to add some documentation around the single commands. This can be helpful in case you want to share the notebooks with others and want to explain why you performed a given command. The easiest way to to this is my changing the cell output in the toolbar: After doing so, you can enter some text in Markdown format around your cells. Test it by entering This was a ** nice ** example ! into a Markdown cell and execute it just as a normal code cell by pushing the button. How to save a Jupyter Notebook? Once you are done with a notebook you have two simple ways to share it with others: * Directly as a Jupyter Notebook. If the recipent has a Jupyter environment, he or she can directly continue working on your code. * Otherwise, you can also export the Notebook in html format: Bibliography [1] https://jupyter.org/","tags":"Remaining Useful Life Prediction","url":"/Remaining Useful Life Prediction/2020-01-01-getting-started.html","loc":"/Remaining Useful Life Prediction/2020-01-01-getting-started.html"},{"title":"Interactive notebook","text":"In [1]: % load_ext autoreload % autoreload 2 # from IPython.core.display import display, HTML # deprecated from IPython.display import display , HTML display ( HTML ( \" \" )) In [2]: import numpy as np import pandas as pd import matplotlib import warnings warnings . filterwarnings ( 'ignore' ) pd . set_option ( 'display.float_format' , ' {:.3f} ' . format ) from support import read_consumption_data , read_climate_data , printmd from visualisations import run_app , plot_effects_resampling , plot_temperature_power_one_year , plot_yearly_data , \\ plot_weekly_data , plot_correlation , plot_consumption_with_holidays_weekends , plot_auto_correlation import holidays In [3]: data = read_consumption_data () ext_data = read_climate_data () 2022-06-14 16:26:14,045 [INFO ] Generating /Users/hcab/Documents/projects/elucidatalab.github/elucidatalab.starterkits/data/SK_1_3/household_power_consumption.txt Downloading file /Users/hcab/Documents/projects/elucidatalab.github/elucidatalab.starterkits/data/SK_1_3/household_power_consumption.txt.zip: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19.7M/19.7M [00:01<00:00, 17.9MiB/s] 2022-06-14 16:26:17,601 [INFO ] Using file: /Users/hcab/Documents/projects/elucidatalab.github/elucidatalab.starterkits/data/SK_1_3/household_power_consumption.txt 2022-06-14 16:26:37,612 [INFO ] Generating /Users/hcab/Documents/projects/elucidatalab.github/elucidatalab.starterkits/data/SK_1_3/hourly_temperature.csv Downloading file /Users/hcab/Documents/projects/elucidatalab.github/elucidatalab.starterkits/data/SK_1_3/hourly_temperature.csv.zip: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 144k/144k [00:00<00:00, 4.20MiB/s] 2022-06-14 16:26:38,469 [INFO ] Using file: /Users/hcab/Documents/projects/elucidatalab.github/elucidatalab.starterkits/data/SK_1_3/hourly_temperature.csv Starter Kit 1.3: Resource Demand Forecasting Business context Resource demand forecasting concerns accurately predicting the future need for a resource, typically using historical information. It is one of the most essential steps in resource demand management, i.e. ensuring sufficient resources are available to satisfy a fluctuating demand. Resource demand forecasting allows one to plan ahead and guarantee that sufficient resources are available when needed and to avoid costly countermeasures in case of shortage. A typical example of resource demand forecasting is provided by the energy sector. For energy suppliers it is more beneficial to buy electricity on the day-ahead market than on the spot market. Consequently, the more accurate the energy consumption can be predicted, the lower the cost. Given that more and more houses and appliances are enabled with a smart meter, an increasing amount of data becomes available that can enable and improve this prediction. Furthermore, these kinds of predictions also allow energy suppliers to better balance demand and supply, and helps them to ensure proper grid operation (and e.g., prepare for possible peak loads in winter time). This Starter Kit will focus energy consumption forecasting as a use case for illustrating how resource demand forecasting using historical data can be realized. Business goal The business goal related to this Starter Kit is forecasting the demand for a particular resource at a particular point in the future based on usage data of that resource that is available from the past. More specifically, we will illustrate a data-driven approach for forecasting the energy consumption of households. Application contexts Forecasting the demand of a particular resource can be useful for several purposes in a variety of industrial contexts: Forecast parking demand or traffic density in a particular neighbourhood in order to control traffic in intelligent ways (e.g. divert to alternative routes or parking spots) Predict battery consumption based on operating conditions in order to intelligently suggest recharging moments or switch to low-power mode Estimate the usage of consumables (e.g., ink, paper) based on product usage data in order to deliver new consumables exactly on time before stock breakage or avoid expensive storage of superfluous stock (e.g. in a manufacturing environment) ... Data requirements In order to realize the business goal, we need a dataset (time series) that includes: a value indicating the demand for the resource at each moment in time. the factors that influence the demand of the resource. These can be internal or external, for example: the energy consumption of a building is influenced by the outside temperature, the amount of people present in the building, etc. the consumption of raw materials is influenced by the type of product that is being produced, the production line settings, etc. the traffic density is influenced by the time of day, the presence of road works, the weather, etc. 'representative' historical data. The amount of data that is needed to make an accurate prediction is typically determined by the length of the temporal patterns in the data. For example, if the data expresses a yearly seasonality, historical data of multiple years is needed. data sampled with the 'appropriate' frequency (i.e., how many times per minute/hour/day you gather data points). This frequency is related to the required forecast frequency, e.g. it is impossible to forecast every 10 minutes when only 15-minute data is available. In some use cases, where resource demand forecasting concerns particular processes, (expert) knowledge on these processes is required (i.e. as labeled data) Starter Kit outline In this Starter Kit, we will illustrate a data-driven approach for forecasting the energy consumption of a household. As a dataset we will use the energy consumption of the household and climate data collected by a nearby weather station. We will use this dataset to illustrate: How to prepare your data for the analysis. We will describe how to handle the typical problems of raw data and how to perform data fusion when multiple sources of data are available. How to get insights from your dataset. We will present visual and numerical techniques in order to explore your data for identifying interesting patterns. How to transform the obtained insights into useful features for building forecasting models. How to compare correctly the performance of multiple forecasting models. Data loading and preprocessing For the household energy consumption use case we consider here, two types of data are relevant: Household data, which refers to the total household energy consumption along with the consumption of the individual energy-consuming appliances Climate data, which contains information of climate factors (e.g. temperature) that can influence on the energy consumption of households In the following subsections, we will load these datasets and perform some basic preprocessing, e.g. correctly handling missing values and fusing both datasets. Household data As a dataset we use the public energy consumption data set from the UCI repository . This dataset contains measurements of electric power consumption in one household, located near Paris, with a one-minute sampling rate over a period of almost 4 years. Different electrical quantities and sub-metering values are available, as can be seen in the table below: Date_Time : the date and time of day of the measurement Global_active_power : household global minute-averaged active power (in kilowatt) Global_reactive_power : household global minute-averaged reactive power (in kilowatt) Voltage : minute-averaged voltage (in volt) Global_intensity : household global minute-averaged current intensity (in ampere) Sub_metering_1 : energy sub-metering No. 1 (in watt-hour of active energy). It corresponds to the kitchen, containing mainly a dishwasher, an oven and a microwave (hot plates are not electric but gas powered). Sub_metering_2 : energy sub-metering No. 2 (in watt-hour of active energy). It corresponds to the laundry room, containing a washing-machine, a tumble-drier, a refrigerator and a light. Sub_metering_3 : energy sub-metering No. 3 (in watt-hour of active energy). It corresponds to an electric water-heater and an air-conditioner. In [4]: start_date = data . index . min () . date () end_date = data . index . max () . date () printmd ( f \"The dataset contains data from { start_date } to { end_date } .\" ) data . head () The dataset contains data from 2007-01-01 to 2010-11-26. Out[4]: Global_active_power Global_reactive_power Voltage Global_intensity Sub_metering_1 Sub_metering_2 Sub_metering_3 Date_Time 2007-01-01 00:00:00 2.580 0.136 241.970 10.600 0.000 0.000 0.000 2007-01-01 00:01:00 2.552 0.100 241.750 10.400 0.000 0.000 0.000 2007-01-01 00:02:00 2.550 0.100 241.640 10.400 0.000 0.000 0.000 2007-01-01 00:03:00 2.550 0.100 241.710 10.400 0.000 0.000 0.000 2007-01-01 00:04:00 2.554 0.100 241.980 10.400 0.000 0.000 0.000 From the table below, we can see that for all sensors we have 2.049.280 instances. This already indicates that there are no missing values. Besides this information, we do not spot immediately any strange insights, e.g. an anomaly in sensor data like negative consumed power. In [5]: data . describe () Out[5]: Global_active_power Global_reactive_power Voltage Global_intensity Sub_metering_1 Sub_metering_2 Sub_metering_3 count 2027288.000 2027288.000 2027288.000 2027288.000 2027288.000 2027288.000 2027288.000 mean 1.083 0.124 240.833 4.591 1.121 1.289 6.448 std 1.049 0.113 3.231 4.411 6.147 5.786 8.434 min 0.076 0.000 223.200 0.200 0.000 0.000 0.000 25% 0.308 0.048 238.990 1.400 0.000 0.000 0.000 50% 0.594 0.100 241.000 2.600 0.000 0.000 1.000 75% 1.520 0.194 242.870 6.400 0.000 1.000 17.000 max 11.122 1.390 254.150 48.400 88.000 80.000 31.000 For the purpose of this Starter Kit, only the Date_Time and the Global_active_power attributes will be considered from now onwards. In [6]: data = data [[ 'Global_active_power' ]] Climate data As an additional dataset, we use the climate information recorded close to the city of Paris, where the household is located, provided by the Wunderground website. It contains information on climate conditions for every 30 minutes, via the following attributes shown in the table below: time_UTC : the date and hour of the measurement temperature : temperature (degrees Celsius) dew_point : dew point (degrees Celsius) humidity : air humidity (percentage) sea_lvl_pressure : pressure at sea level (hPa) visibility : visibility (km) wind_dir_degrees : wind direction expressed in degrees (degrees) In [7]: ext_data . head ( 5 ) Out[7]: datetime 2007-01-01 00:00:00 14.210 2007-01-01 01:00:00 14.110 2007-01-01 02:00:00 13.910 2007-01-01 03:00:00 13.010 2007-01-01 04:00:00 11.910 Name: temperature, dtype: float64 From the statistics in the table below, we can see the data contains minimum values of -9999 for some variables. According to the documentation , values of -9999 or -999 signify missing values. We will replace these with the proper missing value NaN, as otherwise our algorithms would get confused. In [8]: ext_data . describe () Out[8]: count 34164.000 mean 12.407 std 13.868 min -8.710 25% 7.210 50% 12.310 75% 17.310 max 999.990 Name: temperature, dtype: float64 In [9]: ext_data . replace ( to_replace =- 9999 , value = np . nan , inplace = True ) ext_data . describe () Out[9]: count 34164.000 mean 12.407 std 13.868 min -8.710 25% 7.210 50% 12.310 75% 17.310 max 999.990 Name: temperature, dtype: float64 We can now notice that the temperature values seem to be more plausible. As expected, we also see that not all sensors have the same number of instances, meaning there are missing values. For the purpose of this Starter Kit, only the time_UTC and temperature attributes will be considered from now on. Data fusion The evolution of household energy consumption may be influenced by climatological factors, e.g. lower or higher temperatures can lead to a higher energy consumption for some appliances such as water-heaters and air-conditioners. To understand and analyze this relationship, we need to fuse the household data with the climate data. However, these datasets have different sampling time: the household data is sampled every minute, whereas the climate data is sampled every 30 minutes. The integration of these 2 datasets thus requires careful consideration. Two options are possible: we can either upsample the climate data or downsample the household data. We select the second option, as otherwise, we would need to find a way to impute missing data in the climate dataset, and it allows us to reduce the size of the dataset, which will reduce compute time required for the analysis. The effect of downsampling the household data can be seen below. The user can experiment how the window sizes of 30 minutes, 1 hour, or 4 hours affect the evolution of the global active power. In [10]: plot_effects_resampling ( data ) As we can see from the plot, resampling reduces the (small) peaks of the original signal. The effect becomes more pronounced for larger window sizes. A window size of 4 hours arguably removes too much variation. A window of 30 minutes or 1 hours on the other hand still captures the general evolution of the active power, which is exactly what is needed for characterizing the typical household consumption. Between 30 or 60min time windows, we decide to use a window size of 60min since this reduces the dataset and speeds up the computation time required later on for the analysis. To downsample the climate dataset, the mean of the values of the respective hour is used. To downsample the household dataset, the sum of each of the values for the corresponding hour is taken An excerpt of the downsampling of household dataset is shown in the table below. In [11]: ext_data_h = ext_data . resample ( '1H' ) . apply ( np . mean ) data_h = data . resample ( '1H' ) . agg ({ 'Global_active_power' : np . sum , }) data_h [: 3 ] Out[11]: Global_active_power Date_Time 2007-01-01 00:00:00 153.038 2007-01-01 01:00:00 151.404 2007-01-01 02:00:00 154.940 Since Global_reactive_power and Global_active_power are expressed in minute-averaged kW, we will change their units to Wh over an hour, such that this is in line with the resampling frequency. To this end, the following transformations are applied (the summing was already done in the previous step): \\begin{align*} \\mathit{globalactive}\\ Wh &= \\sum&#94;{60}_{min=1} 1000\\cdot\\mathit{globalactivepower}_{\\mathit{min}}\\ kW\\cdot \\frac{1}{60}\\ h\\\\ \\end{align*} The result of this transformation can be seen in the table below. In [12]: data_h [ 'Global_active_power' ] = data_h [ 'Global_active_power' ] . apply ( lambda x : x * 1000 / 60 ) data_h [: 3 ] Out[12]: Global_active_power Date_Time 2007-01-01 00:00:00 2550.633 2007-01-01 01:00:00 2523.400 2007-01-01 02:00:00 2582.333 Now we can finally merge the two datasets into an extended dataset that will be explored in the next section. In [13]: new_data = pd . concat (( data_h , ext_data_h ), axis = 1 , join = 'inner' ) new_data [: 3 ] Out[13]: Global_active_power temperature 2007-01-01 00:00:00 2550.633 14.210 2007-01-01 01:00:00 2523.400 14.110 2007-01-01 02:00:00 2582.333 13.910 Before proceeding, we inspect to what extent this extended dataset contains missing data. In [14]: new_data . isna () . mean () Out[14]: Global_active_power 0.000 temperature 0.025 dtype: float64 In case of too many missing values, we could consider data imputation. However, in this case, only the temperature has 0.01% of msising values, so there is no need for imputation. Data Exploration In this section, we explore the extended dataset in order to understand which factors influence the household energy consumption. Based on this exploration, we will later on identify which features are useful for training a machine learning algorithm to forecast energy consumption. There are several approaches to data exploration. In this section we will use: visual exploration by means of time plots statistical data exploration Visual Data Exploration Time plots are a common way for exploring time series data, such as the energy consumption that we consider here. We explore time plots in different time frames in order to highlight different aspects in the data: we start with a yearly plot, which allows us to identify global patterns. a monthly plot allows us to zoom in and understand relationships between energy consumption in different weeks and days a weekly plot allows us to zoom in even further and identify daily and hourly patterns Yearly patterns It is expected that energy consumption depends on the outdoor temperature, since in winter time heaters are used more frequently than in summer time for example. For this reason, we plot the evolution of the energy consumption along with the temperature. In the plot below, we resample the hourly data of both datasets to daily data, so that we don't get lost in details but can observe more global patterns. The user can toggle on or off the normalization of the data. The normalization that is applied is called min-max normalization and is explained in detail in SK 3.4. It applies the following formulate to the data in order to make sure all values are contained within the [0, 1] range: \\begin{align*} \\mathit{X_{scaled}} = \\frac{X - X_{min}}{X_{max} - {X_{min}}}\\\\ \\end{align*} The user is invited to verify, without selecting the option normalization, whether of temperature and global active power have the same trend. The reader can then repeat the analysis using the option normalization. In [15]: plot_temperature_power_one_year ( new_data ) It should be clear that without normalization, it is difficult to compare the temperature with the global active power because both use different ranges, and the range of the global active power dominates the range of the temperature. With normalization however, the plot above reveals interesting insights. Both the temperature and the global active power follow a seasonal trend. Obviously, the temperature is lower in winter and higher in summer, whereas the reverse is true for the global active power. This is to be expected: energy consumption is higher during winter time. In more formal terms, we can observe that the temperate and global active power exhibit an inverse correlation: when one increases the other decreases, and vice versa. Monthly patterns Let's now see if there is a monthly recurring pattern as well. In the following figure, the active power of the selected year(s) is shown. The user can change the resampling rate to make the insights more clear to them. To make the plot more clear, the user can (de)selected the years they want on the right of the figure. In [16]: plot_yearly_data ( new_data ) While we can again clearly see the seasonal effect, there is no clear pattern recurring every month. One interesting thing that can be seen is that strong dips in power consumption tend to happen in the same time in February and November in 2007 and 2008. Most likely, these are holidays which tend to happen at the same time for a fixed household. However, these dips are not present at the same time for 2009. Weekly patterns In this section we further explore the evolution of the energy consumption in a more fine grained time frame. We want to verify whether during a week, the energy consumption exhibits the same patterns. In the plot below, the user can inspect and compare the energy consumption in December of different years. We mark weekends in yellow and Christmas day in orange. We invite the user to see for each year if they can notice a difference between week days and weekends and if there are any changes around the Holidays. In [17]: plot_consumption_with_holidays_weekends ( new_data ) What should be obvious from exploring this plot is the existence of a daily pattern for energy consumption dependent on the day of the week: in most weekend days of the three years, we can observe slightly higher energy consumption than during weekdays. This can depend on the common habits of working at the office or children being at school. The effect of the holidays can also be seen. On Christmas day in 2007, the power consumption is slightly higher than other typical weekdays, but similar values can be seen e.g. exactly one week before. So the holidays don't have a strong impact here. In 2008 on the other hand, the power consumption is lower, indicating that perhaps the household celebrated outside their house. In 2009, there is a peak in consumption on Christmas eve. We can conclude that whether the day is a weekday or in the weekend or a holiday has an influence on the expected power consumption. Daily patterns Next to yearly and monthly patterns, we are also interested in daily patterns. Below, we plot the evolution of the global active power for one week (The first week of December 2009). We can see how the power consumption follows a daily recurring pattern, with peaks in the morning and evening and troughs at night and midday. In fact, this pattern is also already present in the previous plot, where daily morning and evening peaks can also be observed. During the night and the day, we still see low amounts of power usage, most likely due to appliances like refrigerators and heating. In [18]: plot_weekly_data ( new_data ) Statistical Data Exploration The visualizations above highlighted interesting patterns in the energy consumption of the household that can be exploited for feature extraction. In this section we use statistical methods to verify, and eventually quantify those observations. Autocorrelation In order to find repetitive patterns in time series data, we can use autocorrelation , which provides the correlation between the time series and a delayed copy of itself. In order to investigate this autocorrelation, the plot below visualizes the autocorrelation of the global active power with time lags of 1 day, 2 days, etc. Note that we do not show the results of a delay of 0 days, since the correlation with the exact same day would trivially be 1. In [19]: plot_auto_correlation ( new_data ) The plot above reveals that a clear peak occurs after 1 day, indicating the consumption of a day is highly correlated with the consumption of the previous day another clear peak occurs every 7 days, which confirms the existence of a weekly pattern, i.e. the consumption of a day is highly correlated with the consumption of the same day a week earlier Correlation As visually observed above, the temperature influences the energy consumption of a household. In order to verify the strength of this relationship, we can compute the correlation between these two variables. For this purpose, we compute Pearson's correlation. The plot below shows again both temperature and consumption and allows to investigate their relationship. This relationship is influenced by the frequency of the dataset, e.g. temperature changes are typically less drastic than energy consumption changes. We invite the user to experiment with the different resampling periods, to understand how that changes the correlation and what frequency leads to the highest correlation. In [20]: plot_correlation ( new_data ) What should be obvious from the above is that there is a negative correlation between temperature and energy consumption, meaning the consumption rises when temperature drops and vice versa. The larger the resampling window is chosen, the higher is the correlation. This is because with larger windows, we smooth out more the short-term fluctuations, leaving only the seasonal pattern described above. Feature engineering In this section, we will extract several features from the extended dataset that can be used for forecasting energy consumption. A feature is an individual measurable property of the phenomenon being observed. Choosing informative, discriminating and independent features is a crucial step for building effective machine learning algorithms. Based on the previous visual and statistical observation, we will consider the following features to predict the energy consumption for the hour ahead: energy consumption of the day before energy consumption of the 7 days before the hour of the day, which will allow to distinguish between different periods of the day (i.e. night, morning, afternoon, evening) the day of the week, which will allow to distinguish between weekdays and weekend days whether the day is a holiday or not the month of the year, which will allow to distinguish between yearly seasons the temperature In [21]: new_feats = new_data [[ 'Global_active_power' ]] . rename ( columns = ({ 'Global_active_power' : 'consumption' })) new_feats [ 'Pastday' ] = new_feats [ 'consumption' ] . shift ( 1 , freq = 'D' ) new_feats [ 'Pastweek' ] = new_feats [ 'consumption' ] . shift ( 7 , freq = 'D' ) new_feats [ 'Hour' ] = new_feats . index . hour new_feats [ 'Weekday' ] = new_feats . index . dayofweek new_feats [ 'Month' ] = new_feats . index . month new_feats [ 'Holiday' ] = [ 1 if ( date in holidays . France ()) else 0 for date in new_feats . index ] new_feats [ 'Temperature' ] = new_data [ 'temperature' ] new_feats = new_feats . dropna () Modeling This section allows the user to discover the most important factors for training a machine learning model. More specifically, the user will get insights on the influence of the training strategy , type of machine learning model and effect of data normalization on model performance. To evaluate the performance of the models the mean absolute error (MAE) is used (a metric commonly used in literature for this purpose). This metric quantifies to what extent the model forecasts are close to the real values. As the name suggests, the mean absolute error is an average of the absolute errors. It holds that the lower the MAE of the model, the better its performance. This section ends with an interactive application where the user can test all options in order to find the one that leads the model to achieve the best performance. Training strategies The training data influences models performances. For this experiment, the following options are available: Use 1 month before the test month as training data. In this experiment, several models are trained for the predictions of each individual month. Each model is trained on one month before the month we are making predictions for. One month gap is introduced between the training and the test month (this is done to avoid that the last day of the training set is also included in the first day of the test set). For example, the models that will make the predictions for April 2008 will be trained on the data from February 2008. Use 6 months before the test month as training data. Similar to the above experiment, but with 6 months of training data. It still includes a 1 month gap. Use 1 year before the test month as training data. Similar to the above experiment, but with 1 year of training data, still include a 1 month gap. Use 1 month the year before as training data. Similar to strategy number 1, but with an 11 months gap. This way, the training data and test data are taken from the same month, one year apart. Use all months before the test month as training data. For each test month, train a model using all the data prior to this (including a 1 month gap). This strategy simulates the scenario where the model is retrained as new data comes in. Since this requires training several models based on potentially large training sets, this might require a certain computational time. Use train-test split . Train on the first year of data and make predictions on the rest. This strategy is commonly used for training these kinds of models. Notice that for the benchmark model, there is no training phase. Machine learning models There is a plethora of models that can be used for forecasting. In this notebook, we decide to use two models that are commonly used in literature for the task at hand: Random Forest Regression (RFR) and Support Vector Regression (SVR) . Besides these models, we will also use a simple benchmark model that we use to compare the performance of the other models against. For the prediction of the energy of a certain day, this benchmark model simply takes the energy consumption of the previous day at the same time. We will use these models to forecast the energy consumption based on the past energy measurements (see training strategies). Normalization Before being used for training, data may need to be normalized, i.e. rescaled so that all input and output values are between 0 and 1. This is indeed a requirement for the correct training of some machine learning models. It is left to the user the exercise of discovering which model requires the normalization of the data. In [22]: from IPython.utils import io PORT = '8095' # in AWS use ports 8050-8060 mode = 'external' #external if mode == 'inline' : run_app ( new_feats , mode = 'inline' , port = PORT ) else : with io . capture_output () as captured : run_app ( new_feats , mode = 'external' , port = PORT , host = '0.0.0.0' ) HOST = '0.0.0.0' # in AWS replace here by public IP print ( f 'Dash app running on http:// { HOST } : { PORT } ' ) Dash app running on http://0.0.0.0:8095 The mean absolute error of each experiment that was run is reported in the table above. Insights from the experiments These are the basic insights the user could have noticed when trying different models, parameters and training strategies. Effect of the training strategy For the SVR, we see that the standard train-test split has the best performance, although using all months and using a one year window before the test month as training data both have a similar performance. For the RFR on the other hand, we clearly see that using a one year window before the test month as training data leads to better performance than the other training strategies. We note that for both this \"one year window\" strategy and the standard train-test split, the training data is one year. The difference however, is that for the former the training set changes, while for the latter it is fixed (using all the data from 2017). A general trend that can be seen is that the performance increases as a larger training set is used. We therefore urge the user to look at the predictions made using the training strategy where all months before the test month are used as training data: we can expect that the predictions for the later months will be better than for the first months since more training data was used. However, this is hard to see by eye in the prediction plot directly. Effect of the chosen model We can see that, for most parameter choices, both the SVR and RFR can outperform the simple benchmark model we set up. The RFR outperforms the SVR for most (if not all) parameter choices and training strategies. This shows the importance of knowing which model is best suited for your problem and testing different ones. Effect of the chosen parameters The search for the optimal set of hyperparameters for your machine learning model is a non-trivial task, since there might be a strong interplay between the parameters so we can not do the optimal parameter search one by one. Here, we let the user try a pre-set number of choices, but more automatic and smart search algorithms exist to tackle this problem. We found that for the RFR, the higher the number of estimators, the better the results, but at a greater computational cost and with diminishing returns. We see that 50 estimators and leaving the max depth at 10 is a good choice. For the SVR, $C=10$ and $\\gamma=0.01$ work well. Effect of normalization Normalizing the data, rescaling it so that the input and output variables all have values within a similar range (in this case, between 0 and 1) is a common step when setting up machine learning models. This is because, depending on the model, they work much better with values of the order 1. We see that normalization indeed greatly improves the predictions for the SVR, but has very little influence on the RFR. Indeed, algorithms such as random forest and decision tree are not influenced by the scale of the input variables. Conclusion In this Starter Kit we demonstrated forecasting the demand for a particular resource at a particular point in the future, illustrated in the case of energy demand forecasting. To this end, we first analysed the business problem, preprocessed the data, visually and numerically explored it to eventually illustrate the use of different models ( Random Forest Regression, Support Vector Regression and benchmark) to perform the actual forecasting with different amounts of available historical data. From the analysis, it may be clear that achieving good prediction results depend on different factors (appropriate data preprocessing, the amount of available training data, algorithm parametrisation, etc.). Additional Information Contributions by Mathias Verbeke, Wannes Meert, Vincent Vercruyssen, Alessandro Murgia, Tom Tourw√©, and Robbert Verbeke. This Starter Kit was developed in the context of the EluciDATA project ( http://www.elucidata.be ). For more information, please contact info@elucidata.be .","tags":"Resource demand Forecasting","url":"/Resource demand Forecasting/2020-01-01-interactive-notebook.html","loc":"/Resource demand Forecasting/2020-01-01-interactive-notebook.html"},{"title":"Interactive notebook","text":"In [1]: % load_ext autoreload % autoreload 2 # from IPython.core.display import display, HTML # deprecated from IPython.display import display , HTML display ( HTML ( \" \" )) In [2]: import numpy as np import pandas as pd import matplotlib import warnings warnings . filterwarnings ( 'ignore' ) pd . set_option ( 'display.float_format' , ' {:.3f} ' . format ) from support import read_consumption_data , read_climate_data , printmd from visualisations import run_app , plot_effects_resampling , plot_temperature_power_one_year , plot_yearly_data , \\ plot_weekly_data , plot_correlation , plot_consumption_with_holidays_weekends , plot_auto_correlation import holidays In [3]: data = read_consumption_data () ext_data = read_climate_data () 2022-06-14 16:26:14,045 [INFO ] Generating /Users/hcab/Documents/projects/elucidatalab.github/elucidatalab.starterkits/data/SK_1_3/household_power_consumption.txt Downloading file /Users/hcab/Documents/projects/elucidatalab.github/elucidatalab.starterkits/data/SK_1_3/household_power_consumption.txt.zip: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19.7M/19.7M [00:01<00:00, 17.9MiB/s] 2022-06-14 16:26:17,601 [INFO ] Using file: /Users/hcab/Documents/projects/elucidatalab.github/elucidatalab.starterkits/data/SK_1_3/household_power_consumption.txt 2022-06-14 16:26:37,612 [INFO ] Generating /Users/hcab/Documents/projects/elucidatalab.github/elucidatalab.starterkits/data/SK_1_3/hourly_temperature.csv Downloading file /Users/hcab/Documents/projects/elucidatalab.github/elucidatalab.starterkits/data/SK_1_3/hourly_temperature.csv.zip: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 144k/144k [00:00<00:00, 4.20MiB/s] 2022-06-14 16:26:38,469 [INFO ] Using file: /Users/hcab/Documents/projects/elucidatalab.github/elucidatalab.starterkits/data/SK_1_3/hourly_temperature.csv Starter Kit 1.3: Resource Demand Forecasting Business context Resource demand forecasting concerns accurately predicting the future need for a resource, typically using historical information. It is one of the most essential steps in resource demand management, i.e. ensuring sufficient resources are available to satisfy a fluctuating demand. Resource demand forecasting allows one to plan ahead and guarantee that sufficient resources are available when needed and to avoid costly countermeasures in case of shortage. A typical example of resource demand forecasting is provided by the energy sector. For energy suppliers it is more beneficial to buy electricity on the day-ahead market than on the spot market. Consequently, the more accurate the energy consumption can be predicted, the lower the cost. Given that more and more houses and appliances are enabled with a smart meter, an increasing amount of data becomes available that can enable and improve this prediction. Furthermore, these kinds of predictions also allow energy suppliers to better balance demand and supply, and helps them to ensure proper grid operation (and e.g., prepare for possible peak loads in winter time). This Starter Kit will focus energy consumption forecasting as a use case for illustrating how resource demand forecasting using historical data can be realized. Business goal The business goal related to this Starter Kit is forecasting the demand for a particular resource at a particular point in the future based on usage data of that resource that is available from the past. More specifically, we will illustrate a data-driven approach for forecasting the energy consumption of households. Application contexts Forecasting the demand of a particular resource can be useful for several purposes in a variety of industrial contexts: Forecast parking demand or traffic density in a particular neighbourhood in order to control traffic in intelligent ways (e.g. divert to alternative routes or parking spots) Predict battery consumption based on operating conditions in order to intelligently suggest recharging moments or switch to low-power mode Estimate the usage of consumables (e.g., ink, paper) based on product usage data in order to deliver new consumables exactly on time before stock breakage or avoid expensive storage of superfluous stock (e.g. in a manufacturing environment) ... Data requirements In order to realize the business goal, we need a dataset (time series) that includes: a value indicating the demand for the resource at each moment in time. the factors that influence the demand of the resource. These can be internal or external, for example: the energy consumption of a building is influenced by the outside temperature, the amount of people present in the building, etc. the consumption of raw materials is influenced by the type of product that is being produced, the production line settings, etc. the traffic density is influenced by the time of day, the presence of road works, the weather, etc. 'representative' historical data. The amount of data that is needed to make an accurate prediction is typically determined by the length of the temporal patterns in the data. For example, if the data expresses a yearly seasonality, historical data of multiple years is needed. data sampled with the 'appropriate' frequency (i.e., how many times per minute/hour/day you gather data points). This frequency is related to the required forecast frequency, e.g. it is impossible to forecast every 10 minutes when only 15-minute data is available. In some use cases, where resource demand forecasting concerns particular processes, (expert) knowledge on these processes is required (i.e. as labeled data) Starter Kit outline In this Starter Kit, we will illustrate a data-driven approach for forecasting the energy consumption of a household. As a dataset we will use the energy consumption of the household and climate data collected by a nearby weather station. We will use this dataset to illustrate: How to prepare your data for the analysis. We will describe how to handle the typical problems of raw data and how to perform data fusion when multiple sources of data are available. How to get insights from your dataset. We will present visual and numerical techniques in order to explore your data for identifying interesting patterns. How to transform the obtained insights into useful features for building forecasting models. How to compare correctly the performance of multiple forecasting models. Data loading and preprocessing For the household energy consumption use case we consider here, two types of data are relevant: Household data, which refers to the total household energy consumption along with the consumption of the individual energy-consuming appliances Climate data, which contains information of climate factors (e.g. temperature) that can influence on the energy consumption of households In the following subsections, we will load these datasets and perform some basic preprocessing, e.g. correctly handling missing values and fusing both datasets. Household data As a dataset we use the public energy consumption data set from the UCI repository . This dataset contains measurements of electric power consumption in one household, located near Paris, with a one-minute sampling rate over a period of almost 4 years. Different electrical quantities and sub-metering values are available, as can be seen in the table below: Date_Time : the date and time of day of the measurement Global_active_power : household global minute-averaged active power (in kilowatt) Global_reactive_power : household global minute-averaged reactive power (in kilowatt) Voltage : minute-averaged voltage (in volt) Global_intensity : household global minute-averaged current intensity (in ampere) Sub_metering_1 : energy sub-metering No. 1 (in watt-hour of active energy). It corresponds to the kitchen, containing mainly a dishwasher, an oven and a microwave (hot plates are not electric but gas powered). Sub_metering_2 : energy sub-metering No. 2 (in watt-hour of active energy). It corresponds to the laundry room, containing a washing-machine, a tumble-drier, a refrigerator and a light. Sub_metering_3 : energy sub-metering No. 3 (in watt-hour of active energy). It corresponds to an electric water-heater and an air-conditioner. In [4]: start_date = data . index . min () . date () end_date = data . index . max () . date () printmd ( f \"The dataset contains data from { start_date } to { end_date } .\" ) data . head () The dataset contains data from 2007-01-01 to 2010-11-26. Out[4]: Global_active_power Global_reactive_power Voltage Global_intensity Sub_metering_1 Sub_metering_2 Sub_metering_3 Date_Time 2007-01-01 00:00:00 2.580 0.136 241.970 10.600 0.000 0.000 0.000 2007-01-01 00:01:00 2.552 0.100 241.750 10.400 0.000 0.000 0.000 2007-01-01 00:02:00 2.550 0.100 241.640 10.400 0.000 0.000 0.000 2007-01-01 00:03:00 2.550 0.100 241.710 10.400 0.000 0.000 0.000 2007-01-01 00:04:00 2.554 0.100 241.980 10.400 0.000 0.000 0.000 From the table below, we can see that for all sensors we have 2.049.280 instances. This already indicates that there are no missing values. Besides this information, we do not spot immediately any strange insights, e.g. an anomaly in sensor data like negative consumed power. In [5]: data . describe () Out[5]: Global_active_power Global_reactive_power Voltage Global_intensity Sub_metering_1 Sub_metering_2 Sub_metering_3 count 2027288.000 2027288.000 2027288.000 2027288.000 2027288.000 2027288.000 2027288.000 mean 1.083 0.124 240.833 4.591 1.121 1.289 6.448 std 1.049 0.113 3.231 4.411 6.147 5.786 8.434 min 0.076 0.000 223.200 0.200 0.000 0.000 0.000 25% 0.308 0.048 238.990 1.400 0.000 0.000 0.000 50% 0.594 0.100 241.000 2.600 0.000 0.000 1.000 75% 1.520 0.194 242.870 6.400 0.000 1.000 17.000 max 11.122 1.390 254.150 48.400 88.000 80.000 31.000 For the purpose of this Starter Kit, only the Date_Time and the Global_active_power attributes will be considered from now onwards. In [6]: data = data [[ 'Global_active_power' ]] Climate data As an additional dataset, we use the climate information recorded close to the city of Paris, where the household is located, provided by the Wunderground website. It contains information on climate conditions for every 30 minutes, via the following attributes shown in the table below: time_UTC : the date and hour of the measurement temperature : temperature (degrees Celsius) dew_point : dew point (degrees Celsius) humidity : air humidity (percentage) sea_lvl_pressure : pressure at sea level (hPa) visibility : visibility (km) wind_dir_degrees : wind direction expressed in degrees (degrees) In [7]: ext_data . head ( 5 ) Out[7]: datetime 2007-01-01 00:00:00 14.210 2007-01-01 01:00:00 14.110 2007-01-01 02:00:00 13.910 2007-01-01 03:00:00 13.010 2007-01-01 04:00:00 11.910 Name: temperature, dtype: float64 From the statistics in the table below, we can see the data contains minimum values of -9999 for some variables. According to the documentation , values of -9999 or -999 signify missing values. We will replace these with the proper missing value NaN, as otherwise our algorithms would get confused. In [8]: ext_data . describe () Out[8]: count 34164.000 mean 12.407 std 13.868 min -8.710 25% 7.210 50% 12.310 75% 17.310 max 999.990 Name: temperature, dtype: float64 In [9]: ext_data . replace ( to_replace =- 9999 , value = np . nan , inplace = True ) ext_data . describe () Out[9]: count 34164.000 mean 12.407 std 13.868 min -8.710 25% 7.210 50% 12.310 75% 17.310 max 999.990 Name: temperature, dtype: float64 We can now notice that the temperature values seem to be more plausible. As expected, we also see that not all sensors have the same number of instances, meaning there are missing values. For the purpose of this Starter Kit, only the time_UTC and temperature attributes will be considered from now on. Data fusion The evolution of household energy consumption may be influenced by climatological factors, e.g. lower or higher temperatures can lead to a higher energy consumption for some appliances such as water-heaters and air-conditioners. To understand and analyze this relationship, we need to fuse the household data with the climate data. However, these datasets have different sampling time: the household data is sampled every minute, whereas the climate data is sampled every 30 minutes. The integration of these 2 datasets thus requires careful consideration. Two options are possible: we can either upsample the climate data or downsample the household data. We select the second option, as otherwise, we would need to find a way to impute missing data in the climate dataset, and it allows us to reduce the size of the dataset, which will reduce compute time required for the analysis. The effect of downsampling the household data can be seen below. The user can experiment how the window sizes of 30 minutes, 1 hour, or 4 hours affect the evolution of the global active power. In [10]: plot_effects_resampling ( data ) As we can see from the plot, resampling reduces the (small) peaks of the original signal. The effect becomes more pronounced for larger window sizes. A window size of 4 hours arguably removes too much variation. A window of 30 minutes or 1 hours on the other hand still captures the general evolution of the active power, which is exactly what is needed for characterizing the typical household consumption. Between 30 or 60min time windows, we decide to use a window size of 60min since this reduces the dataset and speeds up the computation time required later on for the analysis. To downsample the climate dataset, the mean of the values of the respective hour is used. To downsample the household dataset, the sum of each of the values for the corresponding hour is taken An excerpt of the downsampling of household dataset is shown in the table below. In [11]: ext_data_h = ext_data . resample ( '1H' ) . apply ( np . mean ) data_h = data . resample ( '1H' ) . agg ({ 'Global_active_power' : np . sum , }) data_h [: 3 ] Out[11]: Global_active_power Date_Time 2007-01-01 00:00:00 153.038 2007-01-01 01:00:00 151.404 2007-01-01 02:00:00 154.940 Since Global_reactive_power and Global_active_power are expressed in minute-averaged kW, we will change their units to Wh over an hour, such that this is in line with the resampling frequency. To this end, the following transformations are applied (the summing was already done in the previous step): \\begin{align*} \\mathit{globalactive}\\ Wh &= \\sum&#94;{60}_{min=1} 1000\\cdot\\mathit{globalactivepower}_{\\mathit{min}}\\ kW\\cdot \\frac{1}{60}\\ h\\\\ \\end{align*} The result of this transformation can be seen in the table below. In [12]: data_h [ 'Global_active_power' ] = data_h [ 'Global_active_power' ] . apply ( lambda x : x * 1000 / 60 ) data_h [: 3 ] Out[12]: Global_active_power Date_Time 2007-01-01 00:00:00 2550.633 2007-01-01 01:00:00 2523.400 2007-01-01 02:00:00 2582.333 Now we can finally merge the two datasets into an extended dataset that will be explored in the next section. In [13]: new_data = pd . concat (( data_h , ext_data_h ), axis = 1 , join = 'inner' ) new_data [: 3 ] Out[13]: Global_active_power temperature 2007-01-01 00:00:00 2550.633 14.210 2007-01-01 01:00:00 2523.400 14.110 2007-01-01 02:00:00 2582.333 13.910 Before proceeding, we inspect to what extent this extended dataset contains missing data. In [14]: new_data . isna () . mean () Out[14]: Global_active_power 0.000 temperature 0.025 dtype: float64 In case of too many missing values, we could consider data imputation. However, in this case, only the temperature has 0.01% of msising values, so there is no need for imputation. Data Exploration In this section, we explore the extended dataset in order to understand which factors influence the household energy consumption. Based on this exploration, we will later on identify which features are useful for training a machine learning algorithm to forecast energy consumption. There are several approaches to data exploration. In this section we will use: visual exploration by means of time plots statistical data exploration Visual Data Exploration Time plots are a common way for exploring time series data, such as the energy consumption that we consider here. We explore time plots in different time frames in order to highlight different aspects in the data: we start with a yearly plot, which allows us to identify global patterns. a monthly plot allows us to zoom in and understand relationships between energy consumption in different weeks and days a weekly plot allows us to zoom in even further and identify daily and hourly patterns Yearly patterns It is expected that energy consumption depends on the outdoor temperature, since in winter time heaters are used more frequently than in summer time for example. For this reason, we plot the evolution of the energy consumption along with the temperature. In the plot below, we resample the hourly data of both datasets to daily data, so that we don't get lost in details but can observe more global patterns. The user can toggle on or off the normalization of the data. The normalization that is applied is called min-max normalization and is explained in detail in SK 3.4. It applies the following formulate to the data in order to make sure all values are contained within the [0, 1] range: \\begin{align*} \\mathit{X_{scaled}} = \\frac{X - X_{min}}{X_{max} - {X_{min}}}\\\\ \\end{align*} The user is invited to verify, without selecting the option normalization, whether of temperature and global active power have the same trend. The reader can then repeat the analysis using the option normalization. In [15]: plot_temperature_power_one_year ( new_data ) It should be clear that without normalization, it is difficult to compare the temperature with the global active power because both use different ranges, and the range of the global active power dominates the range of the temperature. With normalization however, the plot above reveals interesting insights. Both the temperature and the global active power follow a seasonal trend. Obviously, the temperature is lower in winter and higher in summer, whereas the reverse is true for the global active power. This is to be expected: energy consumption is higher during winter time. In more formal terms, we can observe that the temperate and global active power exhibit an inverse correlation: when one increases the other decreases, and vice versa. Monthly patterns Let's now see if there is a monthly recurring pattern as well. In the following figure, the active power of the selected year(s) is shown. The user can change the resampling rate to make the insights more clear to them. To make the plot more clear, the user can (de)selected the years they want on the right of the figure. In [16]: plot_yearly_data ( new_data ) While we can again clearly see the seasonal effect, there is no clear pattern recurring every month. One interesting thing that can be seen is that strong dips in power consumption tend to happen in the same time in February and November in 2007 and 2008. Most likely, these are holidays which tend to happen at the same time for a fixed household. However, these dips are not present at the same time for 2009. Weekly patterns In this section we further explore the evolution of the energy consumption in a more fine grained time frame. We want to verify whether during a week, the energy consumption exhibits the same patterns. In the plot below, the user can inspect and compare the energy consumption in December of different years. We mark weekends in yellow and Christmas day in orange. We invite the user to see for each year if they can notice a difference between week days and weekends and if there are any changes around the Holidays. In [17]: plot_consumption_with_holidays_weekends ( new_data ) What should be obvious from exploring this plot is the existence of a daily pattern for energy consumption dependent on the day of the week: in most weekend days of the three years, we can observe slightly higher energy consumption than during weekdays. This can depend on the common habits of working at the office or children being at school. The effect of the holidays can also be seen. On Christmas day in 2007, the power consumption is slightly higher than other typical weekdays, but similar values can be seen e.g. exactly one week before. So the holidays don't have a strong impact here. In 2008 on the other hand, the power consumption is lower, indicating that perhaps the household celebrated outside their house. In 2009, there is a peak in consumption on Christmas eve. We can conclude that whether the day is a weekday or in the weekend or a holiday has an influence on the expected power consumption. Daily patterns Next to yearly and monthly patterns, we are also interested in daily patterns. Below, we plot the evolution of the global active power for one week (The first week of December 2009). We can see how the power consumption follows a daily recurring pattern, with peaks in the morning and evening and troughs at night and midday. In fact, this pattern is also already present in the previous plot, where daily morning and evening peaks can also be observed. During the night and the day, we still see low amounts of power usage, most likely due to appliances like refrigerators and heating. In [18]: plot_weekly_data ( new_data ) Statistical Data Exploration The visualizations above highlighted interesting patterns in the energy consumption of the household that can be exploited for feature extraction. In this section we use statistical methods to verify, and eventually quantify those observations. Autocorrelation In order to find repetitive patterns in time series data, we can use autocorrelation , which provides the correlation between the time series and a delayed copy of itself. In order to investigate this autocorrelation, the plot below visualizes the autocorrelation of the global active power with time lags of 1 day, 2 days, etc. Note that we do not show the results of a delay of 0 days, since the correlation with the exact same day would trivially be 1. In [19]: plot_auto_correlation ( new_data ) The plot above reveals that a clear peak occurs after 1 day, indicating the consumption of a day is highly correlated with the consumption of the previous day another clear peak occurs every 7 days, which confirms the existence of a weekly pattern, i.e. the consumption of a day is highly correlated with the consumption of the same day a week earlier Correlation As visually observed above, the temperature influences the energy consumption of a household. In order to verify the strength of this relationship, we can compute the correlation between these two variables. For this purpose, we compute Pearson's correlation. The plot below shows again both temperature and consumption and allows to investigate their relationship. This relationship is influenced by the frequency of the dataset, e.g. temperature changes are typically less drastic than energy consumption changes. We invite the user to experiment with the different resampling periods, to understand how that changes the correlation and what frequency leads to the highest correlation. In [20]: plot_correlation ( new_data ) What should be obvious from the above is that there is a negative correlation between temperature and energy consumption, meaning the consumption rises when temperature drops and vice versa. The larger the resampling window is chosen, the higher is the correlation. This is because with larger windows, we smooth out more the short-term fluctuations, leaving only the seasonal pattern described above. Feature engineering In this section, we will extract several features from the extended dataset that can be used for forecasting energy consumption. A feature is an individual measurable property of the phenomenon being observed. Choosing informative, discriminating and independent features is a crucial step for building effective machine learning algorithms. Based on the previous visual and statistical observation, we will consider the following features to predict the energy consumption for the hour ahead: energy consumption of the day before energy consumption of the 7 days before the hour of the day, which will allow to distinguish between different periods of the day (i.e. night, morning, afternoon, evening) the day of the week, which will allow to distinguish between weekdays and weekend days whether the day is a holiday or not the month of the year, which will allow to distinguish between yearly seasons the temperature In [21]: new_feats = new_data [[ 'Global_active_power' ]] . rename ( columns = ({ 'Global_active_power' : 'consumption' })) new_feats [ 'Pastday' ] = new_feats [ 'consumption' ] . shift ( 1 , freq = 'D' ) new_feats [ 'Pastweek' ] = new_feats [ 'consumption' ] . shift ( 7 , freq = 'D' ) new_feats [ 'Hour' ] = new_feats . index . hour new_feats [ 'Weekday' ] = new_feats . index . dayofweek new_feats [ 'Month' ] = new_feats . index . month new_feats [ 'Holiday' ] = [ 1 if ( date in holidays . France ()) else 0 for date in new_feats . index ] new_feats [ 'Temperature' ] = new_data [ 'temperature' ] new_feats = new_feats . dropna () Modeling This section allows the user to discover the most important factors for training a machine learning model. More specifically, the user will get insights on the influence of the training strategy , type of machine learning model and effect of data normalization on model performance. To evaluate the performance of the models the mean absolute error (MAE) is used (a metric commonly used in literature for this purpose). This metric quantifies to what extent the model forecasts are close to the real values. As the name suggests, the mean absolute error is an average of the absolute errors. It holds that the lower the MAE of the model, the better its performance. This section ends with an interactive application where the user can test all options in order to find the one that leads the model to achieve the best performance. Training strategies The training data influences models performances. For this experiment, the following options are available: Use 1 month before the test month as training data. In this experiment, several models are trained for the predictions of each individual month. Each model is trained on one month before the month we are making predictions for. One month gap is introduced between the training and the test month (this is done to avoid that the last day of the training set is also included in the first day of the test set). For example, the models that will make the predictions for April 2008 will be trained on the data from February 2008. Use 6 months before the test month as training data. Similar to the above experiment, but with 6 months of training data. It still includes a 1 month gap. Use 1 year before the test month as training data. Similar to the above experiment, but with 1 year of training data, still include a 1 month gap. Use 1 month the year before as training data. Similar to strategy number 1, but with an 11 months gap. This way, the training data and test data are taken from the same month, one year apart. Use all months before the test month as training data. For each test month, train a model using all the data prior to this (including a 1 month gap). This strategy simulates the scenario where the model is retrained as new data comes in. Since this requires training several models based on potentially large training sets, this might require a certain computational time. Use train-test split . Train on the first year of data and make predictions on the rest. This strategy is commonly used for training these kinds of models. Notice that for the benchmark model, there is no training phase. Machine learning models There is a plethora of models that can be used for forecasting. In this notebook, we decide to use two models that are commonly used in literature for the task at hand: Random Forest Regression (RFR) and Support Vector Regression (SVR) . Besides these models, we will also use a simple benchmark model that we use to compare the performance of the other models against. For the prediction of the energy of a certain day, this benchmark model simply takes the energy consumption of the previous day at the same time. We will use these models to forecast the energy consumption based on the past energy measurements (see training strategies). Normalization Before being used for training, data may need to be normalized, i.e. rescaled so that all input and output values are between 0 and 1. This is indeed a requirement for the correct training of some machine learning models. It is left to the user the exercise of discovering which model requires the normalization of the data. In [22]: from IPython.utils import io PORT = '8095' # in AWS use ports 8050-8060 mode = 'external' #external if mode == 'inline' : run_app ( new_feats , mode = 'inline' , port = PORT ) else : with io . capture_output () as captured : run_app ( new_feats , mode = 'external' , port = PORT , host = '0.0.0.0' ) HOST = '0.0.0.0' # in AWS replace here by public IP print ( f 'Dash app running on http:// { HOST } : { PORT } ' ) Dash app running on http://0.0.0.0:8095 The mean absolute error of each experiment that was run is reported in the table above. Insights from the experiments These are the basic insights the user could have noticed when trying different models, parameters and training strategies. Effect of the training strategy For the SVR, we see that the standard train-test split has the best performance, although using all months and using a one year window before the test month as training data both have a similar performance. For the RFR on the other hand, we clearly see that using a one year window before the test month as training data leads to better performance than the other training strategies. We note that for both this \"one year window\" strategy and the standard train-test split, the training data is one year. The difference however, is that for the former the training set changes, while for the latter it is fixed (using all the data from 2017). A general trend that can be seen is that the performance increases as a larger training set is used. We therefore urge the user to look at the predictions made using the training strategy where all months before the test month are used as training data: we can expect that the predictions for the later months will be better than for the first months since more training data was used. However, this is hard to see by eye in the prediction plot directly. Effect of the chosen model We can see that, for most parameter choices, both the SVR and RFR can outperform the simple benchmark model we set up. The RFR outperforms the SVR for most (if not all) parameter choices and training strategies. This shows the importance of knowing which model is best suited for your problem and testing different ones. Effect of the chosen parameters The search for the optimal set of hyperparameters for your machine learning model is a non-trivial task, since there might be a strong interplay between the parameters so we can not do the optimal parameter search one by one. Here, we let the user try a pre-set number of choices, but more automatic and smart search algorithms exist to tackle this problem. We found that for the RFR, the higher the number of estimators, the better the results, but at a greater computational cost and with diminishing returns. We see that 50 estimators and leaving the max depth at 10 is a good choice. For the SVR, $C=10$ and $\\gamma=0.01$ work well. Effect of normalization Normalizing the data, rescaling it so that the input and output variables all have values within a similar range (in this case, between 0 and 1) is a common step when setting up machine learning models. This is because, depending on the model, they work much better with values of the order 1. We see that normalization indeed greatly improves the predictions for the SVR, but has very little influence on the RFR. Indeed, algorithms such as random forest and decision tree are not influenced by the scale of the input variables. Conclusion In this Starter Kit we demonstrated forecasting the demand for a particular resource at a particular point in the future, illustrated in the case of energy demand forecasting. To this end, we first analysed the business problem, preprocessed the data, visually and numerically explored it to eventually illustrate the use of different models ( Random Forest Regression, Support Vector Regression and benchmark) to perform the actual forecasting with different amounts of available historical data. From the analysis, it may be clear that achieving good prediction results depend on different factors (appropriate data preprocessing, the amount of available training data, algorithm parametrisation, etc.). Additional Information Contributions by Mathias Verbeke, Wannes Meert, Vincent Vercruyssen, Alessandro Murgia, Tom Tourw√©, and Robbert Verbeke. This Starter Kit was developed in the context of the EluciDATA project ( http://www.elucidata.be ). For more information, please contact info@elucidata.be .","tags":"Time Series Preprocessing","url":"/Time Series Preprocessing/2020-01-01-interactive-notebook.html","loc":"/Time Series Preprocessing/2020-01-01-interactive-notebook.html"},{"title":"Interactive notebook","text":"In [2]: import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn import preprocessing import re import os from ipywidgets import register import cufflinks as cf cf . go_offline ( connected = True ) cf . set_config_file ( colorscale = 'plotly' , world_readable = True ) % matplotlib inline from IPython.core.display import HTML from IPython.display import Markdown , display def printmd ( string ): display ( Markdown ( string )) from starterkits.visualization import vis_plotly_widgets as vpw import visualizations as v import support as sp from support import LstmApp , get_test_scores # Fixing the used seeds for reproducibility of the experiments np . random . seed ( 1234 ) PYTHONHASHSEED = 0 display ( HTML ( \" \" )) /Users/hcab/opt/miniconda3/lib/python3.7/site-packages/dash_bootstrap_components/table.py:1: UserWarning: The dash_html_components package is deprecated. Please replace `import dash_html_components as html` with `from dash import html` Starter Kit 1.2.1: Remaining useful life estimation Business context Maintenance is an important part of an asset's lifecycle. In the past, corrective and preventive maintenance were the norm: corrective maintenance was performed when an asset had failed and involved tasks for identifying the fault and rectifying it so that the asset could resume normal operation preventive maintenance was performed to avoid asset failure and involved executing maintenance tasks at regular intervals, e.g. when an asset was used for a certain period of time or when it executed a pre-determined number of cycles. Nowadays, ever more assets are equipped with sensors and ever more data is gathered. This data can be exploited to realize predictive maintenance, which promises costs saving over corrective or preventive maintenance because maintenance is only performed when warranted. Predictive maintenance encompasses a variety of topics, such as failure prediction, remaining useful lifetime estimation (RUL), failure detection, failure diagnosis (root cause analysis) and recommendation of mitigation or maintenance actions after failure. Business goal The business goal addressed by this Starter Kit is the estimation of the remaining useful lifetime of an asset. 'Lifetime' in this context is expressed in terms of the evolution of a particular quantity, such as the distance travelled, fuel consumed, repetition cycles performed, number of transactions experienced, etc. Estimating the remaining useful lifetime is non-trivial, as it is influenced by a multitude of internal and external factors, e.g. operating conditions, weather conditions, usage scenarios, etc. Application contexts Estimating the remaining useful lifetime of an asset can be beneficial for several purposes in a variety of industrial contexts, for example: to better schedule maintenance operations, e.g. for offshore wind turbines, for which maintenance can only be performed under the right weather conditions. to optimise the operational efficiency of an asset, by more optimally planning the use of that asset before its end of life, or adapting its operational use to extend its useful lifetime. to avoid unplanned downtime, e.g. in manufacturing settings, where a downtime of the production line results in several operational problems and associated costs. to anticipate and avoid safety-critical situations, e.g. for aircrafts or vehicles. ... Data characteristics and requirements Typically, the data to be exploited for remaining useful lifetime estimation has the following characteristics: it consists of time-series data, in particular for the quantity used to express the lifetime of the asset but also for all the factors that influence that quantity (e.g. weather and operating conditions). temporal patterns such as trends, cycles, seasonality, day/night or week/weekend patterns can be present in the data. random fluctuation (noise) can also be present, as a result of random variation or unexplained causes. In a large number of cases the data is measured using sensors, therefore some noise is inherently present on the measured signal. Moreover, the following requirements are imposed on the dataset: a sufficiently large amount of historical data needs to be available. Two aspects are important in this respect: the implicit assumption is that an asset has a continuous degradation pattern, which is reflected in the asset's sensor measurements. Degradation is often slow, however, so the monitoring period should be sufficiently long so as to include a clear presence of the degradation pattern. a sufficient number of examples of assets that reached their end of life needs to be present. the data needs to include explicit information on when an asset has reached its end of life or should allow to easily derive it (e.g. the lifetime quantity has reached a particular threshold). Starter Kit outline The data-driven approach for remaining useful life estimation will be illustrated on the industrially-relevant use case of predicting the remaining useful lifetime of aircraft engines based on run-to-failure data . We will use this dataset to illustrate: How to appropriately preprocess the data and construct training, test and validation sets to learn and evaluate a model able to predict whether an engine will fail within a certain number of operational cycles. How to use deep learning to construct a model for remaining useful lifetime prediction. In particular, we apply a deep learning approach that uses short-term memory ( LSTM ) networks and allows the construction of a predictive model without the time-consuming feature engineering step, which requires one to manually extract the characteristics from which the algorithm can learn. How to experimentally validate the resulting model and objectively compare it with other approaches. The resulting model achieves a high accuracy. The approach is simple and flexible enough to be applied to a wide range of RUL problems. Data understanding The time series data we will use in this Starter Kit is simulated run-to-failure data from aircraft engines. Engines in the data are assumed to start with varying degrees of wear and manufacturing variation. This information is unknown to the user. Furthermore, in this simulated data, engines are assumed to be operating normally at the beginning, and start to degrade at some point during operation. The degradation progresses and grows in magnitude. When a predefined threshold is reached, the engine is considered unsafe for further operation. In other words, the last operational cycle of the engine can be considered as the failure point of the corresponding engine. In machine learning experiments, a dataset is often split into a training set and a test set, which allows one to quickly evaluate the performance of an algorithm on the problem at hand. The training dataset is used to prepare a model, to train it. We pretend the test dataset is new, unseen data where the output values (in this case, whether or not a specific engine is going to fail within a predefined number of cycles) are withheld from the algorithm. We gather predictions from the trained model on the inputs from the test dataset and compare them to the withheld output values of the test set, the so-called ground truth . Comparing the predictions and withheld outputs on the test dataset allows us to compute a performance measure for the model on the test dataset. This is an estimate of the skill of the algorithm trained on the problem when making predictions on unseen data. For this reason, the dataset is divided into three parts, namely the training, test and ground truth datasets: The training data will be used by the algorithm to learn the prediction model. It consists of multiple time series with \"cycle\" as the time unit, and 21 sensor readings for each cycle. Each sequence of cycles can be assumed as being generated from a different engine (identified by the engine ID) of the same type. Since the algorithm needs to be able to learn when the engine will fail, the last time period does represent the failure point . The test data has the same data schema as the training data and is used to test the quality of the resulting model. The only difference is that the data does not indicate when the failure occurs, or put differently, the last time period does not represent the failure point. The ground truth data provides the number of remaining working cycles for the engines in the test data, which will only be used to assess the quality of the results, but will not be given as input to the learned model. More details on this dataset can be found in [1]. In [3]: train_df , test_df , truth_df = sp . load_data () The table below shows an excerpt of the training data, with the following attributes: id: the identifier of the engine cycle: the current cycle of the engine from which the data in the respective row is originating, i.e. the current flight of the aircraft setting 1-3: the values of 3 different operational settings of the engine, e.g. an operational setting could correspond to a cruise setting when the aircraft is flying at cruise speed s1-21: 21 sensor readings monitoring various parameters of the engine, e.g. temperature or vibration Since the input file is space-separated, and two spurious spaces appear at the end of each line, we remove the last two columns. In [4]: train_df . head () Out[4]: id cycle setting1 setting2 setting3 s1 s2 s3 s4 s5 ... s12 s13 s14 s15 s16 s17 s18 s19 s20 s21 0 1 2 0.0019 -0.0003 100.0 518.67 642.15 1591.82 1403.14 14.62 ... 522.28 2388.07 8131.49 8.4318 0.03 392 2388 100.0 39.00 23.4236 1 1 3 -0.0043 0.0003 100.0 518.67 642.35 1587.99 1404.20 14.62 ... 522.42 2388.03 8133.23 8.4178 0.03 390 2388 100.0 38.95 23.3442 2 1 4 0.0007 0.0000 100.0 518.67 642.35 1582.79 1401.87 14.62 ... 522.86 2388.08 8133.83 8.3682 0.03 392 2388 100.0 38.88 23.3739 3 1 5 -0.0019 -0.0002 100.0 518.67 642.37 1582.85 1406.22 14.62 ... 522.19 2388.04 8133.80 8.4294 0.03 393 2388 100.0 38.90 23.4044 4 1 6 -0.0043 -0.0001 100.0 518.67 642.10 1584.47 1398.37 14.62 ... 521.68 2388.03 8132.85 8.4108 0.03 391 2388 100.0 38.98 23.3669 5 rows √ó 26 columns The test data looks similar to the training data, with the difference that it does not indicate when the failure occurs, or put differently, the last time period does not represent the failure point. Taking the sample test data shown in the table below as an example, the engine with id=1 runs from cycle 1 through cycle 31. It is not shown how many more cycles this engine can last before it fails. In [5]: test_df [ 25 : 31 ] Out[5]: id cycle setting1 setting2 setting3 s1 s2 s3 s4 s5 ... s12 s13 s14 s15 s16 s17 s18 s19 s20 s21 25 1 27 -0.0007 0.0001 100.0 518.67 642.08 1586.65 1400.31 14.62 ... 521.82 2388.02 8127.24 8.4494 0.03 392 2388 100.0 38.87 23.3931 26 1 28 0.0022 0.0005 100.0 518.67 641.93 1594.25 1401.29 14.62 ... 521.84 2388.07 8134.89 8.4470 0.03 392 2388 100.0 38.83 23.3502 27 1 29 0.0014 0.0001 100.0 518.67 641.95 1587.15 1398.11 14.62 ... 522.39 2388.07 8133.13 8.4212 0.03 392 2388 100.0 39.02 23.3621 28 1 30 -0.0025 0.0004 100.0 518.67 642.79 1585.72 1400.97 14.62 ... 521.78 2388.10 8134.79 8.4110 0.03 391 2388 100.0 39.09 23.4069 29 1 31 -0.0006 0.0004 100.0 518.67 642.58 1581.22 1398.91 14.62 ... 521.79 2388.06 8130.11 8.4024 0.03 393 2388 100.0 38.81 23.3552 30 2 1 -0.0009 0.0004 100.0 518.67 642.66 1589.30 1407.16 14.62 ... 521.62 2388.14 8129.59 8.4283 0.03 392 2388 100.0 39.00 23.3923 6 rows √ó 26 columns The ground truth data provides the number of remaining working cycles for the engines in the test dataset. Taking the sample ground truth data shown in the following table as an example, the engine with id 1 in the test data can run for another 112 cycles before it fails. In [6]: display ( truth_df . set_index ( truth_df . index + 1 ) . head ( 3 )) 112 1 98 2 69 3 82 Data preprocessing Before we can start training a model to predict the RUL, a number of preparatory steps needs to be taken. As could be seen from the training data sample above, currenly the data does not include a column indicating the RUL, which is the target value we want to predict. As the algorithm needs this information to learn from, the first step will be to create such a column. A second observation that can be made on the training data sample is that the scale of the values differs across columns. This difference could cause problems when the model needs to calculate the similarity between different cycles (i.e., the rows in the table) during modeling. To address this problem, we will normalise the data. The goal of normalisation is to change the values of the numeric columns in the dataset, in order to have a common scale and avoid distorting differences in the ranges of values or losing information. The normalised values maintain the general distribution and ratios in the source data, while keeping values within a scale applied across all numeric columns used in the model. RUL label creation The first preprocessing step is to generate a label indicating the Remaining Useful Lifetime. In this Starter Kit, we will not predict the exact number of remaining cycles the engine still has left before failure, yet rather try answering the question 'Is a specific engine going to fail within w1 cycles?', where w1 is a predefined threshold used during the label creation process. Training data In order to determine the remaining number of cycles at each time point, the maximum number of cycles per engine is first determined. Subsequently, the current cycle number is substracted from the max number of cycles to obtain the number of cycles remaining at a particular time point. In the following sample, the column RUL indicates the remaining number of cycles for that particular point in time (i.e. the row). In [7]: rul = pd . DataFrame ( train_df . groupby ( 'id' )[ 'cycle' ] . max ()) . reset_index () rul . columns = [ 'id' , 'max' ] train_df = train_df . merge ( rul , on = [ 'id' ], how = 'left' ) train_df [ 'RUL' ] = train_df [ 'max' ] - train_df [ 'cycle' ] train_df . drop ( 'max' , axis = 1 , inplace = True ) Xtrain = train_df . copy () train_df [[ 'id' , 'cycle' , 'RUL' ]] . head () Out[7]: id cycle RUL 0 1 2 190 1 1 3 189 2 1 4 188 3 1 5 187 4 1 6 186 In order to arrive to a binary label indicating whether an engine is going to fail within w1 cycles, the next step is to test whether the number of remaining cycles in the RUL column is smaller than or equal to the predefined threshold w1 , currently set to 30 cycles. This is illustrated in the sample below, where for the first row, the value in the column RUL_label is 1, because the engine will fail within 4 cycles. In [8]: w1 = 30 train_df [ 'RUL_label' ] = train_df . RUL . apply ( lambda x : x <= w1 ) . astype ( int ) train_df [[ 'id' , 'cycle' , 'RUL' , 'RUL_label' ]] . tail () Out[8]: id cycle RUL RUL_label 20625 100 196 4 1 20626 100 197 3 1 20627 100 198 2 1 20628 100 199 1 1 20629 100 200 0 1 Test data A similar approach is adopted for the test set. Note that, in order to generate the labels for the test data, we need to use the ground truth dataset, because the test data does not indicate when the failure occurs, or put differently, the last time period does not represent the failure point. In [9]: # Generate column max for test data rul = pd . DataFrame ( test_df . groupby ( 'id' )[ 'cycle' ] . max ()) . reset_index () rul . columns = [ 'id' , 'max' ] truth_df . columns = [ 'more' ] truth_df [ 'id' ] = truth_df . index + 1 truth_df [ 'max' ] = rul [ 'max' ] + truth_df [ 'more' ] truth_df . drop ( 'more' , axis = 1 , inplace = True ) # Generate RUL for test data test_df = test_df . merge ( truth_df , on = [ 'id' ], how = 'left' ) test_df [ 'RUL' ] = test_df [ 'max' ] - test_df [ 'cycle' ] test_df . drop ( 'max' , axis = 1 , inplace = True ) # Generate label columns RUL and RUL_label for test data test_df [ 'RUL_label' ] = np . where ( test_df [ 'RUL' ] <= w1 , 1 , 0 ) test_df [[ 'id' , 'cycle' , 'RUL' , 'RUL_label' ]]; Normalisation using Min-Max scaling As explained before, the goal of normalisation is to change the values of numeric columns in the dataset to a common scale. In this notebook, to put it simply, normalisation is performed to avoid variables on a higher scale affecting the outcome in bigger measure only because they are on a higher scale. One of the most popular normalisation technique is Min-Max normalisation , which we also use here. It scales the values of a variable to a range between 0 and 1 using the following formula, where $X$ represents the value to be normalised, $X_{min}$ is the minimum value of the variable in that column and $X_{max}$ is the maximum value for that variable: $X_{norm} = \\frac{X - X_{min}}{X_{max} - X_{min}}$ Note: an alternative approach to Min-Max scaling is Z-score normalisation (or standardisation ) in which the variables will be rescaled so that they have the properties of a standard normal distribution with zero mean and unit variance. For both the training set and the test set, we normalise all columns except id , cycle , and RUL , as we want to keep the original values for these columns. The following sample shows a portion of the normalised training data. In [10]: train_df [ 'cycle_norm' ] = train_df [ 'cycle' ] cols_normalize = train_df . columns . difference ([ 'id' , 'cycle' , 'RUL' ]) min_max_scaler = preprocessing . MinMaxScaler () norm_train_df = pd . DataFrame ( min_max_scaler . fit_transform ( train_df [ cols_normalize ]), columns = cols_normalize , index = train_df . index ) join_df = train_df [ train_df . columns . difference ( cols_normalize )] . join ( norm_train_df ) train_df = join_df . reindex ( columns = train_df . columns ) train_df . describe () . loc [[ 'mean' , 'min' , 'max' ]] Out[10]: id cycle setting1 setting2 setting3 s1 s2 s3 s4 s5 ... s15 s16 s17 s18 s19 s20 s21 RUL RUL_label cycle_norm mean 51.509016 108.813088 0.499492 0.501975 0.0 0.0 0.443065 0.424747 0.450442 0.0 ... 0.451122 0.0 0.434226 0.0 0.0 0.524232 0.546119 107.803829 0.150267 0.298651 min 1.000000 1.000000 0.000000 0.000000 0.0 0.0 0.000000 0.000000 0.000000 0.0 ... 0.000000 0.0 0.000000 0.0 0.0 0.000000 0.000000 0.000000 0.000000 0.000000 max 100.000000 362.000000 1.000000 1.000000 0.0 0.0 1.000000 1.000000 1.000000 0.0 ... 1.000000 0.0 1.000000 0.0 0.0 1.000000 1.000000 361.000000 1.000000 1.000000 3 rows √ó 29 columns As can be seen, both the settings variables and the sensor variables now only contain values between 0 and 1. In [11]: test_df [ 'cycle_norm' ] = test_df [ 'cycle' ] norm_test_df = pd . DataFrame ( min_max_scaler . transform ( test_df [ cols_normalize ]), columns = cols_normalize , index = test_df . index ) test_join_df = test_df [ test_df . columns . difference ( cols_normalize )] . join ( norm_test_df ) test_df = test_join_df . reindex ( columns = test_df . columns ) test_df = test_df . reset_index ( drop = True ); Data Modeling and Analysis Traditional machine learning models are usually based on manually extracting distinguishing factors that characterise the phenomenon to be learned using domain expertise (the so-called feature engineering step). This usually makes these models hard to reuse since feature engineering is specific to the problem scenario and the available data, and therefore a labour-intensive process. In contrast, deep learning algorithms can automatically extract the right features from the data, eliminating the need for manual feature engineering. Deep learning refers to machine learning approaches involving artificial neural networks (ANNs) that contain a multitude of so-called hidden layers. One particular type of ANNs are the Long Short-Term Memory networks, or LSTMs. One critical advantage of LSTMs is their ability to remember information from long-term sequences, which is hard to achieve with traditional feature engineering. When manually engineering features for this problem, one would for example compute the rolling average per windows of 50 cycles. This averaging may however lead to a loss of information due to the smoothing and abstracting of values over such a long period. Instead, using all 50 values as input may provide better results. While performing feature engineering over large window sizes may complicate things for traditional machine learning methods, LSTMs are naturally able to handle larger window sizes and use all the information in the window as input. Algorithm-specific data preparation As a first step in the modeling phase, we will prepare the data to serve as input for the LSTM network. When using LSTMs in the time-series domain, one important parameter to pick is the sequence length, which is the size of the window within which the LSTMs needs to remember information. This is similar to picking a window size when calculating time series features using a rolling window. The idea of using LSTMs is to let the model extract abstract features from the sequence of sensor values in the window rather than engineering those manually. The expectation is that if there is a pattern in these sensor values within the window prior to failure, the pattern should be extracted and learned by the LSTM. The window size chosen for the training data set strongly influences the prediction results. In the animation below, you can see how differently the single variables for a given engine behave. Furthermore, the time at which the degradation becomes visible is different for different engines. When using the initial settings in the animation below, i.e. Variable equals s11 , ID equals 1 and period equals 50, we see a comparably short line fluctuating around a constant value. When selecting the IDs 3 and 13 instead and increasing the period to 130 at the same time, one sees a clear deviation from the median value indicated by the horizontal gray dashed line. The deviation for both starts at around index 110. When then additionally selecting the engine with ID 36, the deviation starts already at approximately index 80 and therewith significantly earlier. Further looking at Variable s12 , the deviation from the median starts at the same times as for s11 for the engines under inspection. When selecting the training period for the model, we need to take the different start of the deviation into account. Here we need to find a good balance between the two different influences: on the one hand, the longer the training data sequence, the better the results; on the other hand, when selecting a too long training period, it is possible that the model learns that a slight deviation is fine in several engines and will therewith fail in predicting the remaining useful lifetime. The period length will be used in the remainder of the notebook to calculate the model sequences. In [12]: date_range_slider = v . interactive_plot ( test_df ) Based on the figures above, it is very hard to visually spot any trends that are influencial for the remaining useful life, let alone to manually extract features from it. As indicated above, the advantage of deep learning algorithms is that they can automatically extract the right features from the data, eliminating the need for manual feature engineering. In the following, we pick the feature columns ( sensor_cols ) to use for modelling as well as the columns identifying the single sequences ( sequence_cols ). In [13]: sensor_cols = [ x for x in test_df if re . findall ( string = x , pattern = 's[0-9]' )] sequence_cols = [ 'setting1' , 'setting2' , 'setting3' , 'cycle_norm' ] sequence_cols . extend ( sensor_cols ) Keras is a deep learning library written in Python, which naturally interfaces with TensorFlow , an open-source software library for machine learning. Its LSTM layer expects an input in the shape of a 3-dimensional matrix of (samples, time steps, features) (more specifically a 3-dimensional NumPy array) where 'samples' is the number of training sequences, 'time steps' is the look back window or sequence length and 'features' is the number of features of each sequence at each time step. During this transformation, we also select the columns from the input data that can serve as features. In this case, we selected all sensor values, as well as the 3 settings parameters and the (normalised) current cycle the machine is in at this point. In [14]: def gen_sequence ( id_df , seq_length , seq_cols ): \"\"\" Function to reshape the data into a 3-dimensional (samples, time steps, features) matrix \"\"\" data_array = id_df [ seq_cols ] . values num_elements = data_array . shape [ 0 ] for start , stop in zip ( range ( 0 , num_elements - seq_length ), range ( seq_length , num_elements )): yield data_array [ start : stop , :] For each id in the data set, a data sequence will be created. As initial sequence length, we will use 50 time steps. In [15]: register ( date_range_slider ) sequence_length = 50 seq_gen = ( list ( gen_sequence ( train_df [ train_df [ 'id' ] == id ], sequence_length , sequence_cols )) for id in train_df [ 'id' ] . unique ()) seq_array = np . concatenate ( list ( seq_gen )) . astype ( np . float32 ) printmd ( f '''This results in a 3-dimensional input matrix with { seq_array . shape [ 0 ] } samples, { seq_array . shape [ 1 ] } time steps and { seq_array . shape [ 2 ] } features (including 21 sensor values, the 3 settings parameters and the normalised current cycle).''' ) This results in a 3-dimensional input matrix with 15630 samples, 50 time steps and 25 features (including 21 sensor values, the 3 settings parameters and the normalised current cycle). For the training data, the labels are also added to this data structure, so that the LSTM can learn from it. In [16]: def gen_labels ( id_df , seq_length , label ): data_array = id_df [ label ] . values num_elements = data_array . shape [ 0 ] return data_array [ seq_length : num_elements , :] def handler ( x ): pass sequence_length = date_range_slider . get_interact_value () label_gen = [ gen_labels ( train_df [ train_df [ 'id' ] == id_ ], sequence_length , [ 'RUL_label' ]) for id_ in train_df [ 'id' ] . unique ()] label_array = np . concatenate ( label_gen ) . astype ( np . float32 ) v . plot_label_frequency ( label_array ) The majority of data points, roughly 80 %, are labeled as zero, indicating that no failure was observed. LSTM network Note: we refer the interested reader to Appendix A for a brief introduction to LSTM networks. In this section, we guide you through the general process of building, training, evaluating and testing a LSTM model. This will be done by means of an interactive interface. In order to see the code used for these steps, please expand the fields below. Exercise 1 Network construction Let us start by building an LSTM network. In our setup, we will experiment with a stacked structure. In order to learn from different architectures of the LSTM model, it is possible to change the model variables in the animation below. We ask the reader to test different model scenarios by applying the following steps: Select the number of intermediate layers. For the first experiment, please select a single intermediate layer only. Set the size of the intermediate layer to 30. This corresponds to a rather small network. Set the number of epochs for training to 15. Epochs define the number of times the model iterates over the training data arrays, the aim being for the model to improve during each training epoch. In general, the more epochs the better the results, until we reach the given model's limit. Similarly as before, the training sequence length defines the number of data points to take into account for training per engine. Please select a sequence length of 50. Furthermore, we add a dropout layer after each LSTM layer. The dropout consists in randomly setting a fraction rate of input units to 0 at each update during training time, which helps preventing overfitting. Overfitting means that a model fits too closely a limited set of data points. This typically results in an overly complex model that fits all little details in the data under study. Please choose a value of 0.2 for this first experiment. Model training Now that we have fixed the network design, we can fit the network to the training data by pushing the button Train model . After a successful training, a green badge will appear. The fitting step is influenced by the following parameters, which we will keep fixed during our experiments: Batch size: The number of samples that are used per gradient update (while fixing the best parameters for the model). We will use a batch size of 200. Validation split: fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. We will use a validation split of 5%. Furthermore, the fitting function will stop early if the validation loss (which is the quality criterion that is used to evaluate the model performance) is not decreasing anymore. To parametrise the early stopping function, the following parameters can be handled by TensorFlow: Minimal delta: minimum change in the monitored quantity as given by the validation loss to qualify as an improvement, i.e. an absolute change of less than min_delta will count as no improvement. Patience: number of epochs with no improvement after which training will be stopped. In the experiments defined in this Starter Kit, we keep both minimal delta and patience at zero. Model evaluation and testing Now that we have a fitted model, we can evaluate its performance. We will first evaluate the model performance on the training data and subsequently on the test data. If both evaluations result in approximately the same score, this gives an indication that the model is generalisable to unseen datasets and thus is not overfitting on the training data. In the interface, please switch to the tab Evaluation at the top and press the button Evaluate model . This will lead to the evaluation of the last trained model. The model can be evaluated using different measures. We will on the one hand evaluate the model in function of accuracy , which measures the fraction of all instances that are correctly categorized; it is the ratio of the number of correct classifications to the total number of instances to classify. On the other hand, we will show the so-called confusion matrix . The confusion matrix for the model that we just trained shows that the model is able to correctly classify that a specific engine is not going to fail within w1 cycles for almost all of the 12531 samples. Vice versa, the model is able to correctly classify for almost all of the 3100 cases that a specific engine is going to fail within w1 cycles. \\ Based on the confusion matrix, several other measures such as precision and recall can be calculated. Intuitively, precision is the ability of the classifier not to label a sample that is negative as positive. When instantiated for the RUL prediction problem we are currently considering, precision indicates how many of the engines that the model predicted to reach their end of life within 30 cycles also actually reach their end of life within this amount of cycles. Recall is the ability of the classifier to find all the positive samples, i.e., all engines that reach their end of life within 30 cycles. The respective results are given in the model evaluation summary table. A unique id will be assigned to each evaluated model in order to compare the results for different models. Note that the evaluation is performed on the training data. In order to evaluate it on the test data (that is unknown to the model), please continue to the tab Testing . By pushing the button Evaluate model , the model selected in the dropdown menu is evaluated on the test data. Questions: Is there a significant difference in accuracy when evaluating the model on the validation and test data? Do you think the model is overfitting? Exercise 2: The LSTM network built in Exercise 1 performs well on the evaluation data set with an accuracy of about 95% but with a lower accuracy on the test data. This gives us a hint that the model might be overfitting on the training data. Therefore, in this second experiment, we add a second LSTM layer to the network. Please increase the number of layers to 2 and set the following layer sizes: Intermediate Layer 1: 200 Intermediate Layer 2: 50 Keep the remaining parameters as they are. Subsequently repeat the steps taken in the previous exercise: Train the model Evaluate it on the validation data Evaluate it on the test data Questions: Which difference do you observe? Compare the results in the tables in both tabs - evaluation and testing When comparing the two models, which one would you use and why? Exercise 3: Use the interface to experiment with alternative architectures and see how the parameters influence the quality of the model. You can try to answer questions such as: Will the model improve when the number of epochs is increased? Is it possible to reach an accuracy of 98 % on both data sets (validation and test)? Does adding a third LSTM layer improve the results even further? LSTM interface Note: if the current port is already in use, change it to any port between 8050 and 8060 In [17]: PORT = 8050 v . start_dashboard ( Xtrain , sequence_cols , test_df , PORT ) Dash app running on http://127.0.0.1:8050 Code In what follows, you can find the most prominent functions that are called in the interactive interface to construct and train the LSTM model, and subsequently evaluate it. Model building and compilation def compile_model ( num_layers = 2 , sizes ): model = Sequential ( name = 'Sequential_Layer' ) for i in range ( num_layers ): if sizes [ i ]: if i == 0 : model . add ( LSTM ( input_shape = ( seq_len , nb_features ), units = sizes [ i ], return_sequences = bool ( sizes [ i + 1 ] is not None ))) else : model . add ( LSTM ( units = sizes [ i ], return_sequences = bool ( sizes [ i + 1 ] is not None ))) model . add ( Dropout ( dropout )) model . add ( Dense ( units = nb_out , activation = 'sigmoid' )) model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ]) return ( model ) Model training model . fit ( seq_array , label_array , epochs = epochs , batch_size = 200 , validation_split = 0.05 , verbose = 0 , callbacks = [ keras . callbacks . EarlyStopping ( monitor = 'val_loss' , min_delta = 0 , patience = 0 , verbose = 0 , mode = 'auto' )]) Make predictions and compute confusion matrix y_pred = model . predict_classes ( seq_array , verbose = 1 , batch_size = 200 ) y_true = label_array cm = confusion_matrix ( y_true , y_pred ) Comparison of performance against other methods In order to get an idea of the quality of the LSTM model, we will compare it with the best performing model from [1]. In this experiment, four binary classification models are compared, namely a Two-Class Logistic Regression model, a Two-Class Boosted Decision Tree, a Two-Class Decision Forest, and a Two-Class Neural Network. Note that for these models, a manual feature engineering step was needed, whereas in the case of the LSTM model, the algorithm automatically decides on the most appropriate features. The table below compares the best performing model, namely a Two-Class Neural Network, with the LSTM approach, in terms of accuracy, precision, recall, and F1-score. In [18]: results_df = get_test_scores () results_df Out[18]: model id Accuracy Precision Recall F1-score model_type Two-Class Neural Network (best performing model from [1]) - 94.0 0.952381 0.8 0.869565 Conclusion In this Starter Kit, we covered the basics of using deep learning for remaining useful life prediction, which is one of the central topics in predictive maintenance. We illustrated the approach on the industrially-relevant use case of predicting the remaining useful lifetime of aircraft engines based on run-to-failure data. In doing so, we have shown: How to appropriately preprocess the data and construct training, validation and test sets to learn and evaluate a model that predicts whether an engine will fail within a certain number of operational cycles. How to use deep learning to construct a model for remaining useful life prediction. In particular, we applied a deep learning approach called long short-term memory (LSTM) networks which allows the construction of a predictive model without the time-consuming feature engineering step, which requires one to manually extract the characteristics from which the algorithm can learn. How to experimentally validate the resulting model and objectively compare it with other approaches. Comparing the above results on the test set, we see that the LSTM results are comparable to the results of the best performing model in [1] , with the advantage of not requiring a manual feature engineering step. Furthermore, it should be noted that the data set used here is very small and deep learning models are known to perform better with large datasets, so for a more fair comparison larger datasets should be used. The approach is simple and flexible enough to be applied to a wide range of RUL problems. Possible next steps One of the attention points when working with deep learning techniques is that it is important to tune the models to the right parameters, such as the window size. While the presented approach already gives promising results, quite some improvements are still possible, such as: Trying different window sizes. Trying different architectures with a different number of layers and nodes. Trying to tune the hyperparameters of the network. Trying to cast the RUL prediction problem as a regression or multi-class classification task instead of as a binary classification task. Trying on a larger dataset with more instances. Appendix A: A brief introduction to LSTM networks In the following, we will briefly explain the intuition behind an LSTM network, yet for a more detailed explanation we refer the interested reader to the original publication [2] or more intuitive descriptions (e.g. [3] , [4] ). A single layer LSTM consists of several blocks . Each block takes three inputs: 1) the input of the current time step, 2) the output from the previous LSTM block (of the previous time step) and 3) the \"memory\" of the previous block. Each block also provides output for the current timestep, and produces an updated memory of the current block. Therefore, a single block makes a decision by considering the current input, previous output and previous memory, generates a new output and alters its memory. This information flow is graphically represented in the following figure (taken from [4] ): The way the internal memory changes is controlled by different gates that control the information flow. Each gate computes a value between 0 and 1 using a logistic function to compute a value. The data is multiplied by this value to partially allow or deny information to flow into or out of the memory. An input gate controls the extent to which a new value flows into the memory. A forget gate controls the extent to which a value remains in memory. An output gate controls the extent to which the value in memory is used to compute the output activation of the block. An LSTM thus keeps two pieces of information as it propagates through time: A hidden state ; which is the memory the LSTM accumulates using its (forget, input, and output) gates through time, The previous time-step output . An important parameter when constructing LSTM networks is the size of the LSTM's hidden state. In the underlying computation framework TensorFlow that is used in this notebook, this parameter is set by the variable num_units . To make the name num_units more intuitive, you can think of it as the number of hidden units in the LSTM block, or the number of memory units in the block. The explaination above describes the structure of a single LSTM layer. However, an LSTM network usually consists of multiple layers , that can be stacked or merged. In case the layers are stacked, the output of the previous layer serves as input for the next layer. References [Predictive Maintenance Template](https://gallery.cortanaintelligence.com/Collection/Predictive-Maintenance-Template-3) Hochreiter, Sepp, and Jurgen Schmidhuber. \"Long short-term memory.\" Neural computation 9.8 (1997): 1735-1780. Olah, Christopher. [\"Understanding LSTM Networks\"](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) Yan, Shi. [\"Understanding LSTM and its diagrams\"](https://medium.com/@shiyan/understanding-lstm-and-its-diagrams-37e2f46f1714) MathWorks, [\"Models for Predicting Remaining Useful Life\"](https://nl.mathworks.com/help/predmaint/ug/models-for-predicting-remaining-useful-life.html) Additional information This notebook is based on the 'Deep Learning for Predictive Maintenance' notebook by Fidan Boylu Uz. MIT License Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.","tags":"Remaining Useful Life Prediction","url":"/Remaining Useful Life Prediction/2020-01-01-interactive-notebook.html","loc":"/Remaining Useful Life Prediction/2020-01-01-interactive-notebook.html"},{"title":"Interactive notebook","text":"In [1]: % load_ext autoreload % autoreload 2 # from IPython.core.display import display, HTML # deprecated from IPython.display import display , HTML display ( HTML ( \" \" )) In [2]: import numpy as np import pandas as pd import matplotlib import warnings warnings . filterwarnings ( 'ignore' ) pd . set_option ( 'display.float_format' , ' {:.3f} ' . format ) from support import read_consumption_data , read_climate_data , printmd from visualisations import run_app , plot_effects_resampling , plot_temperature_power_one_year , plot_yearly_data , \\ plot_weekly_data , plot_correlation , plot_consumption_with_holidays_weekends , plot_auto_correlation import holidays In [3]: data = read_consumption_data () ext_data = read_climate_data () 2022-06-14 16:26:14,045 [INFO ] Generating /Users/hcab/Documents/projects/elucidatalab.github/elucidatalab.starterkits/data/SK_1_3/household_power_consumption.txt Downloading file /Users/hcab/Documents/projects/elucidatalab.github/elucidatalab.starterkits/data/SK_1_3/household_power_consumption.txt.zip: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19.7M/19.7M [00:01<00:00, 17.9MiB/s] 2022-06-14 16:26:17,601 [INFO ] Using file: /Users/hcab/Documents/projects/elucidatalab.github/elucidatalab.starterkits/data/SK_1_3/household_power_consumption.txt 2022-06-14 16:26:37,612 [INFO ] Generating /Users/hcab/Documents/projects/elucidatalab.github/elucidatalab.starterkits/data/SK_1_3/hourly_temperature.csv Downloading file /Users/hcab/Documents/projects/elucidatalab.github/elucidatalab.starterkits/data/SK_1_3/hourly_temperature.csv.zip: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 144k/144k [00:00<00:00, 4.20MiB/s] 2022-06-14 16:26:38,469 [INFO ] Using file: /Users/hcab/Documents/projects/elucidatalab.github/elucidatalab.starterkits/data/SK_1_3/hourly_temperature.csv Starter Kit 1.3: Resource Demand Forecasting Business context Resource demand forecasting concerns accurately predicting the future need for a resource, typically using historical information. It is one of the most essential steps in resource demand management, i.e. ensuring sufficient resources are available to satisfy a fluctuating demand. Resource demand forecasting allows one to plan ahead and guarantee that sufficient resources are available when needed and to avoid costly countermeasures in case of shortage. A typical example of resource demand forecasting is provided by the energy sector. For energy suppliers it is more beneficial to buy electricity on the day-ahead market than on the spot market. Consequently, the more accurate the energy consumption can be predicted, the lower the cost. Given that more and more houses and appliances are enabled with a smart meter, an increasing amount of data becomes available that can enable and improve this prediction. Furthermore, these kinds of predictions also allow energy suppliers to better balance demand and supply, and helps them to ensure proper grid operation (and e.g., prepare for possible peak loads in winter time). This Starter Kit will focus energy consumption forecasting as a use case for illustrating how resource demand forecasting using historical data can be realized. Business goal The business goal related to this Starter Kit is forecasting the demand for a particular resource at a particular point in the future based on usage data of that resource that is available from the past. More specifically, we will illustrate a data-driven approach for forecasting the energy consumption of households. Application contexts Forecasting the demand of a particular resource can be useful for several purposes in a variety of industrial contexts: Forecast parking demand or traffic density in a particular neighbourhood in order to control traffic in intelligent ways (e.g. divert to alternative routes or parking spots) Predict battery consumption based on operating conditions in order to intelligently suggest recharging moments or switch to low-power mode Estimate the usage of consumables (e.g., ink, paper) based on product usage data in order to deliver new consumables exactly on time before stock breakage or avoid expensive storage of superfluous stock (e.g. in a manufacturing environment) ... Data requirements In order to realize the business goal, we need a dataset (time series) that includes: a value indicating the demand for the resource at each moment in time. the factors that influence the demand of the resource. These can be internal or external, for example: the energy consumption of a building is influenced by the outside temperature, the amount of people present in the building, etc. the consumption of raw materials is influenced by the type of product that is being produced, the production line settings, etc. the traffic density is influenced by the time of day, the presence of road works, the weather, etc. 'representative' historical data. The amount of data that is needed to make an accurate prediction is typically determined by the length of the temporal patterns in the data. For example, if the data expresses a yearly seasonality, historical data of multiple years is needed. data sampled with the 'appropriate' frequency (i.e., how many times per minute/hour/day you gather data points). This frequency is related to the required forecast frequency, e.g. it is impossible to forecast every 10 minutes when only 15-minute data is available. In some use cases, where resource demand forecasting concerns particular processes, (expert) knowledge on these processes is required (i.e. as labeled data) Starter Kit outline In this Starter Kit, we will illustrate a data-driven approach for forecasting the energy consumption of a household. As a dataset we will use the energy consumption of the household and climate data collected by a nearby weather station. We will use this dataset to illustrate: How to prepare your data for the analysis. We will describe how to handle the typical problems of raw data and how to perform data fusion when multiple sources of data are available. How to get insights from your dataset. We will present visual and numerical techniques in order to explore your data for identifying interesting patterns. How to transform the obtained insights into useful features for building forecasting models. How to compare correctly the performance of multiple forecasting models. Data loading and preprocessing For the household energy consumption use case we consider here, two types of data are relevant: Household data, which refers to the total household energy consumption along with the consumption of the individual energy-consuming appliances Climate data, which contains information of climate factors (e.g. temperature) that can influence on the energy consumption of households In the following subsections, we will load these datasets and perform some basic preprocessing, e.g. correctly handling missing values and fusing both datasets. Household data As a dataset we use the public energy consumption data set from the UCI repository . This dataset contains measurements of electric power consumption in one household, located near Paris, with a one-minute sampling rate over a period of almost 4 years. Different electrical quantities and sub-metering values are available, as can be seen in the table below: Date_Time : the date and time of day of the measurement Global_active_power : household global minute-averaged active power (in kilowatt) Global_reactive_power : household global minute-averaged reactive power (in kilowatt) Voltage : minute-averaged voltage (in volt) Global_intensity : household global minute-averaged current intensity (in ampere) Sub_metering_1 : energy sub-metering No. 1 (in watt-hour of active energy). It corresponds to the kitchen, containing mainly a dishwasher, an oven and a microwave (hot plates are not electric but gas powered). Sub_metering_2 : energy sub-metering No. 2 (in watt-hour of active energy). It corresponds to the laundry room, containing a washing-machine, a tumble-drier, a refrigerator and a light. Sub_metering_3 : energy sub-metering No. 3 (in watt-hour of active energy). It corresponds to an electric water-heater and an air-conditioner. In [4]: start_date = data . index . min () . date () end_date = data . index . max () . date () printmd ( f \"The dataset contains data from { start_date } to { end_date } .\" ) data . head () The dataset contains data from 2007-01-01 to 2010-11-26. Out[4]: Global_active_power Global_reactive_power Voltage Global_intensity Sub_metering_1 Sub_metering_2 Sub_metering_3 Date_Time 2007-01-01 00:00:00 2.580 0.136 241.970 10.600 0.000 0.000 0.000 2007-01-01 00:01:00 2.552 0.100 241.750 10.400 0.000 0.000 0.000 2007-01-01 00:02:00 2.550 0.100 241.640 10.400 0.000 0.000 0.000 2007-01-01 00:03:00 2.550 0.100 241.710 10.400 0.000 0.000 0.000 2007-01-01 00:04:00 2.554 0.100 241.980 10.400 0.000 0.000 0.000 From the table below, we can see that for all sensors we have 2.049.280 instances. This already indicates that there are no missing values. Besides this information, we do not spot immediately any strange insights, e.g. an anomaly in sensor data like negative consumed power. In [5]: data . describe () Out[5]: Global_active_power Global_reactive_power Voltage Global_intensity Sub_metering_1 Sub_metering_2 Sub_metering_3 count 2027288.000 2027288.000 2027288.000 2027288.000 2027288.000 2027288.000 2027288.000 mean 1.083 0.124 240.833 4.591 1.121 1.289 6.448 std 1.049 0.113 3.231 4.411 6.147 5.786 8.434 min 0.076 0.000 223.200 0.200 0.000 0.000 0.000 25% 0.308 0.048 238.990 1.400 0.000 0.000 0.000 50% 0.594 0.100 241.000 2.600 0.000 0.000 1.000 75% 1.520 0.194 242.870 6.400 0.000 1.000 17.000 max 11.122 1.390 254.150 48.400 88.000 80.000 31.000 For the purpose of this Starter Kit, only the Date_Time and the Global_active_power attributes will be considered from now onwards. In [6]: data = data [[ 'Global_active_power' ]] Climate data As an additional dataset, we use the climate information recorded close to the city of Paris, where the household is located, provided by the Wunderground website. It contains information on climate conditions for every 30 minutes, via the following attributes shown in the table below: time_UTC : the date and hour of the measurement temperature : temperature (degrees Celsius) dew_point : dew point (degrees Celsius) humidity : air humidity (percentage) sea_lvl_pressure : pressure at sea level (hPa) visibility : visibility (km) wind_dir_degrees : wind direction expressed in degrees (degrees) In [7]: ext_data . head ( 5 ) Out[7]: datetime 2007-01-01 00:00:00 14.210 2007-01-01 01:00:00 14.110 2007-01-01 02:00:00 13.910 2007-01-01 03:00:00 13.010 2007-01-01 04:00:00 11.910 Name: temperature, dtype: float64 From the statistics in the table below, we can see the data contains minimum values of -9999 for some variables. According to the documentation , values of -9999 or -999 signify missing values. We will replace these with the proper missing value NaN, as otherwise our algorithms would get confused. In [8]: ext_data . describe () Out[8]: count 34164.000 mean 12.407 std 13.868 min -8.710 25% 7.210 50% 12.310 75% 17.310 max 999.990 Name: temperature, dtype: float64 In [9]: ext_data . replace ( to_replace =- 9999 , value = np . nan , inplace = True ) ext_data . describe () Out[9]: count 34164.000 mean 12.407 std 13.868 min -8.710 25% 7.210 50% 12.310 75% 17.310 max 999.990 Name: temperature, dtype: float64 We can now notice that the temperature values seem to be more plausible. As expected, we also see that not all sensors have the same number of instances, meaning there are missing values. For the purpose of this Starter Kit, only the time_UTC and temperature attributes will be considered from now on. Data fusion The evolution of household energy consumption may be influenced by climatological factors, e.g. lower or higher temperatures can lead to a higher energy consumption for some appliances such as water-heaters and air-conditioners. To understand and analyze this relationship, we need to fuse the household data with the climate data. However, these datasets have different sampling time: the household data is sampled every minute, whereas the climate data is sampled every 30 minutes. The integration of these 2 datasets thus requires careful consideration. Two options are possible: we can either upsample the climate data or downsample the household data. We select the second option, as otherwise, we would need to find a way to impute missing data in the climate dataset, and it allows us to reduce the size of the dataset, which will reduce compute time required for the analysis. The effect of downsampling the household data can be seen below. The user can experiment how the window sizes of 30 minutes, 1 hour, or 4 hours affect the evolution of the global active power. In [10]: plot_effects_resampling ( data ) As we can see from the plot, resampling reduces the (small) peaks of the original signal. The effect becomes more pronounced for larger window sizes. A window size of 4 hours arguably removes too much variation. A window of 30 minutes or 1 hours on the other hand still captures the general evolution of the active power, which is exactly what is needed for characterizing the typical household consumption. Between 30 or 60min time windows, we decide to use a window size of 60min since this reduces the dataset and speeds up the computation time required later on for the analysis. To downsample the climate dataset, the mean of the values of the respective hour is used. To downsample the household dataset, the sum of each of the values for the corresponding hour is taken An excerpt of the downsampling of household dataset is shown in the table below. In [11]: ext_data_h = ext_data . resample ( '1H' ) . apply ( np . mean ) data_h = data . resample ( '1H' ) . agg ({ 'Global_active_power' : np . sum , }) data_h [: 3 ] Out[11]: Global_active_power Date_Time 2007-01-01 00:00:00 153.038 2007-01-01 01:00:00 151.404 2007-01-01 02:00:00 154.940 Since Global_reactive_power and Global_active_power are expressed in minute-averaged kW, we will change their units to Wh over an hour, such that this is in line with the resampling frequency. To this end, the following transformations are applied (the summing was already done in the previous step): \\begin{align*} \\mathit{globalactive}\\ Wh &= \\sum&#94;{60}_{min=1} 1000\\cdot\\mathit{globalactivepower}_{\\mathit{min}}\\ kW\\cdot \\frac{1}{60}\\ h\\\\ \\end{align*} The result of this transformation can be seen in the table below. In [12]: data_h [ 'Global_active_power' ] = data_h [ 'Global_active_power' ] . apply ( lambda x : x * 1000 / 60 ) data_h [: 3 ] Out[12]: Global_active_power Date_Time 2007-01-01 00:00:00 2550.633 2007-01-01 01:00:00 2523.400 2007-01-01 02:00:00 2582.333 Now we can finally merge the two datasets into an extended dataset that will be explored in the next section. In [13]: new_data = pd . concat (( data_h , ext_data_h ), axis = 1 , join = 'inner' ) new_data [: 3 ] Out[13]: Global_active_power temperature 2007-01-01 00:00:00 2550.633 14.210 2007-01-01 01:00:00 2523.400 14.110 2007-01-01 02:00:00 2582.333 13.910 Before proceeding, we inspect to what extent this extended dataset contains missing data. In [14]: new_data . isna () . mean () Out[14]: Global_active_power 0.000 temperature 0.025 dtype: float64 In case of too many missing values, we could consider data imputation. However, in this case, only the temperature has 0.01% of msising values, so there is no need for imputation. Data Exploration In this section, we explore the extended dataset in order to understand which factors influence the household energy consumption. Based on this exploration, we will later on identify which features are useful for training a machine learning algorithm to forecast energy consumption. There are several approaches to data exploration. In this section we will use: visual exploration by means of time plots statistical data exploration Visual Data Exploration Time plots are a common way for exploring time series data, such as the energy consumption that we consider here. We explore time plots in different time frames in order to highlight different aspects in the data: we start with a yearly plot, which allows us to identify global patterns. a monthly plot allows us to zoom in and understand relationships between energy consumption in different weeks and days a weekly plot allows us to zoom in even further and identify daily and hourly patterns Yearly patterns It is expected that energy consumption depends on the outdoor temperature, since in winter time heaters are used more frequently than in summer time for example. For this reason, we plot the evolution of the energy consumption along with the temperature. In the plot below, we resample the hourly data of both datasets to daily data, so that we don't get lost in details but can observe more global patterns. The user can toggle on or off the normalization of the data. The normalization that is applied is called min-max normalization and is explained in detail in SK 3.4. It applies the following formulate to the data in order to make sure all values are contained within the [0, 1] range: \\begin{align*} \\mathit{X_{scaled}} = \\frac{X - X_{min}}{X_{max} - {X_{min}}}\\\\ \\end{align*} The user is invited to verify, without selecting the option normalization, whether of temperature and global active power have the same trend. The reader can then repeat the analysis using the option normalization. In [15]: plot_temperature_power_one_year ( new_data ) It should be clear that without normalization, it is difficult to compare the temperature with the global active power because both use different ranges, and the range of the global active power dominates the range of the temperature. With normalization however, the plot above reveals interesting insights. Both the temperature and the global active power follow a seasonal trend. Obviously, the temperature is lower in winter and higher in summer, whereas the reverse is true for the global active power. This is to be expected: energy consumption is higher during winter time. In more formal terms, we can observe that the temperate and global active power exhibit an inverse correlation: when one increases the other decreases, and vice versa. Monthly patterns Let's now see if there is a monthly recurring pattern as well. In the following figure, the active power of the selected year(s) is shown. The user can change the resampling rate to make the insights more clear to them. To make the plot more clear, the user can (de)selected the years they want on the right of the figure. In [16]: plot_yearly_data ( new_data ) While we can again clearly see the seasonal effect, there is no clear pattern recurring every month. One interesting thing that can be seen is that strong dips in power consumption tend to happen in the same time in February and November in 2007 and 2008. Most likely, these are holidays which tend to happen at the same time for a fixed household. However, these dips are not present at the same time for 2009. Weekly patterns In this section we further explore the evolution of the energy consumption in a more fine grained time frame. We want to verify whether during a week, the energy consumption exhibits the same patterns. In the plot below, the user can inspect and compare the energy consumption in December of different years. We mark weekends in yellow and Christmas day in orange. We invite the user to see for each year if they can notice a difference between week days and weekends and if there are any changes around the Holidays. In [17]: plot_consumption_with_holidays_weekends ( new_data ) What should be obvious from exploring this plot is the existence of a daily pattern for energy consumption dependent on the day of the week: in most weekend days of the three years, we can observe slightly higher energy consumption than during weekdays. This can depend on the common habits of working at the office or children being at school. The effect of the holidays can also be seen. On Christmas day in 2007, the power consumption is slightly higher than other typical weekdays, but similar values can be seen e.g. exactly one week before. So the holidays don't have a strong impact here. In 2008 on the other hand, the power consumption is lower, indicating that perhaps the household celebrated outside their house. In 2009, there is a peak in consumption on Christmas eve. We can conclude that whether the day is a weekday or in the weekend or a holiday has an influence on the expected power consumption. Daily patterns Next to yearly and monthly patterns, we are also interested in daily patterns. Below, we plot the evolution of the global active power for one week (The first week of December 2009). We can see how the power consumption follows a daily recurring pattern, with peaks in the morning and evening and troughs at night and midday. In fact, this pattern is also already present in the previous plot, where daily morning and evening peaks can also be observed. During the night and the day, we still see low amounts of power usage, most likely due to appliances like refrigerators and heating. In [18]: plot_weekly_data ( new_data ) Statistical Data Exploration The visualizations above highlighted interesting patterns in the energy consumption of the household that can be exploited for feature extraction. In this section we use statistical methods to verify, and eventually quantify those observations. Autocorrelation In order to find repetitive patterns in time series data, we can use autocorrelation , which provides the correlation between the time series and a delayed copy of itself. In order to investigate this autocorrelation, the plot below visualizes the autocorrelation of the global active power with time lags of 1 day, 2 days, etc. Note that we do not show the results of a delay of 0 days, since the correlation with the exact same day would trivially be 1. In [19]: plot_auto_correlation ( new_data ) The plot above reveals that a clear peak occurs after 1 day, indicating the consumption of a day is highly correlated with the consumption of the previous day another clear peak occurs every 7 days, which confirms the existence of a weekly pattern, i.e. the consumption of a day is highly correlated with the consumption of the same day a week earlier Correlation As visually observed above, the temperature influences the energy consumption of a household. In order to verify the strength of this relationship, we can compute the correlation between these two variables. For this purpose, we compute Pearson's correlation. The plot below shows again both temperature and consumption and allows to investigate their relationship. This relationship is influenced by the frequency of the dataset, e.g. temperature changes are typically less drastic than energy consumption changes. We invite the user to experiment with the different resampling periods, to understand how that changes the correlation and what frequency leads to the highest correlation. In [20]: plot_correlation ( new_data ) What should be obvious from the above is that there is a negative correlation between temperature and energy consumption, meaning the consumption rises when temperature drops and vice versa. The larger the resampling window is chosen, the higher is the correlation. This is because with larger windows, we smooth out more the short-term fluctuations, leaving only the seasonal pattern described above. Feature engineering In this section, we will extract several features from the extended dataset that can be used for forecasting energy consumption. A feature is an individual measurable property of the phenomenon being observed. Choosing informative, discriminating and independent features is a crucial step for building effective machine learning algorithms. Based on the previous visual and statistical observation, we will consider the following features to predict the energy consumption for the hour ahead: energy consumption of the day before energy consumption of the 7 days before the hour of the day, which will allow to distinguish between different periods of the day (i.e. night, morning, afternoon, evening) the day of the week, which will allow to distinguish between weekdays and weekend days whether the day is a holiday or not the month of the year, which will allow to distinguish between yearly seasons the temperature In [21]: new_feats = new_data [[ 'Global_active_power' ]] . rename ( columns = ({ 'Global_active_power' : 'consumption' })) new_feats [ 'Pastday' ] = new_feats [ 'consumption' ] . shift ( 1 , freq = 'D' ) new_feats [ 'Pastweek' ] = new_feats [ 'consumption' ] . shift ( 7 , freq = 'D' ) new_feats [ 'Hour' ] = new_feats . index . hour new_feats [ 'Weekday' ] = new_feats . index . dayofweek new_feats [ 'Month' ] = new_feats . index . month new_feats [ 'Holiday' ] = [ 1 if ( date in holidays . France ()) else 0 for date in new_feats . index ] new_feats [ 'Temperature' ] = new_data [ 'temperature' ] new_feats = new_feats . dropna () Modeling This section allows the user to discover the most important factors for training a machine learning model. More specifically, the user will get insights on the influence of the training strategy , type of machine learning model and effect of data normalization on model performance. To evaluate the performance of the models the mean absolute error (MAE) is used (a metric commonly used in literature for this purpose). This metric quantifies to what extent the model forecasts are close to the real values. As the name suggests, the mean absolute error is an average of the absolute errors. It holds that the lower the MAE of the model, the better its performance. This section ends with an interactive application where the user can test all options in order to find the one that leads the model to achieve the best performance. Training strategies The training data influences models performances. For this experiment, the following options are available: Use 1 month before the test month as training data. In this experiment, several models are trained for the predictions of each individual month. Each model is trained on one month before the month we are making predictions for. One month gap is introduced between the training and the test month (this is done to avoid that the last day of the training set is also included in the first day of the test set). For example, the models that will make the predictions for April 2008 will be trained on the data from February 2008. Use 6 months before the test month as training data. Similar to the above experiment, but with 6 months of training data. It still includes a 1 month gap. Use 1 year before the test month as training data. Similar to the above experiment, but with 1 year of training data, still include a 1 month gap. Use 1 month the year before as training data. Similar to strategy number 1, but with an 11 months gap. This way, the training data and test data are taken from the same month, one year apart. Use all months before the test month as training data. For each test month, train a model using all the data prior to this (including a 1 month gap). This strategy simulates the scenario where the model is retrained as new data comes in. Since this requires training several models based on potentially large training sets, this might require a certain computational time. Use train-test split . Train on the first year of data and make predictions on the rest. This strategy is commonly used for training these kinds of models. Notice that for the benchmark model, there is no training phase. Machine learning models There is a plethora of models that can be used for forecasting. In this notebook, we decide to use two models that are commonly used in literature for the task at hand: Random Forest Regression (RFR) and Support Vector Regression (SVR) . Besides these models, we will also use a simple benchmark model that we use to compare the performance of the other models against. For the prediction of the energy of a certain day, this benchmark model simply takes the energy consumption of the previous day at the same time. We will use these models to forecast the energy consumption based on the past energy measurements (see training strategies). Normalization Before being used for training, data may need to be normalized, i.e. rescaled so that all input and output values are between 0 and 1. This is indeed a requirement for the correct training of some machine learning models. It is left to the user the exercise of discovering which model requires the normalization of the data. In [22]: from IPython.utils import io PORT = '8095' # in AWS use ports 8050-8060 mode = 'external' #external if mode == 'inline' : run_app ( new_feats , mode = 'inline' , port = PORT ) else : with io . capture_output () as captured : run_app ( new_feats , mode = 'external' , port = PORT , host = '0.0.0.0' ) HOST = '0.0.0.0' # in AWS replace here by public IP print ( f 'Dash app running on http:// { HOST } : { PORT } ' ) Dash app running on http://0.0.0.0:8095 The mean absolute error of each experiment that was run is reported in the table above. Insights from the experiments These are the basic insights the user could have noticed when trying different models, parameters and training strategies. Effect of the training strategy For the SVR, we see that the standard train-test split has the best performance, although using all months and using a one year window before the test month as training data both have a similar performance. For the RFR on the other hand, we clearly see that using a one year window before the test month as training data leads to better performance than the other training strategies. We note that for both this \"one year window\" strategy and the standard train-test split, the training data is one year. The difference however, is that for the former the training set changes, while for the latter it is fixed (using all the data from 2017). A general trend that can be seen is that the performance increases as a larger training set is used. We therefore urge the user to look at the predictions made using the training strategy where all months before the test month are used as training data: we can expect that the predictions for the later months will be better than for the first months since more training data was used. However, this is hard to see by eye in the prediction plot directly. Effect of the chosen model We can see that, for most parameter choices, both the SVR and RFR can outperform the simple benchmark model we set up. The RFR outperforms the SVR for most (if not all) parameter choices and training strategies. This shows the importance of knowing which model is best suited for your problem and testing different ones. Effect of the chosen parameters The search for the optimal set of hyperparameters for your machine learning model is a non-trivial task, since there might be a strong interplay between the parameters so we can not do the optimal parameter search one by one. Here, we let the user try a pre-set number of choices, but more automatic and smart search algorithms exist to tackle this problem. We found that for the RFR, the higher the number of estimators, the better the results, but at a greater computational cost and with diminishing returns. We see that 50 estimators and leaving the max depth at 10 is a good choice. For the SVR, $C=10$ and $\\gamma=0.01$ work well. Effect of normalization Normalizing the data, rescaling it so that the input and output variables all have values within a similar range (in this case, between 0 and 1) is a common step when setting up machine learning models. This is because, depending on the model, they work much better with values of the order 1. We see that normalization indeed greatly improves the predictions for the SVR, but has very little influence on the RFR. Indeed, algorithms such as random forest and decision tree are not influenced by the scale of the input variables. Conclusion In this Starter Kit we demonstrated forecasting the demand for a particular resource at a particular point in the future, illustrated in the case of energy demand forecasting. To this end, we first analysed the business problem, preprocessed the data, visually and numerically explored it to eventually illustrate the use of different models ( Random Forest Regression, Support Vector Regression and benchmark) to perform the actual forecasting with different amounts of available historical data. From the analysis, it may be clear that achieving good prediction results depend on different factors (appropriate data preprocessing, the amount of available training data, algorithm parametrisation, etc.). Additional Information Contributions by Mathias Verbeke, Wannes Meert, Vincent Vercruyssen, Alessandro Murgia, Tom Tourw√©, and Robbert Verbeke. This Starter Kit was developed in the context of the EluciDATA project ( http://www.elucidata.be ). For more information, please contact info@elucidata.be .","tags":"Time Series Preprocessing","url":"/Time Series Preprocessing/2020-01-01-interactive-notebook.html","loc":"/Time Series Preprocessing/2020-01-01-interactive-notebook.html"},{"title":"Interactive notebook","text":"In [2]: import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn import preprocessing import re import os from ipywidgets import register import cufflinks as cf cf . go_offline ( connected = True ) cf . set_config_file ( colorscale = 'plotly' , world_readable = True ) % matplotlib inline from IPython.core.display import HTML from IPython.display import Markdown , display def printmd ( string ): display ( Markdown ( string )) from starterkits.visualization import vis_plotly_widgets as vpw import visualizations as v import support as sp from support import LstmApp , get_test_scores # Fixing the used seeds for reproducibility of the experiments np . random . seed ( 1234 ) PYTHONHASHSEED = 0 display ( HTML ( \" \" )) /Users/hcab/opt/miniconda3/lib/python3.7/site-packages/dash_bootstrap_components/table.py:1: UserWarning: The dash_html_components package is deprecated. Please replace `import dash_html_components as html` with `from dash import html` Starter Kit 1.2.1: Remaining useful life estimation Business context Maintenance is an important part of an asset's lifecycle. In the past, corrective and preventive maintenance were the norm: corrective maintenance was performed when an asset had failed and involved tasks for identifying the fault and rectifying it so that the asset could resume normal operation preventive maintenance was performed to avoid asset failure and involved executing maintenance tasks at regular intervals, e.g. when an asset was used for a certain period of time or when it executed a pre-determined number of cycles. Nowadays, ever more assets are equipped with sensors and ever more data is gathered. This data can be exploited to realize predictive maintenance, which promises costs saving over corrective or preventive maintenance because maintenance is only performed when warranted. Predictive maintenance encompasses a variety of topics, such as failure prediction, remaining useful lifetime estimation (RUL), failure detection, failure diagnosis (root cause analysis) and recommendation of mitigation or maintenance actions after failure. Business goal The business goal addressed by this Starter Kit is the estimation of the remaining useful lifetime of an asset. 'Lifetime' in this context is expressed in terms of the evolution of a particular quantity, such as the distance travelled, fuel consumed, repetition cycles performed, number of transactions experienced, etc. Estimating the remaining useful lifetime is non-trivial, as it is influenced by a multitude of internal and external factors, e.g. operating conditions, weather conditions, usage scenarios, etc. Application contexts Estimating the remaining useful lifetime of an asset can be beneficial for several purposes in a variety of industrial contexts, for example: to better schedule maintenance operations, e.g. for offshore wind turbines, for which maintenance can only be performed under the right weather conditions. to optimise the operational efficiency of an asset, by more optimally planning the use of that asset before its end of life, or adapting its operational use to extend its useful lifetime. to avoid unplanned downtime, e.g. in manufacturing settings, where a downtime of the production line results in several operational problems and associated costs. to anticipate and avoid safety-critical situations, e.g. for aircrafts or vehicles. ... Data characteristics and requirements Typically, the data to be exploited for remaining useful lifetime estimation has the following characteristics: it consists of time-series data, in particular for the quantity used to express the lifetime of the asset but also for all the factors that influence that quantity (e.g. weather and operating conditions). temporal patterns such as trends, cycles, seasonality, day/night or week/weekend patterns can be present in the data. random fluctuation (noise) can also be present, as a result of random variation or unexplained causes. In a large number of cases the data is measured using sensors, therefore some noise is inherently present on the measured signal. Moreover, the following requirements are imposed on the dataset: a sufficiently large amount of historical data needs to be available. Two aspects are important in this respect: the implicit assumption is that an asset has a continuous degradation pattern, which is reflected in the asset's sensor measurements. Degradation is often slow, however, so the monitoring period should be sufficiently long so as to include a clear presence of the degradation pattern. a sufficient number of examples of assets that reached their end of life needs to be present. the data needs to include explicit information on when an asset has reached its end of life or should allow to easily derive it (e.g. the lifetime quantity has reached a particular threshold). Starter Kit outline The data-driven approach for remaining useful life estimation will be illustrated on the industrially-relevant use case of predicting the remaining useful lifetime of aircraft engines based on run-to-failure data . We will use this dataset to illustrate: How to appropriately preprocess the data and construct training, test and validation sets to learn and evaluate a model able to predict whether an engine will fail within a certain number of operational cycles. How to use deep learning to construct a model for remaining useful lifetime prediction. In particular, we apply a deep learning approach that uses short-term memory ( LSTM ) networks and allows the construction of a predictive model without the time-consuming feature engineering step, which requires one to manually extract the characteristics from which the algorithm can learn. How to experimentally validate the resulting model and objectively compare it with other approaches. The resulting model achieves a high accuracy. The approach is simple and flexible enough to be applied to a wide range of RUL problems. Data understanding The time series data we will use in this Starter Kit is simulated run-to-failure data from aircraft engines. Engines in the data are assumed to start with varying degrees of wear and manufacturing variation. This information is unknown to the user. Furthermore, in this simulated data, engines are assumed to be operating normally at the beginning, and start to degrade at some point during operation. The degradation progresses and grows in magnitude. When a predefined threshold is reached, the engine is considered unsafe for further operation. In other words, the last operational cycle of the engine can be considered as the failure point of the corresponding engine. In machine learning experiments, a dataset is often split into a training set and a test set, which allows one to quickly evaluate the performance of an algorithm on the problem at hand. The training dataset is used to prepare a model, to train it. We pretend the test dataset is new, unseen data where the output values (in this case, whether or not a specific engine is going to fail within a predefined number of cycles) are withheld from the algorithm. We gather predictions from the trained model on the inputs from the test dataset and compare them to the withheld output values of the test set, the so-called ground truth . Comparing the predictions and withheld outputs on the test dataset allows us to compute a performance measure for the model on the test dataset. This is an estimate of the skill of the algorithm trained on the problem when making predictions on unseen data. For this reason, the dataset is divided into three parts, namely the training, test and ground truth datasets: The training data will be used by the algorithm to learn the prediction model. It consists of multiple time series with \"cycle\" as the time unit, and 21 sensor readings for each cycle. Each sequence of cycles can be assumed as being generated from a different engine (identified by the engine ID) of the same type. Since the algorithm needs to be able to learn when the engine will fail, the last time period does represent the failure point . The test data has the same data schema as the training data and is used to test the quality of the resulting model. The only difference is that the data does not indicate when the failure occurs, or put differently, the last time period does not represent the failure point. The ground truth data provides the number of remaining working cycles for the engines in the test data, which will only be used to assess the quality of the results, but will not be given as input to the learned model. More details on this dataset can be found in [1]. In [3]: train_df , test_df , truth_df = sp . load_data () The table below shows an excerpt of the training data, with the following attributes: id: the identifier of the engine cycle: the current cycle of the engine from which the data in the respective row is originating, i.e. the current flight of the aircraft setting 1-3: the values of 3 different operational settings of the engine, e.g. an operational setting could correspond to a cruise setting when the aircraft is flying at cruise speed s1-21: 21 sensor readings monitoring various parameters of the engine, e.g. temperature or vibration Since the input file is space-separated, and two spurious spaces appear at the end of each line, we remove the last two columns. In [4]: train_df . head () Out[4]: id cycle setting1 setting2 setting3 s1 s2 s3 s4 s5 ... s12 s13 s14 s15 s16 s17 s18 s19 s20 s21 0 1 2 0.0019 -0.0003 100.0 518.67 642.15 1591.82 1403.14 14.62 ... 522.28 2388.07 8131.49 8.4318 0.03 392 2388 100.0 39.00 23.4236 1 1 3 -0.0043 0.0003 100.0 518.67 642.35 1587.99 1404.20 14.62 ... 522.42 2388.03 8133.23 8.4178 0.03 390 2388 100.0 38.95 23.3442 2 1 4 0.0007 0.0000 100.0 518.67 642.35 1582.79 1401.87 14.62 ... 522.86 2388.08 8133.83 8.3682 0.03 392 2388 100.0 38.88 23.3739 3 1 5 -0.0019 -0.0002 100.0 518.67 642.37 1582.85 1406.22 14.62 ... 522.19 2388.04 8133.80 8.4294 0.03 393 2388 100.0 38.90 23.4044 4 1 6 -0.0043 -0.0001 100.0 518.67 642.10 1584.47 1398.37 14.62 ... 521.68 2388.03 8132.85 8.4108 0.03 391 2388 100.0 38.98 23.3669 5 rows √ó 26 columns The test data looks similar to the training data, with the difference that it does not indicate when the failure occurs, or put differently, the last time period does not represent the failure point. Taking the sample test data shown in the table below as an example, the engine with id=1 runs from cycle 1 through cycle 31. It is not shown how many more cycles this engine can last before it fails. In [5]: test_df [ 25 : 31 ] Out[5]: id cycle setting1 setting2 setting3 s1 s2 s3 s4 s5 ... s12 s13 s14 s15 s16 s17 s18 s19 s20 s21 25 1 27 -0.0007 0.0001 100.0 518.67 642.08 1586.65 1400.31 14.62 ... 521.82 2388.02 8127.24 8.4494 0.03 392 2388 100.0 38.87 23.3931 26 1 28 0.0022 0.0005 100.0 518.67 641.93 1594.25 1401.29 14.62 ... 521.84 2388.07 8134.89 8.4470 0.03 392 2388 100.0 38.83 23.3502 27 1 29 0.0014 0.0001 100.0 518.67 641.95 1587.15 1398.11 14.62 ... 522.39 2388.07 8133.13 8.4212 0.03 392 2388 100.0 39.02 23.3621 28 1 30 -0.0025 0.0004 100.0 518.67 642.79 1585.72 1400.97 14.62 ... 521.78 2388.10 8134.79 8.4110 0.03 391 2388 100.0 39.09 23.4069 29 1 31 -0.0006 0.0004 100.0 518.67 642.58 1581.22 1398.91 14.62 ... 521.79 2388.06 8130.11 8.4024 0.03 393 2388 100.0 38.81 23.3552 30 2 1 -0.0009 0.0004 100.0 518.67 642.66 1589.30 1407.16 14.62 ... 521.62 2388.14 8129.59 8.4283 0.03 392 2388 100.0 39.00 23.3923 6 rows √ó 26 columns The ground truth data provides the number of remaining working cycles for the engines in the test dataset. Taking the sample ground truth data shown in the following table as an example, the engine with id 1 in the test data can run for another 112 cycles before it fails. In [6]: display ( truth_df . set_index ( truth_df . index + 1 ) . head ( 3 )) 112 1 98 2 69 3 82 Data preprocessing Before we can start training a model to predict the RUL, a number of preparatory steps needs to be taken. As could be seen from the training data sample above, currenly the data does not include a column indicating the RUL, which is the target value we want to predict. As the algorithm needs this information to learn from, the first step will be to create such a column. A second observation that can be made on the training data sample is that the scale of the values differs across columns. This difference could cause problems when the model needs to calculate the similarity between different cycles (i.e., the rows in the table) during modeling. To address this problem, we will normalise the data. The goal of normalisation is to change the values of the numeric columns in the dataset, in order to have a common scale and avoid distorting differences in the ranges of values or losing information. The normalised values maintain the general distribution and ratios in the source data, while keeping values within a scale applied across all numeric columns used in the model. RUL label creation The first preprocessing step is to generate a label indicating the Remaining Useful Lifetime. In this Starter Kit, we will not predict the exact number of remaining cycles the engine still has left before failure, yet rather try answering the question 'Is a specific engine going to fail within w1 cycles?', where w1 is a predefined threshold used during the label creation process. Training data In order to determine the remaining number of cycles at each time point, the maximum number of cycles per engine is first determined. Subsequently, the current cycle number is substracted from the max number of cycles to obtain the number of cycles remaining at a particular time point. In the following sample, the column RUL indicates the remaining number of cycles for that particular point in time (i.e. the row). In [7]: rul = pd . DataFrame ( train_df . groupby ( 'id' )[ 'cycle' ] . max ()) . reset_index () rul . columns = [ 'id' , 'max' ] train_df = train_df . merge ( rul , on = [ 'id' ], how = 'left' ) train_df [ 'RUL' ] = train_df [ 'max' ] - train_df [ 'cycle' ] train_df . drop ( 'max' , axis = 1 , inplace = True ) Xtrain = train_df . copy () train_df [[ 'id' , 'cycle' , 'RUL' ]] . head () Out[7]: id cycle RUL 0 1 2 190 1 1 3 189 2 1 4 188 3 1 5 187 4 1 6 186 In order to arrive to a binary label indicating whether an engine is going to fail within w1 cycles, the next step is to test whether the number of remaining cycles in the RUL column is smaller than or equal to the predefined threshold w1 , currently set to 30 cycles. This is illustrated in the sample below, where for the first row, the value in the column RUL_label is 1, because the engine will fail within 4 cycles. In [8]: w1 = 30 train_df [ 'RUL_label' ] = train_df . RUL . apply ( lambda x : x <= w1 ) . astype ( int ) train_df [[ 'id' , 'cycle' , 'RUL' , 'RUL_label' ]] . tail () Out[8]: id cycle RUL RUL_label 20625 100 196 4 1 20626 100 197 3 1 20627 100 198 2 1 20628 100 199 1 1 20629 100 200 0 1 Test data A similar approach is adopted for the test set. Note that, in order to generate the labels for the test data, we need to use the ground truth dataset, because the test data does not indicate when the failure occurs, or put differently, the last time period does not represent the failure point. In [9]: # Generate column max for test data rul = pd . DataFrame ( test_df . groupby ( 'id' )[ 'cycle' ] . max ()) . reset_index () rul . columns = [ 'id' , 'max' ] truth_df . columns = [ 'more' ] truth_df [ 'id' ] = truth_df . index + 1 truth_df [ 'max' ] = rul [ 'max' ] + truth_df [ 'more' ] truth_df . drop ( 'more' , axis = 1 , inplace = True ) # Generate RUL for test data test_df = test_df . merge ( truth_df , on = [ 'id' ], how = 'left' ) test_df [ 'RUL' ] = test_df [ 'max' ] - test_df [ 'cycle' ] test_df . drop ( 'max' , axis = 1 , inplace = True ) # Generate label columns RUL and RUL_label for test data test_df [ 'RUL_label' ] = np . where ( test_df [ 'RUL' ] <= w1 , 1 , 0 ) test_df [[ 'id' , 'cycle' , 'RUL' , 'RUL_label' ]]; Normalisation using Min-Max scaling As explained before, the goal of normalisation is to change the values of numeric columns in the dataset to a common scale. In this notebook, to put it simply, normalisation is performed to avoid variables on a higher scale affecting the outcome in bigger measure only because they are on a higher scale. One of the most popular normalisation technique is Min-Max normalisation , which we also use here. It scales the values of a variable to a range between 0 and 1 using the following formula, where $X$ represents the value to be normalised, $X_{min}$ is the minimum value of the variable in that column and $X_{max}$ is the maximum value for that variable: $X_{norm} = \\frac{X - X_{min}}{X_{max} - X_{min}}$ Note: an alternative approach to Min-Max scaling is Z-score normalisation (or standardisation ) in which the variables will be rescaled so that they have the properties of a standard normal distribution with zero mean and unit variance. For both the training set and the test set, we normalise all columns except id , cycle , and RUL , as we want to keep the original values for these columns. The following sample shows a portion of the normalised training data. In [10]: train_df [ 'cycle_norm' ] = train_df [ 'cycle' ] cols_normalize = train_df . columns . difference ([ 'id' , 'cycle' , 'RUL' ]) min_max_scaler = preprocessing . MinMaxScaler () norm_train_df = pd . DataFrame ( min_max_scaler . fit_transform ( train_df [ cols_normalize ]), columns = cols_normalize , index = train_df . index ) join_df = train_df [ train_df . columns . difference ( cols_normalize )] . join ( norm_train_df ) train_df = join_df . reindex ( columns = train_df . columns ) train_df . describe () . loc [[ 'mean' , 'min' , 'max' ]] Out[10]: id cycle setting1 setting2 setting3 s1 s2 s3 s4 s5 ... s15 s16 s17 s18 s19 s20 s21 RUL RUL_label cycle_norm mean 51.509016 108.813088 0.499492 0.501975 0.0 0.0 0.443065 0.424747 0.450442 0.0 ... 0.451122 0.0 0.434226 0.0 0.0 0.524232 0.546119 107.803829 0.150267 0.298651 min 1.000000 1.000000 0.000000 0.000000 0.0 0.0 0.000000 0.000000 0.000000 0.0 ... 0.000000 0.0 0.000000 0.0 0.0 0.000000 0.000000 0.000000 0.000000 0.000000 max 100.000000 362.000000 1.000000 1.000000 0.0 0.0 1.000000 1.000000 1.000000 0.0 ... 1.000000 0.0 1.000000 0.0 0.0 1.000000 1.000000 361.000000 1.000000 1.000000 3 rows √ó 29 columns As can be seen, both the settings variables and the sensor variables now only contain values between 0 and 1. In [11]: test_df [ 'cycle_norm' ] = test_df [ 'cycle' ] norm_test_df = pd . DataFrame ( min_max_scaler . transform ( test_df [ cols_normalize ]), columns = cols_normalize , index = test_df . index ) test_join_df = test_df [ test_df . columns . difference ( cols_normalize )] . join ( norm_test_df ) test_df = test_join_df . reindex ( columns = test_df . columns ) test_df = test_df . reset_index ( drop = True ); Data Modeling and Analysis Traditional machine learning models are usually based on manually extracting distinguishing factors that characterise the phenomenon to be learned using domain expertise (the so-called feature engineering step). This usually makes these models hard to reuse since feature engineering is specific to the problem scenario and the available data, and therefore a labour-intensive process. In contrast, deep learning algorithms can automatically extract the right features from the data, eliminating the need for manual feature engineering. Deep learning refers to machine learning approaches involving artificial neural networks (ANNs) that contain a multitude of so-called hidden layers. One particular type of ANNs are the Long Short-Term Memory networks, or LSTMs. One critical advantage of LSTMs is their ability to remember information from long-term sequences, which is hard to achieve with traditional feature engineering. When manually engineering features for this problem, one would for example compute the rolling average per windows of 50 cycles. This averaging may however lead to a loss of information due to the smoothing and abstracting of values over such a long period. Instead, using all 50 values as input may provide better results. While performing feature engineering over large window sizes may complicate things for traditional machine learning methods, LSTMs are naturally able to handle larger window sizes and use all the information in the window as input. Algorithm-specific data preparation As a first step in the modeling phase, we will prepare the data to serve as input for the LSTM network. When using LSTMs in the time-series domain, one important parameter to pick is the sequence length, which is the size of the window within which the LSTMs needs to remember information. This is similar to picking a window size when calculating time series features using a rolling window. The idea of using LSTMs is to let the model extract abstract features from the sequence of sensor values in the window rather than engineering those manually. The expectation is that if there is a pattern in these sensor values within the window prior to failure, the pattern should be extracted and learned by the LSTM. The window size chosen for the training data set strongly influences the prediction results. In the animation below, you can see how differently the single variables for a given engine behave. Furthermore, the time at which the degradation becomes visible is different for different engines. When using the initial settings in the animation below, i.e. Variable equals s11 , ID equals 1 and period equals 50, we see a comparably short line fluctuating around a constant value. When selecting the IDs 3 and 13 instead and increasing the period to 130 at the same time, one sees a clear deviation from the median value indicated by the horizontal gray dashed line. The deviation for both starts at around index 110. When then additionally selecting the engine with ID 36, the deviation starts already at approximately index 80 and therewith significantly earlier. Further looking at Variable s12 , the deviation from the median starts at the same times as for s11 for the engines under inspection. When selecting the training period for the model, we need to take the different start of the deviation into account. Here we need to find a good balance between the two different influences: on the one hand, the longer the training data sequence, the better the results; on the other hand, when selecting a too long training period, it is possible that the model learns that a slight deviation is fine in several engines and will therewith fail in predicting the remaining useful lifetime. The period length will be used in the remainder of the notebook to calculate the model sequences. In [12]: date_range_slider = v . interactive_plot ( test_df ) Based on the figures above, it is very hard to visually spot any trends that are influencial for the remaining useful life, let alone to manually extract features from it. As indicated above, the advantage of deep learning algorithms is that they can automatically extract the right features from the data, eliminating the need for manual feature engineering. In the following, we pick the feature columns ( sensor_cols ) to use for modelling as well as the columns identifying the single sequences ( sequence_cols ). In [13]: sensor_cols = [ x for x in test_df if re . findall ( string = x , pattern = 's[0-9]' )] sequence_cols = [ 'setting1' , 'setting2' , 'setting3' , 'cycle_norm' ] sequence_cols . extend ( sensor_cols ) Keras is a deep learning library written in Python, which naturally interfaces with TensorFlow , an open-source software library for machine learning. Its LSTM layer expects an input in the shape of a 3-dimensional matrix of (samples, time steps, features) (more specifically a 3-dimensional NumPy array) where 'samples' is the number of training sequences, 'time steps' is the look back window or sequence length and 'features' is the number of features of each sequence at each time step. During this transformation, we also select the columns from the input data that can serve as features. In this case, we selected all sensor values, as well as the 3 settings parameters and the (normalised) current cycle the machine is in at this point. In [14]: def gen_sequence ( id_df , seq_length , seq_cols ): \"\"\" Function to reshape the data into a 3-dimensional (samples, time steps, features) matrix \"\"\" data_array = id_df [ seq_cols ] . values num_elements = data_array . shape [ 0 ] for start , stop in zip ( range ( 0 , num_elements - seq_length ), range ( seq_length , num_elements )): yield data_array [ start : stop , :] For each id in the data set, a data sequence will be created. As initial sequence length, we will use 50 time steps. In [15]: register ( date_range_slider ) sequence_length = 50 seq_gen = ( list ( gen_sequence ( train_df [ train_df [ 'id' ] == id ], sequence_length , sequence_cols )) for id in train_df [ 'id' ] . unique ()) seq_array = np . concatenate ( list ( seq_gen )) . astype ( np . float32 ) printmd ( f '''This results in a 3-dimensional input matrix with { seq_array . shape [ 0 ] } samples, { seq_array . shape [ 1 ] } time steps and { seq_array . shape [ 2 ] } features (including 21 sensor values, the 3 settings parameters and the normalised current cycle).''' ) This results in a 3-dimensional input matrix with 15630 samples, 50 time steps and 25 features (including 21 sensor values, the 3 settings parameters and the normalised current cycle). For the training data, the labels are also added to this data structure, so that the LSTM can learn from it. In [16]: def gen_labels ( id_df , seq_length , label ): data_array = id_df [ label ] . values num_elements = data_array . shape [ 0 ] return data_array [ seq_length : num_elements , :] def handler ( x ): pass sequence_length = date_range_slider . get_interact_value () label_gen = [ gen_labels ( train_df [ train_df [ 'id' ] == id_ ], sequence_length , [ 'RUL_label' ]) for id_ in train_df [ 'id' ] . unique ()] label_array = np . concatenate ( label_gen ) . astype ( np . float32 ) v . plot_label_frequency ( label_array ) The majority of data points, roughly 80 %, are labeled as zero, indicating that no failure was observed. LSTM network Note: we refer the interested reader to Appendix A for a brief introduction to LSTM networks. In this section, we guide you through the general process of building, training, evaluating and testing a LSTM model. This will be done by means of an interactive interface. In order to see the code used for these steps, please expand the fields below. Exercise 1 Network construction Let us start by building an LSTM network. In our setup, we will experiment with a stacked structure. In order to learn from different architectures of the LSTM model, it is possible to change the model variables in the animation below. We ask the reader to test different model scenarios by applying the following steps: Select the number of intermediate layers. For the first experiment, please select a single intermediate layer only. Set the size of the intermediate layer to 30. This corresponds to a rather small network. Set the number of epochs for training to 15. Epochs define the number of times the model iterates over the training data arrays, the aim being for the model to improve during each training epoch. In general, the more epochs the better the results, until we reach the given model's limit. Similarly as before, the training sequence length defines the number of data points to take into account for training per engine. Please select a sequence length of 50. Furthermore, we add a dropout layer after each LSTM layer. The dropout consists in randomly setting a fraction rate of input units to 0 at each update during training time, which helps preventing overfitting. Overfitting means that a model fits too closely a limited set of data points. This typically results in an overly complex model that fits all little details in the data under study. Please choose a value of 0.2 for this first experiment. Model training Now that we have fixed the network design, we can fit the network to the training data by pushing the button Train model . After a successful training, a green badge will appear. The fitting step is influenced by the following parameters, which we will keep fixed during our experiments: Batch size: The number of samples that are used per gradient update (while fixing the best parameters for the model). We will use a batch size of 200. Validation split: fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. We will use a validation split of 5%. Furthermore, the fitting function will stop early if the validation loss (which is the quality criterion that is used to evaluate the model performance) is not decreasing anymore. To parametrise the early stopping function, the following parameters can be handled by TensorFlow: Minimal delta: minimum change in the monitored quantity as given by the validation loss to qualify as an improvement, i.e. an absolute change of less than min_delta will count as no improvement. Patience: number of epochs with no improvement after which training will be stopped. In the experiments defined in this Starter Kit, we keep both minimal delta and patience at zero. Model evaluation and testing Now that we have a fitted model, we can evaluate its performance. We will first evaluate the model performance on the training data and subsequently on the test data. If both evaluations result in approximately the same score, this gives an indication that the model is generalisable to unseen datasets and thus is not overfitting on the training data. In the interface, please switch to the tab Evaluation at the top and press the button Evaluate model . This will lead to the evaluation of the last trained model. The model can be evaluated using different measures. We will on the one hand evaluate the model in function of accuracy , which measures the fraction of all instances that are correctly categorized; it is the ratio of the number of correct classifications to the total number of instances to classify. On the other hand, we will show the so-called confusion matrix . The confusion matrix for the model that we just trained shows that the model is able to correctly classify that a specific engine is not going to fail within w1 cycles for almost all of the 12531 samples. Vice versa, the model is able to correctly classify for almost all of the 3100 cases that a specific engine is going to fail within w1 cycles. \\ Based on the confusion matrix, several other measures such as precision and recall can be calculated. Intuitively, precision is the ability of the classifier not to label a sample that is negative as positive. When instantiated for the RUL prediction problem we are currently considering, precision indicates how many of the engines that the model predicted to reach their end of life within 30 cycles also actually reach their end of life within this amount of cycles. Recall is the ability of the classifier to find all the positive samples, i.e., all engines that reach their end of life within 30 cycles. The respective results are given in the model evaluation summary table. A unique id will be assigned to each evaluated model in order to compare the results for different models. Note that the evaluation is performed on the training data. In order to evaluate it on the test data (that is unknown to the model), please continue to the tab Testing . By pushing the button Evaluate model , the model selected in the dropdown menu is evaluated on the test data. Questions: Is there a significant difference in accuracy when evaluating the model on the validation and test data? Do you think the model is overfitting? Exercise 2: The LSTM network built in Exercise 1 performs well on the evaluation data set with an accuracy of about 95% but with a lower accuracy on the test data. This gives us a hint that the model might be overfitting on the training data. Therefore, in this second experiment, we add a second LSTM layer to the network. Please increase the number of layers to 2 and set the following layer sizes: Intermediate Layer 1: 200 Intermediate Layer 2: 50 Keep the remaining parameters as they are. Subsequently repeat the steps taken in the previous exercise: Train the model Evaluate it on the validation data Evaluate it on the test data Questions: Which difference do you observe? Compare the results in the tables in both tabs - evaluation and testing When comparing the two models, which one would you use and why? Exercise 3: Use the interface to experiment with alternative architectures and see how the parameters influence the quality of the model. You can try to answer questions such as: Will the model improve when the number of epochs is increased? Is it possible to reach an accuracy of 98 % on both data sets (validation and test)? Does adding a third LSTM layer improve the results even further? LSTM interface Note: if the current port is already in use, change it to any port between 8050 and 8060 In [17]: PORT = 8050 v . start_dashboard ( Xtrain , sequence_cols , test_df , PORT ) Dash app running on http://127.0.0.1:8050 Code In what follows, you can find the most prominent functions that are called in the interactive interface to construct and train the LSTM model, and subsequently evaluate it. Model building and compilation def compile_model ( num_layers = 2 , sizes ): model = Sequential ( name = 'Sequential_Layer' ) for i in range ( num_layers ): if sizes [ i ]: if i == 0 : model . add ( LSTM ( input_shape = ( seq_len , nb_features ), units = sizes [ i ], return_sequences = bool ( sizes [ i + 1 ] is not None ))) else : model . add ( LSTM ( units = sizes [ i ], return_sequences = bool ( sizes [ i + 1 ] is not None ))) model . add ( Dropout ( dropout )) model . add ( Dense ( units = nb_out , activation = 'sigmoid' )) model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ]) return ( model ) Model training model . fit ( seq_array , label_array , epochs = epochs , batch_size = 200 , validation_split = 0.05 , verbose = 0 , callbacks = [ keras . callbacks . EarlyStopping ( monitor = 'val_loss' , min_delta = 0 , patience = 0 , verbose = 0 , mode = 'auto' )]) Make predictions and compute confusion matrix y_pred = model . predict_classes ( seq_array , verbose = 1 , batch_size = 200 ) y_true = label_array cm = confusion_matrix ( y_true , y_pred ) Comparison of performance against other methods In order to get an idea of the quality of the LSTM model, we will compare it with the best performing model from [1]. In this experiment, four binary classification models are compared, namely a Two-Class Logistic Regression model, a Two-Class Boosted Decision Tree, a Two-Class Decision Forest, and a Two-Class Neural Network. Note that for these models, a manual feature engineering step was needed, whereas in the case of the LSTM model, the algorithm automatically decides on the most appropriate features. The table below compares the best performing model, namely a Two-Class Neural Network, with the LSTM approach, in terms of accuracy, precision, recall, and F1-score. In [18]: results_df = get_test_scores () results_df Out[18]: model id Accuracy Precision Recall F1-score model_type Two-Class Neural Network (best performing model from [1]) - 94.0 0.952381 0.8 0.869565 Conclusion In this Starter Kit, we covered the basics of using deep learning for remaining useful life prediction, which is one of the central topics in predictive maintenance. We illustrated the approach on the industrially-relevant use case of predicting the remaining useful lifetime of aircraft engines based on run-to-failure data. In doing so, we have shown: How to appropriately preprocess the data and construct training, validation and test sets to learn and evaluate a model that predicts whether an engine will fail within a certain number of operational cycles. How to use deep learning to construct a model for remaining useful life prediction. In particular, we applied a deep learning approach called long short-term memory (LSTM) networks which allows the construction of a predictive model without the time-consuming feature engineering step, which requires one to manually extract the characteristics from which the algorithm can learn. How to experimentally validate the resulting model and objectively compare it with other approaches. Comparing the above results on the test set, we see that the LSTM results are comparable to the results of the best performing model in [1] , with the advantage of not requiring a manual feature engineering step. Furthermore, it should be noted that the data set used here is very small and deep learning models are known to perform better with large datasets, so for a more fair comparison larger datasets should be used. The approach is simple and flexible enough to be applied to a wide range of RUL problems. Possible next steps One of the attention points when working with deep learning techniques is that it is important to tune the models to the right parameters, such as the window size. While the presented approach already gives promising results, quite some improvements are still possible, such as: Trying different window sizes. Trying different architectures with a different number of layers and nodes. Trying to tune the hyperparameters of the network. Trying to cast the RUL prediction problem as a regression or multi-class classification task instead of as a binary classification task. Trying on a larger dataset with more instances. Appendix A: A brief introduction to LSTM networks In the following, we will briefly explain the intuition behind an LSTM network, yet for a more detailed explanation we refer the interested reader to the original publication [2] or more intuitive descriptions (e.g. [3] , [4] ). A single layer LSTM consists of several blocks . Each block takes three inputs: 1) the input of the current time step, 2) the output from the previous LSTM block (of the previous time step) and 3) the \"memory\" of the previous block. Each block also provides output for the current timestep, and produces an updated memory of the current block. Therefore, a single block makes a decision by considering the current input, previous output and previous memory, generates a new output and alters its memory. This information flow is graphically represented in the following figure (taken from [4] ): The way the internal memory changes is controlled by different gates that control the information flow. Each gate computes a value between 0 and 1 using a logistic function to compute a value. The data is multiplied by this value to partially allow or deny information to flow into or out of the memory. An input gate controls the extent to which a new value flows into the memory. A forget gate controls the extent to which a value remains in memory. An output gate controls the extent to which the value in memory is used to compute the output activation of the block. An LSTM thus keeps two pieces of information as it propagates through time: A hidden state ; which is the memory the LSTM accumulates using its (forget, input, and output) gates through time, The previous time-step output . An important parameter when constructing LSTM networks is the size of the LSTM's hidden state. In the underlying computation framework TensorFlow that is used in this notebook, this parameter is set by the variable num_units . To make the name num_units more intuitive, you can think of it as the number of hidden units in the LSTM block, or the number of memory units in the block. The explaination above describes the structure of a single LSTM layer. However, an LSTM network usually consists of multiple layers , that can be stacked or merged. In case the layers are stacked, the output of the previous layer serves as input for the next layer. References [Predictive Maintenance Template](https://gallery.cortanaintelligence.com/Collection/Predictive-Maintenance-Template-3) Hochreiter, Sepp, and Jurgen Schmidhuber. \"Long short-term memory.\" Neural computation 9.8 (1997): 1735-1780. Olah, Christopher. [\"Understanding LSTM Networks\"](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) Yan, Shi. [\"Understanding LSTM and its diagrams\"](https://medium.com/@shiyan/understanding-lstm-and-its-diagrams-37e2f46f1714) MathWorks, [\"Models for Predicting Remaining Useful Life\"](https://nl.mathworks.com/help/predmaint/ug/models-for-predicting-remaining-useful-life.html) Additional information This notebook is based on the 'Deep Learning for Predictive Maintenance' notebook by Fidan Boylu Uz. MIT License Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.","tags":"Remaining Useful Life Prediction","url":"/Remaining Useful Life Prediction/2020-01-01-interactive-notebook.html","loc":"/Remaining Useful Life Prediction/2020-01-01-interactive-notebook.html"},{"title":"Interactive notebook","text":"In [1]: % load_ext autoreload % autoreload 2 # from IPython.core.display import display, HTML # deprecated from IPython.display import display , HTML display ( HTML ( \" \" )) In [2]: import numpy as np import pandas as pd import matplotlib import warnings warnings . filterwarnings ( 'ignore' ) pd . set_option ( 'display.float_format' , ' {:.3f} ' . format ) from support import read_consumption_data , read_climate_data , printmd from visualisations import run_app , plot_effects_resampling , plot_temperature_power_one_year , plot_yearly_data , \\ plot_weekly_data , plot_correlation , plot_consumption_with_holidays_weekends , plot_auto_correlation import holidays In [3]: data = read_consumption_data () ext_data = read_climate_data () 2022-06-14 16:26:14,045 [INFO ] Generating /Users/hcab/Documents/projects/elucidatalab.github/elucidatalab.starterkits/data/SK_1_3/household_power_consumption.txt Downloading file /Users/hcab/Documents/projects/elucidatalab.github/elucidatalab.starterkits/data/SK_1_3/household_power_consumption.txt.zip: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19.7M/19.7M [00:01<00:00, 17.9MiB/s] 2022-06-14 16:26:17,601 [INFO ] Using file: /Users/hcab/Documents/projects/elucidatalab.github/elucidatalab.starterkits/data/SK_1_3/household_power_consumption.txt 2022-06-14 16:26:37,612 [INFO ] Generating /Users/hcab/Documents/projects/elucidatalab.github/elucidatalab.starterkits/data/SK_1_3/hourly_temperature.csv Downloading file /Users/hcab/Documents/projects/elucidatalab.github/elucidatalab.starterkits/data/SK_1_3/hourly_temperature.csv.zip: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 144k/144k [00:00<00:00, 4.20MiB/s] 2022-06-14 16:26:38,469 [INFO ] Using file: /Users/hcab/Documents/projects/elucidatalab.github/elucidatalab.starterkits/data/SK_1_3/hourly_temperature.csv Starter Kit 1.3: Resource Demand Forecasting Business context Resource demand forecasting concerns accurately predicting the future need for a resource, typically using historical information. It is one of the most essential steps in resource demand management, i.e. ensuring sufficient resources are available to satisfy a fluctuating demand. Resource demand forecasting allows one to plan ahead and guarantee that sufficient resources are available when needed and to avoid costly countermeasures in case of shortage. A typical example of resource demand forecasting is provided by the energy sector. For energy suppliers it is more beneficial to buy electricity on the day-ahead market than on the spot market. Consequently, the more accurate the energy consumption can be predicted, the lower the cost. Given that more and more houses and appliances are enabled with a smart meter, an increasing amount of data becomes available that can enable and improve this prediction. Furthermore, these kinds of predictions also allow energy suppliers to better balance demand and supply, and helps them to ensure proper grid operation (and e.g., prepare for possible peak loads in winter time). This Starter Kit will focus energy consumption forecasting as a use case for illustrating how resource demand forecasting using historical data can be realized. Business goal The business goal related to this Starter Kit is forecasting the demand for a particular resource at a particular point in the future based on usage data of that resource that is available from the past. More specifically, we will illustrate a data-driven approach for forecasting the energy consumption of households. Application contexts Forecasting the demand of a particular resource can be useful for several purposes in a variety of industrial contexts: Forecast parking demand or traffic density in a particular neighbourhood in order to control traffic in intelligent ways (e.g. divert to alternative routes or parking spots) Predict battery consumption based on operating conditions in order to intelligently suggest recharging moments or switch to low-power mode Estimate the usage of consumables (e.g., ink, paper) based on product usage data in order to deliver new consumables exactly on time before stock breakage or avoid expensive storage of superfluous stock (e.g. in a manufacturing environment) ... Data requirements In order to realize the business goal, we need a dataset (time series) that includes: a value indicating the demand for the resource at each moment in time. the factors that influence the demand of the resource. These can be internal or external, for example: the energy consumption of a building is influenced by the outside temperature, the amount of people present in the building, etc. the consumption of raw materials is influenced by the type of product that is being produced, the production line settings, etc. the traffic density is influenced by the time of day, the presence of road works, the weather, etc. 'representative' historical data. The amount of data that is needed to make an accurate prediction is typically determined by the length of the temporal patterns in the data. For example, if the data expresses a yearly seasonality, historical data of multiple years is needed. data sampled with the 'appropriate' frequency (i.e., how many times per minute/hour/day you gather data points). This frequency is related to the required forecast frequency, e.g. it is impossible to forecast every 10 minutes when only 15-minute data is available. In some use cases, where resource demand forecasting concerns particular processes, (expert) knowledge on these processes is required (i.e. as labeled data) Starter Kit outline In this Starter Kit, we will illustrate a data-driven approach for forecasting the energy consumption of a household. As a dataset we will use the energy consumption of the household and climate data collected by a nearby weather station. We will use this dataset to illustrate: How to prepare your data for the analysis. We will describe how to handle the typical problems of raw data and how to perform data fusion when multiple sources of data are available. How to get insights from your dataset. We will present visual and numerical techniques in order to explore your data for identifying interesting patterns. How to transform the obtained insights into useful features for building forecasting models. How to compare correctly the performance of multiple forecasting models. Data loading and preprocessing For the household energy consumption use case we consider here, two types of data are relevant: Household data, which refers to the total household energy consumption along with the consumption of the individual energy-consuming appliances Climate data, which contains information of climate factors (e.g. temperature) that can influence on the energy consumption of households In the following subsections, we will load these datasets and perform some basic preprocessing, e.g. correctly handling missing values and fusing both datasets. Household data As a dataset we use the public energy consumption data set from the UCI repository . This dataset contains measurements of electric power consumption in one household, located near Paris, with a one-minute sampling rate over a period of almost 4 years. Different electrical quantities and sub-metering values are available, as can be seen in the table below: Date_Time : the date and time of day of the measurement Global_active_power : household global minute-averaged active power (in kilowatt) Global_reactive_power : household global minute-averaged reactive power (in kilowatt) Voltage : minute-averaged voltage (in volt) Global_intensity : household global minute-averaged current intensity (in ampere) Sub_metering_1 : energy sub-metering No. 1 (in watt-hour of active energy). It corresponds to the kitchen, containing mainly a dishwasher, an oven and a microwave (hot plates are not electric but gas powered). Sub_metering_2 : energy sub-metering No. 2 (in watt-hour of active energy). It corresponds to the laundry room, containing a washing-machine, a tumble-drier, a refrigerator and a light. Sub_metering_3 : energy sub-metering No. 3 (in watt-hour of active energy). It corresponds to an electric water-heater and an air-conditioner. In [4]: start_date = data . index . min () . date () end_date = data . index . max () . date () printmd ( f \"The dataset contains data from { start_date } to { end_date } .\" ) data . head () The dataset contains data from 2007-01-01 to 2010-11-26. Out[4]: Global_active_power Global_reactive_power Voltage Global_intensity Sub_metering_1 Sub_metering_2 Sub_metering_3 Date_Time 2007-01-01 00:00:00 2.580 0.136 241.970 10.600 0.000 0.000 0.000 2007-01-01 00:01:00 2.552 0.100 241.750 10.400 0.000 0.000 0.000 2007-01-01 00:02:00 2.550 0.100 241.640 10.400 0.000 0.000 0.000 2007-01-01 00:03:00 2.550 0.100 241.710 10.400 0.000 0.000 0.000 2007-01-01 00:04:00 2.554 0.100 241.980 10.400 0.000 0.000 0.000 From the table below, we can see that for all sensors we have 2.049.280 instances. This already indicates that there are no missing values. Besides this information, we do not spot immediately any strange insights, e.g. an anomaly in sensor data like negative consumed power. In [5]: data . describe () Out[5]: Global_active_power Global_reactive_power Voltage Global_intensity Sub_metering_1 Sub_metering_2 Sub_metering_3 count 2027288.000 2027288.000 2027288.000 2027288.000 2027288.000 2027288.000 2027288.000 mean 1.083 0.124 240.833 4.591 1.121 1.289 6.448 std 1.049 0.113 3.231 4.411 6.147 5.786 8.434 min 0.076 0.000 223.200 0.200 0.000 0.000 0.000 25% 0.308 0.048 238.990 1.400 0.000 0.000 0.000 50% 0.594 0.100 241.000 2.600 0.000 0.000 1.000 75% 1.520 0.194 242.870 6.400 0.000 1.000 17.000 max 11.122 1.390 254.150 48.400 88.000 80.000 31.000 For the purpose of this Starter Kit, only the Date_Time and the Global_active_power attributes will be considered from now onwards. In [6]: data = data [[ 'Global_active_power' ]] Climate data As an additional dataset, we use the climate information recorded close to the city of Paris, where the household is located, provided by the Wunderground website. It contains information on climate conditions for every 30 minutes, via the following attributes shown in the table below: time_UTC : the date and hour of the measurement temperature : temperature (degrees Celsius) dew_point : dew point (degrees Celsius) humidity : air humidity (percentage) sea_lvl_pressure : pressure at sea level (hPa) visibility : visibility (km) wind_dir_degrees : wind direction expressed in degrees (degrees) In [7]: ext_data . head ( 5 ) Out[7]: datetime 2007-01-01 00:00:00 14.210 2007-01-01 01:00:00 14.110 2007-01-01 02:00:00 13.910 2007-01-01 03:00:00 13.010 2007-01-01 04:00:00 11.910 Name: temperature, dtype: float64 From the statistics in the table below, we can see the data contains minimum values of -9999 for some variables. According to the documentation , values of -9999 or -999 signify missing values. We will replace these with the proper missing value NaN, as otherwise our algorithms would get confused. In [8]: ext_data . describe () Out[8]: count 34164.000 mean 12.407 std 13.868 min -8.710 25% 7.210 50% 12.310 75% 17.310 max 999.990 Name: temperature, dtype: float64 In [9]: ext_data . replace ( to_replace =- 9999 , value = np . nan , inplace = True ) ext_data . describe () Out[9]: count 34164.000 mean 12.407 std 13.868 min -8.710 25% 7.210 50% 12.310 75% 17.310 max 999.990 Name: temperature, dtype: float64 We can now notice that the temperature values seem to be more plausible. As expected, we also see that not all sensors have the same number of instances, meaning there are missing values. For the purpose of this Starter Kit, only the time_UTC and temperature attributes will be considered from now on. Data fusion The evolution of household energy consumption may be influenced by climatological factors, e.g. lower or higher temperatures can lead to a higher energy consumption for some appliances such as water-heaters and air-conditioners. To understand and analyze this relationship, we need to fuse the household data with the climate data. However, these datasets have different sampling time: the household data is sampled every minute, whereas the climate data is sampled every 30 minutes. The integration of these 2 datasets thus requires careful consideration. Two options are possible: we can either upsample the climate data or downsample the household data. We select the second option, as otherwise, we would need to find a way to impute missing data in the climate dataset, and it allows us to reduce the size of the dataset, which will reduce compute time required for the analysis. The effect of downsampling the household data can be seen below. The user can experiment how the window sizes of 30 minutes, 1 hour, or 4 hours affect the evolution of the global active power. In [10]: plot_effects_resampling ( data ) As we can see from the plot, resampling reduces the (small) peaks of the original signal. The effect becomes more pronounced for larger window sizes. A window size of 4 hours arguably removes too much variation. A window of 30 minutes or 1 hours on the other hand still captures the general evolution of the active power, which is exactly what is needed for characterizing the typical household consumption. Between 30 or 60min time windows, we decide to use a window size of 60min since this reduces the dataset and speeds up the computation time required later on for the analysis. To downsample the climate dataset, the mean of the values of the respective hour is used. To downsample the household dataset, the sum of each of the values for the corresponding hour is taken An excerpt of the downsampling of household dataset is shown in the table below. In [11]: ext_data_h = ext_data . resample ( '1H' ) . apply ( np . mean ) data_h = data . resample ( '1H' ) . agg ({ 'Global_active_power' : np . sum , }) data_h [: 3 ] Out[11]: Global_active_power Date_Time 2007-01-01 00:00:00 153.038 2007-01-01 01:00:00 151.404 2007-01-01 02:00:00 154.940 Since Global_reactive_power and Global_active_power are expressed in minute-averaged kW, we will change their units to Wh over an hour, such that this is in line with the resampling frequency. To this end, the following transformations are applied (the summing was already done in the previous step): \\begin{align*} \\mathit{globalactive}\\ Wh &= \\sum&#94;{60}_{min=1} 1000\\cdot\\mathit{globalactivepower}_{\\mathit{min}}\\ kW\\cdot \\frac{1}{60}\\ h\\\\ \\end{align*} The result of this transformation can be seen in the table below. In [12]: data_h [ 'Global_active_power' ] = data_h [ 'Global_active_power' ] . apply ( lambda x : x * 1000 / 60 ) data_h [: 3 ] Out[12]: Global_active_power Date_Time 2007-01-01 00:00:00 2550.633 2007-01-01 01:00:00 2523.400 2007-01-01 02:00:00 2582.333 Now we can finally merge the two datasets into an extended dataset that will be explored in the next section. In [13]: new_data = pd . concat (( data_h , ext_data_h ), axis = 1 , join = 'inner' ) new_data [: 3 ] Out[13]: Global_active_power temperature 2007-01-01 00:00:00 2550.633 14.210 2007-01-01 01:00:00 2523.400 14.110 2007-01-01 02:00:00 2582.333 13.910 Before proceeding, we inspect to what extent this extended dataset contains missing data. In [14]: new_data . isna () . mean () Out[14]: Global_active_power 0.000 temperature 0.025 dtype: float64 In case of too many missing values, we could consider data imputation. However, in this case, only the temperature has 0.01% of msising values, so there is no need for imputation. Data Exploration In this section, we explore the extended dataset in order to understand which factors influence the household energy consumption. Based on this exploration, we will later on identify which features are useful for training a machine learning algorithm to forecast energy consumption. There are several approaches to data exploration. In this section we will use: visual exploration by means of time plots statistical data exploration Visual Data Exploration Time plots are a common way for exploring time series data, such as the energy consumption that we consider here. We explore time plots in different time frames in order to highlight different aspects in the data: we start with a yearly plot, which allows us to identify global patterns. a monthly plot allows us to zoom in and understand relationships between energy consumption in different weeks and days a weekly plot allows us to zoom in even further and identify daily and hourly patterns Yearly patterns It is expected that energy consumption depends on the outdoor temperature, since in winter time heaters are used more frequently than in summer time for example. For this reason, we plot the evolution of the energy consumption along with the temperature. In the plot below, we resample the hourly data of both datasets to daily data, so that we don't get lost in details but can observe more global patterns. The user can toggle on or off the normalization of the data. The normalization that is applied is called min-max normalization and is explained in detail in SK 3.4. It applies the following formulate to the data in order to make sure all values are contained within the [0, 1] range: \\begin{align*} \\mathit{X_{scaled}} = \\frac{X - X_{min}}{X_{max} - {X_{min}}}\\\\ \\end{align*} The user is invited to verify, without selecting the option normalization, whether of temperature and global active power have the same trend. The reader can then repeat the analysis using the option normalization. In [15]: plot_temperature_power_one_year ( new_data ) It should be clear that without normalization, it is difficult to compare the temperature with the global active power because both use different ranges, and the range of the global active power dominates the range of the temperature. With normalization however, the plot above reveals interesting insights. Both the temperature and the global active power follow a seasonal trend. Obviously, the temperature is lower in winter and higher in summer, whereas the reverse is true for the global active power. This is to be expected: energy consumption is higher during winter time. In more formal terms, we can observe that the temperate and global active power exhibit an inverse correlation: when one increases the other decreases, and vice versa. Monthly patterns Let's now see if there is a monthly recurring pattern as well. In the following figure, the active power of the selected year(s) is shown. The user can change the resampling rate to make the insights more clear to them. To make the plot more clear, the user can (de)selected the years they want on the right of the figure. In [16]: plot_yearly_data ( new_data ) While we can again clearly see the seasonal effect, there is no clear pattern recurring every month. One interesting thing that can be seen is that strong dips in power consumption tend to happen in the same time in February and November in 2007 and 2008. Most likely, these are holidays which tend to happen at the same time for a fixed household. However, these dips are not present at the same time for 2009. Weekly patterns In this section we further explore the evolution of the energy consumption in a more fine grained time frame. We want to verify whether during a week, the energy consumption exhibits the same patterns. In the plot below, the user can inspect and compare the energy consumption in December of different years. We mark weekends in yellow and Christmas day in orange. We invite the user to see for each year if they can notice a difference between week days and weekends and if there are any changes around the Holidays. In [17]: plot_consumption_with_holidays_weekends ( new_data ) What should be obvious from exploring this plot is the existence of a daily pattern for energy consumption dependent on the day of the week: in most weekend days of the three years, we can observe slightly higher energy consumption than during weekdays. This can depend on the common habits of working at the office or children being at school. The effect of the holidays can also be seen. On Christmas day in 2007, the power consumption is slightly higher than other typical weekdays, but similar values can be seen e.g. exactly one week before. So the holidays don't have a strong impact here. In 2008 on the other hand, the power consumption is lower, indicating that perhaps the household celebrated outside their house. In 2009, there is a peak in consumption on Christmas eve. We can conclude that whether the day is a weekday or in the weekend or a holiday has an influence on the expected power consumption. Daily patterns Next to yearly and monthly patterns, we are also interested in daily patterns. Below, we plot the evolution of the global active power for one week (The first week of December 2009). We can see how the power consumption follows a daily recurring pattern, with peaks in the morning and evening and troughs at night and midday. In fact, this pattern is also already present in the previous plot, where daily morning and evening peaks can also be observed. During the night and the day, we still see low amounts of power usage, most likely due to appliances like refrigerators and heating. In [18]: plot_weekly_data ( new_data ) Statistical Data Exploration The visualizations above highlighted interesting patterns in the energy consumption of the household that can be exploited for feature extraction. In this section we use statistical methods to verify, and eventually quantify those observations. Autocorrelation In order to find repetitive patterns in time series data, we can use autocorrelation , which provides the correlation between the time series and a delayed copy of itself. In order to investigate this autocorrelation, the plot below visualizes the autocorrelation of the global active power with time lags of 1 day, 2 days, etc. Note that we do not show the results of a delay of 0 days, since the correlation with the exact same day would trivially be 1. In [19]: plot_auto_correlation ( new_data ) The plot above reveals that a clear peak occurs after 1 day, indicating the consumption of a day is highly correlated with the consumption of the previous day another clear peak occurs every 7 days, which confirms the existence of a weekly pattern, i.e. the consumption of a day is highly correlated with the consumption of the same day a week earlier Correlation As visually observed above, the temperature influences the energy consumption of a household. In order to verify the strength of this relationship, we can compute the correlation between these two variables. For this purpose, we compute Pearson's correlation. The plot below shows again both temperature and consumption and allows to investigate their relationship. This relationship is influenced by the frequency of the dataset, e.g. temperature changes are typically less drastic than energy consumption changes. We invite the user to experiment with the different resampling periods, to understand how that changes the correlation and what frequency leads to the highest correlation. In [20]: plot_correlation ( new_data ) What should be obvious from the above is that there is a negative correlation between temperature and energy consumption, meaning the consumption rises when temperature drops and vice versa. The larger the resampling window is chosen, the higher is the correlation. This is because with larger windows, we smooth out more the short-term fluctuations, leaving only the seasonal pattern described above. Feature engineering In this section, we will extract several features from the extended dataset that can be used for forecasting energy consumption. A feature is an individual measurable property of the phenomenon being observed. Choosing informative, discriminating and independent features is a crucial step for building effective machine learning algorithms. Based on the previous visual and statistical observation, we will consider the following features to predict the energy consumption for the hour ahead: energy consumption of the day before energy consumption of the 7 days before the hour of the day, which will allow to distinguish between different periods of the day (i.e. night, morning, afternoon, evening) the day of the week, which will allow to distinguish between weekdays and weekend days whether the day is a holiday or not the month of the year, which will allow to distinguish between yearly seasons the temperature In [21]: new_feats = new_data [[ 'Global_active_power' ]] . rename ( columns = ({ 'Global_active_power' : 'consumption' })) new_feats [ 'Pastday' ] = new_feats [ 'consumption' ] . shift ( 1 , freq = 'D' ) new_feats [ 'Pastweek' ] = new_feats [ 'consumption' ] . shift ( 7 , freq = 'D' ) new_feats [ 'Hour' ] = new_feats . index . hour new_feats [ 'Weekday' ] = new_feats . index . dayofweek new_feats [ 'Month' ] = new_feats . index . month new_feats [ 'Holiday' ] = [ 1 if ( date in holidays . France ()) else 0 for date in new_feats . index ] new_feats [ 'Temperature' ] = new_data [ 'temperature' ] new_feats = new_feats . dropna () Modeling This section allows the user to discover the most important factors for training a machine learning model. More specifically, the user will get insights on the influence of the training strategy , type of machine learning model and effect of data normalization on model performance. To evaluate the performance of the models the mean absolute error (MAE) is used (a metric commonly used in literature for this purpose). This metric quantifies to what extent the model forecasts are close to the real values. As the name suggests, the mean absolute error is an average of the absolute errors. It holds that the lower the MAE of the model, the better its performance. This section ends with an interactive application where the user can test all options in order to find the one that leads the model to achieve the best performance. Training strategies The training data influences models performances. For this experiment, the following options are available: Use 1 month before the test month as training data. In this experiment, several models are trained for the predictions of each individual month. Each model is trained on one month before the month we are making predictions for. One month gap is introduced between the training and the test month (this is done to avoid that the last day of the training set is also included in the first day of the test set). For example, the models that will make the predictions for April 2008 will be trained on the data from February 2008. Use 6 months before the test month as training data. Similar to the above experiment, but with 6 months of training data. It still includes a 1 month gap. Use 1 year before the test month as training data. Similar to the above experiment, but with 1 year of training data, still include a 1 month gap. Use 1 month the year before as training data. Similar to strategy number 1, but with an 11 months gap. This way, the training data and test data are taken from the same month, one year apart. Use all months before the test month as training data. For each test month, train a model using all the data prior to this (including a 1 month gap). This strategy simulates the scenario where the model is retrained as new data comes in. Since this requires training several models based on potentially large training sets, this might require a certain computational time. Use train-test split . Train on the first year of data and make predictions on the rest. This strategy is commonly used for training these kinds of models. Notice that for the benchmark model, there is no training phase. Machine learning models There is a plethora of models that can be used for forecasting. In this notebook, we decide to use two models that are commonly used in literature for the task at hand: Random Forest Regression (RFR) and Support Vector Regression (SVR) . Besides these models, we will also use a simple benchmark model that we use to compare the performance of the other models against. For the prediction of the energy of a certain day, this benchmark model simply takes the energy consumption of the previous day at the same time. We will use these models to forecast the energy consumption based on the past energy measurements (see training strategies). Normalization Before being used for training, data may need to be normalized, i.e. rescaled so that all input and output values are between 0 and 1. This is indeed a requirement for the correct training of some machine learning models. It is left to the user the exercise of discovering which model requires the normalization of the data. In [22]: from IPython.utils import io PORT = '8095' # in AWS use ports 8050-8060 mode = 'external' #external if mode == 'inline' : run_app ( new_feats , mode = 'inline' , port = PORT ) else : with io . capture_output () as captured : run_app ( new_feats , mode = 'external' , port = PORT , host = '0.0.0.0' ) HOST = '0.0.0.0' # in AWS replace here by public IP print ( f 'Dash app running on http:// { HOST } : { PORT } ' ) Dash app running on http://0.0.0.0:8095 The mean absolute error of each experiment that was run is reported in the table above. Insights from the experiments These are the basic insights the user could have noticed when trying different models, parameters and training strategies. Effect of the training strategy For the SVR, we see that the standard train-test split has the best performance, although using all months and using a one year window before the test month as training data both have a similar performance. For the RFR on the other hand, we clearly see that using a one year window before the test month as training data leads to better performance than the other training strategies. We note that for both this \"one year window\" strategy and the standard train-test split, the training data is one year. The difference however, is that for the former the training set changes, while for the latter it is fixed (using all the data from 2017). A general trend that can be seen is that the performance increases as a larger training set is used. We therefore urge the user to look at the predictions made using the training strategy where all months before the test month are used as training data: we can expect that the predictions for the later months will be better than for the first months since more training data was used. However, this is hard to see by eye in the prediction plot directly. Effect of the chosen model We can see that, for most parameter choices, both the SVR and RFR can outperform the simple benchmark model we set up. The RFR outperforms the SVR for most (if not all) parameter choices and training strategies. This shows the importance of knowing which model is best suited for your problem and testing different ones. Effect of the chosen parameters The search for the optimal set of hyperparameters for your machine learning model is a non-trivial task, since there might be a strong interplay between the parameters so we can not do the optimal parameter search one by one. Here, we let the user try a pre-set number of choices, but more automatic and smart search algorithms exist to tackle this problem. We found that for the RFR, the higher the number of estimators, the better the results, but at a greater computational cost and with diminishing returns. We see that 50 estimators and leaving the max depth at 10 is a good choice. For the SVR, $C=10$ and $\\gamma=0.01$ work well. Effect of normalization Normalizing the data, rescaling it so that the input and output variables all have values within a similar range (in this case, between 0 and 1) is a common step when setting up machine learning models. This is because, depending on the model, they work much better with values of the order 1. We see that normalization indeed greatly improves the predictions for the SVR, but has very little influence on the RFR. Indeed, algorithms such as random forest and decision tree are not influenced by the scale of the input variables. Conclusion In this Starter Kit we demonstrated forecasting the demand for a particular resource at a particular point in the future, illustrated in the case of energy demand forecasting. To this end, we first analysed the business problem, preprocessed the data, visually and numerically explored it to eventually illustrate the use of different models ( Random Forest Regression, Support Vector Regression and benchmark) to perform the actual forecasting with different amounts of available historical data. From the analysis, it may be clear that achieving good prediction results depend on different factors (appropriate data preprocessing, the amount of available training data, algorithm parametrisation, etc.). Additional Information Contributions by Mathias Verbeke, Wannes Meert, Vincent Vercruyssen, Alessandro Murgia, Tom Tourw√©, and Robbert Verbeke. This Starter Kit was developed in the context of the EluciDATA project ( http://www.elucidata.be ). For more information, please contact info@elucidata.be .","tags":"Resource demand Forecasting","url":"/Resource demand Forecasting/2020-01-01-interactive-notebook.html","loc":"/Resource demand Forecasting/2020-01-01-interactive-notebook.html"}]};