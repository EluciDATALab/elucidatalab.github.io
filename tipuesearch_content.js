var tipuesearch = {"pages":[{"title":"    EluciDATA Lab\n","text":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    EluciDATA Lab\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\nHome\n\n\n\n\nBlog\n\n\n\n\nStarter Kits\n\n\n\n\nCommunity\n\n\n\n\nAbout\n\n\n\n\n\n\n\nSearch\n\n\n\n\n\n\n\n\n\n\n\n\n\n EluciDATA Lab\nWe are the Data and AI Competence lab of Sirris.\nRead More\n\n\n\n\n\nAI Starter Kit Platform\nDiscover our interactive platform to extend your AI and Data Science skills for several relevant industrial use cases!\nRead More\n\n\n\n\n\nBlog posts\n \"Stay up to date with developments in our projects and of new emerging AI technologies!\nRead More\n\n\n\n\n\nPrevious\n\n\n\nNext\n\n\n\n\nOur approach to AI adoption in industrial contexts\n\n\n\nThe Starter Kits we have developed generalize and distil the AI needs of specific industrial use cases, which provide a source of inspiration.\nConcretely the Starter Kits are self-contained collections of autodidactic material, providing a description of a specific data innovation topic in terms of its business goal, data-related requirements & challenges, relevant data science tasks, etc. These also contain a documented proof-of-concept solution, using public datasets, illustrating which machine learning methods to use and how they should be combined.\n\n\nAI Starter Kits experience\n\n\n\n\n\nAI Starter Kits with complementary video tutorials\n... to guide you through the different steps AI-based methodology for several industrial use cases, complemented with autodidactic video tutorials.\n\n\n\n\n\nInteractive Experience\n... where you can play with model parameters and visualization settings and explore all possible angles.\n\n\n\n\n\nBring Your Own Data workshops\n... in which you are coached to apply the methodology solutions to your own data & problem setting.\nRead More\n\n\n\n\nExplore Starter Kits\n\n\n\n\n\n\n\n\nABOUT EluciDATA Lab\n\n\nelucidatalab@sirris.be\n\n\n\n\n\nHOME\nABOUT\nARCHIVE\n\n\n© Sirris User Agreement Privacy policyEluciDATA Lab -The Data and AI Competence Lab of Sirris\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":"","url":"index.html"},{"title":"Best Industry Paper Award at IEEE Conference on Prognostics and Health Management","text":"A team of EluciDATA Lab researchers wins Best Industry Paper Award at IEEE Conference on Prognostics and Health Management (Montreal, Québec, June 5 - June 7, 2023) Our award-winning paper, titled \"Multi-view Contextual Performance Profiling in Rotating Machinery,\" introduces an innovative unsupervised approach for asset performance profiling. As part of AI-ICON CONSCIOUS project, the proposed methodology addresses the challenge of accurately estimating an asset's performance by considering the varying contexts to which industrial assets are exposed during operation. The approach was validated on a real-world dataset of feedwater pumps. It emphasizes the importance of understanding the factors that shape these contexts and account for them during asset condition monitoring and profiling. The proposed methodology leverages a multi-view representation and matrix decomposition to derive specific vibration fingerprints characterizing the asset's behavior in a context-sensitive fashion. The approach processes data in two separate views: the process view and the vibration view. In the process view , variables related to the asset's internal workings are analyzed and partitioned, with each measurement point associated with a specific label representing the operating modes of the asset. On the other hand, the vibration view focuses on extracting vibration profiles through non-negative matrix decomposition. By linking these two views together, the researchers can derive characteristic fingerprints using a suitable contextual representation and performance-related indicators. In collaboration with I-Care and Engie the methodology was validated using real-world industrial data from feedwater pumps, comprising vibration and operational sensor measurements. The results obtained from the validation demonstrate the efficacy of the profiling methodology in delivering meaningful risk assessment estimations associated with different operating contexts. This award can be seen as an affirmation of our day-to-day work at the EluciDATA Lab to bridge the gap between AI research and practical applications for the industry.","tags":"Blog","url":"/Blog/20230626_CONSCIOUS_PHMC_Award.html","loc":"/Blog/20230626_CONSCIOUS_PHMC_Award.html"},{"title":"Collaboration with Materialise and Cr3do selected for the ITEA Award of Excellence for Innovation","text":"https://www.piqsels.com/en/public-domain-photo-zmdsp Joint collaboration with Materialise and Cr3do selected for the ITEA Award of Excellence 2023 for Innovation Although the additive manufacturing (AM) technology has evolved substantially over the last decades, the overall end to-end process involves many complex steps and still today requires manual input from an engineer at crucial points. We have been collaborating with Materialise and Cr3do to create an AM knowledge base that can assist in all major AM steps. The success of this collaboration has led to the project being selected for the ITEA Award of Excellence 2023 for Innovation Materialise is the largest company active in 3D-printing world-wide. It was founded in 1990 in Leuven, Belgium and has grown to a company with more than 2.100 employees world-wide, with strong international presence in software, industrial and medical applications with AM. Materialise has the largest group of software developers for AM and with more than 250 high-end industrial printers, is the largest service provider of AM parts in Europe. Cr3do is an SME specializing in generating personalized solutions for customers by using modern, smart manufacturing techniques such as AM and laser cutting, to lower the threshold for other SMEs to get started with these technologies. The company provides expertise, 3D modeling services and production of their solutions. Most customers are active in the architectural and residential development sector, for which Cr3do creates high-end 3D models and 3D visuals. Although the AM technology has evolved substantially over the last decades, the overall end-to-end process involves many complex steps that still today requires manual input from an engineer at crucial points. We have been collaborating with Materialise and Cr3do in the context of the ITEA3-SAMUEL project to combine an AM engineer's experience with data-driven machine-learning methods to create an AM knowledge base that can assist in all major AM steps. The focus of the Belgian partners is on use cases related to build-time estimation (BTE) and build preparation (e.g. part orientation and nesting). Data-driven build-time estimation Accurate estimation of build times of 3D objects is of great importance in the different phases of an AM process including: 1) during quotation, an estimation of build time is needed since it impacts the printing cost of a part significantly, 2) during build preparation, part orientation influences build time significantly, hence information on build time can be exploited to find the optimal part orientation and, 3) at planning level, information on build time allows for optimal planning, maximizing machine utilization and minimizing total production time. The existing physics-based models can very precisely tackle this task but at the cost of spending a considerable amount of time. An alternative solution is to use a data-driven machine learning method for BTE. However, estimating build times of a dataset of objects with diverse and heterogeneous characteristics is a challenging task for a single learning algorithm. (left) Examples of the modeled 3D objects with diverse characteristics, (right) complex and non-linear relations between the objects' volumes and their build times We therefore investigated the value of the ‘divide-and-conquer' strategy in partitioning the dataset into subsets of homogeneous objects to facilitate the BTE task for the examined learning models. The usefulness of this strategy has been proven through performing comprehensive experiments. We proposed two different divide-and-conquer algorithms and the experimental results showed that both methods are indeed capable of providing accurate BTEs and can outperform the performance of a single learning method trained with all objects in the dataset. More specifically, the proposed algorithms yielded a mean relative estimation error below 10% for all objects and below 5% for around 40% of the objects in the dataset. The proposed data-driven divide-and-conquer strategies and the obtained results were published in 2021 International Conference on Data Mining Workshops (ICDMW) (DOI: 10.1109/ICDMW53433.2021.00041).","tags":"Blog","url":"/Blog/20230626_SAMUEL_ITEA_Excellence_Award.html","loc":"/Blog/20230626_SAMUEL_ITEA_Excellence_Award.html"},{"title":"Introduction","text":"Introduction Description Most data mining and machine learning algorithms do not work well if you just feed them your raw data: such data often contains noise and the most relevant distinguishing information is often implicit. For example, raw sensor data just contains a timestamp and a corresponding value, while interesting aspects of it might be the trends contained within or the number of times a threshold is exceeded. Feature engineering is the process of extracting from the raw data the most relevant distinguishing characteristics that will be presented to the algorithm during modeling. It is a way of deriving new information from the existing data, so that its characteristics are more explicitly represented. The resulting features are eventually feed to the AI/ML algorithm of the model. In practice, the feature engineering step is achieved by the selection of the most appropriate parameters, or the composition of new features by manipulation, transformation and combination of the raw data. Feature engineering is one of the most important and creative steps in the data science workflow. However, there is no clearly-defined formal process for engineering features and, consequently, this requires a lot of creativity, a good understanding of the domain and of the available data, some trial-and-error, etc. It is also desirable to have upfront an idea of the modeling and analysis task for which you want to use the resulting features, as this might help you to identify relevant features. Business goal The overall goal of this Starter Kit is to present some advanced feature engineering steps related to feature construction and extraction . By keeping in mind what is your business question and what is the corresponding data science task , you will be able to derive valuable features that can be used in the next stage of your analysis. Application context Feature engineering is one of the steps in the data science workflow with the most decisive impact on the accuracy of the model you want to develop. It is the final step before actually training the model and it defines how the input data will be fed to the model. Starter Kit outline In this Starter Kit we use a dataset from the Regional Rail System of Pennsylvania. Before starting the feature extraction per se , we will first apply some basic preprocessing to the dataset and have a high-level overview of the data. We will then derive several interesting features from the dataset that can be used to characterise train rides and delays.","tags":"Feature Engineering","url":"/Feature Engineering/2023-04-01-introduction.html","loc":"/Feature Engineering/2023-04-01-introduction.html"},{"title":"Key Take Away Messages","text":"Key Take Away Messages Congratulations! You completed the tutorial accompanying the AI Starter Kit on data exploration. In this Starter Kit we have demonstrated how appropriate statistical and visualisation techniques can help a data scientist in discovering interesting patterns and gathering insights without applying any complex algorithm. We demonstrated how to explore a dataset by starting from a univariate analysis by means of which we could already get a clear view on who Pronto's customers are with respect to age and gender, where they like to go, and for how long the single bikes are usually in use. By subsequently adding more information and combining different variables, we were able to reveal relationships between gender and most frequently visited stations but also how the bikes are used for different purposes, like a strong commuter's pattern during the week and recreational usage on the weekends. Finally, via a multivariate analysis, we figured out how the elevation influences the biking behaviour. By introducing a cost to the single trips, we could explain why at some stations more bike trips are started while at other stations, more bike trips end. By applying comparably easy visualisations sometimes more insights can be gathered than by applying a sophisticated machine learning model. We thank you for completing this video series and hope to welcome you in another AI Starter Kit tutorial.","tags":"Data Exploration","url":"/Data Exploration/2022-09-08-key-take-away-messages.html","loc":"/Data Exploration/2022-09-08-key-take-away-messages.html"},{"title":"Bivariate Analysis","text":"Bivariate Analysis Welcome to the tutorial video on bivariate analysis of the AI Starter Kit on data exploration. In this video we explore the relationships between the variables analysed in the previous videos. Our main focus is on the factors that may have an influence on the number of trips and their duration. As a starting point, let's inspect how bikes are used through the different days of the week. For this, we have a look at a bar plot where the bars represent the number of trips performed on the different days of the week. On top of the bar plot, we plot a line that depicts the median duration of trips for which the scale is given on the right-hand side. The bar plot shows that there are slightly less trips in the weekend, but that they tend to last longer than during weekdays. Across the single weekdays, the number of trips is similar. From this, we can draw the following hypothesis: On the one hand, we have leisure bikers who use the bike recreationally essentially during the weekend. On the other hand, we have weekday users who use the bike for commuting to work with comparably short trips of around 10 minutes. Let's try to validate the previous hypothesis by looking at the number of trips per hour, distinguishing between weekdays and weekends. In this figure, we see the number of trips for weekdays on the left-hand side and for weekends on the right-hand side. The angular position in the circles represents the hour of the day while the blue bars show the number of trips for each hour. As weekends consist of two days only compared to 5 weekdays, these figures are misleading. To have a clear visual comparison of the trips between weekdays and weekends, we need to normalize the data such that we have the number of trips per day. With this, we can avoid that wrong conclusions are drawn. Now that the data is normalised, we clearly see two different patterns on weekdays and weekends: during weekdays, there are two peaks in the number of trips, occurring during the morning and evening rush hours while on weekends the distribution is much more uniform throughout the day or at least throughout the period comprised between the late morning and the early evening. These patterns support the hypothesis of recreational bike trips at the weekend compared to work- or school-related trips during weekdays. We can further inspect the rush hours during weekdays to understand whether the rental locations share some similarities. As a first step, we divide the trips in two groups. On the one hand morning rush hours for which we select trips done on weekdays during 7 and 10 in the morning and on the other hand, evening rush hours with trips done on weekdays during 4 in the afternoon and 7 in the evening. Furthermore, for both groups we only retain the top 10 stations of arrival. The resulting data is shown in this bar plot with the morning rush hour on the right- and the evening hours on the left-hand side. The y-axis describes how many times a given station was reached. A dashed horizontal line at 1400 is drawn to ease the comparison between the bar plots. By comparing these bar plots, we discover that there are more stations of arrival that count more 1400 visits in morning rush hours than in the evening rush hours. This may indicate that bikers tend to converge in the morning rush hours toward a limited number of stations, like for example close to working places or schools, whereas in the evening rush hours bikers go in different directions like different residential areas. This hypothesis can also justify why stations that are quite popular in the morning rush hours such as Republican Street & Westlake Avenue North or 9th Avenue North & Mercer Street do not appear in the top ten stations of arrival in the evening rush hours and vice versa for Dexter Avenue North & Aloha Street or 1st Avenue & Marion Street. To further compare the difference between stations of arrival in the morning and evening hours, we plot them on the map of Seattle. The green circles indicate stations that were reached in the morning rush hours while the red circles indicate those stations that were reached in the evening rush hours. The size of each circle is proportional to the number of times that the station was reached. What can we learn from this? First of all, in the area of South Lake Union - just below Lake Union - there are green stations but no red stations. This may mean that this location is more commonly reached by working bikers. Further, the green ones seem to be more 'condensed' in the downtown area when compared to the red stations. This may be due to the fact that red stations are reached after working hours. Part of the bikers may decide from 4 to 6 in the afternoon to go back home in more residential areas. Interestingly, none of the top 10 stations is located in the University district. This can have different reasons: Most obviously, students don't bike to university, maybe because they live close by. Another reason might be that lecture hours are not exactly the same as working hours and finally, the number of non-students is simply bigger than the number of students. Another influence on the biking behaviour is surely the weather. Even though, we don't have the weather explicitly given in this tutorial, we know that the chance of rain and cold temperatures in a state like Washington is much higher in the winter months compared to the summer months. For this reason, it is enough to plot the number of trips against the months. The resulting pattern is as expected: More than double as many bikes are used in summer compared to winter. The most-frequented month is July. Interestingly, the mean trip duration given by the black line is more or less constant during the year. Let's see whether on the other hand the trip duration is influenced by the age of a biker. For this purpose, we visualise the joint distribution of these two variables by means of a scatterplot and compute their Spearman's correlation. This correlation is chosen to measure the monotonic relationship between the variables. The Spearman's correlation is chosen in place of the Pearson's correlation since the latter is not robust to outliers. As we can see, there is a broad age dispersion over a relatively narrow distribution of trip durations. Longer-lasting trips occur for many ages. Not surprisingly, the Spearman's correlation is close to zero, hence the age does not seem to play an important role in the duration of trips. Note that driving speed might also play a role, but as we only know where each trip started and ended, we cannot estimate the speed, and hence no further analysis can be done. Another factor that could influence the trip duration is gender. As we can see, women bike longer with an average duration of about 12 minutes per trip compared to men with an average duration close to 10 minutes per trip. The fact that mean duration of the Unknown group is much longer - namely 36 minutes – suggests that short-term users do not use the bikes for commuting, but rather for leisure purposes. Additionally, we can have a look at the relationship between gender and age. In the univariate analysis, we highlighted a peak at 28 years. In the present analysis, we can further investigate it by seeing whether or not it is consistent across genders. We can see that the age distribution is relatively similar between men and women, with a more prominent bike usage around 30 years compared to younger and older ages. The peak at 28 years only clearly stands out for men and Other and remains difficult to interpret. Another relation that can support our hypothesis on commuters is given by the weekday of the trip and the type of user. Bike users can be distinguished based on the fact that they have an Annual Membership or a Short-Term Pass. We notice that Short-Term Pass users are more frequently riding a bike on weekends and much less on weekdays, as opposed to annual members. This suggests that Pronto's choice of offering two types of subscriptions meets the needs of these two types of users. This supports the hypothesis that leisure bikers may often subscribe to a Short-Term Pass whereas bikers with an annual membership are probably commuters that need a bike for commuting to work. From the univariate analysis we know that the bike-sharing service is more often used by men than by women. Here we explore how this usage is over the days of the week. In the plot we see that there is no obvious difference in the gender ratio. The most noteworthy difference is the increase of the Unknown gender category over the weekend. As we already saw, this can be explained by the fact that this group represents Short-Term Pass holders, which travel mostly on weekends. It is possible that there is a relationship between a biker's gender and the location where the biker is going. For this, we analyse whether there exist stations of arrival that are more popular for one gender than for the others. To this end, we split the trips according to gender. For each group, we compute how frequently each station was reached. Finally, for each station, we compute the ratio between the frequency of arrival for men and women. More specifically, we define the two following ratios: the ratio between the visit frequency for women compared to men and the inverse, namely, the ratio between the visit frequency for men compared to women. This bar plot reports the top 5 and bottom 5 stations with the highest difference in ratio. If we focus our attention on the dashed line we can see that the top 5 stations for women are at least twice as popular among women than among men. This popularity is quite striking especially for the first two stations on the left. Similar considerations are valid for the bottom 5 stations that are more popular among men than among women. The existence of such gender-related bias in the reached destinations is interesting to explore since it allows to make further assumptions on how city services or workplaces are organized in Seattle. These aspects will be investigated in the next video during the multivariate analysis. From the univariate analysis we already know that some stations are more popular than others in terms of arrivals. Now, we further inspect this aspect by checking for each station how many bikes arrive and depart. This analysis is important to identify unbalanced stations, namely stations that Pronto needs to take special care of when redistributing bikes among stations. In this plot we show for each station on the x-axis how many times it was the origin and on the y-axis how often it was the destination of a trip. We also include a dashed straight line to easily identify hub stations where the number of arrivals and departures is similar. We can observe the existence of unbalanced stations, namely stations that are depicted far away from the diagonal. A station can be unbalanced because many users pick up bikes from there, but few drop them off, or vice versa. From the graph above it is hard to extract further information on these stations. For this reason, we mark on the map of Seattle where these stations are located. To spot insights related to unbalanced stations, we draw stations using different colours and sizes: the green circles show unbalanced stations with more arrivals than departures. The red circles show unbalanced stations with more departures than arrivals. For both holds, the bigger its size, the higher the unbalance. To ease the visualisation, we only plot the top 10 (unbalanced) stations. As we can see on the map, the unbalanced stations with highest amount of departures - hence the red circles – are all located on Capitol Hill while the unbalanced stations with highest amount of arrivals - the green circles - are all located downtown. This last finding opens another possible explanation to justify why there are many stations of arrival downtown: bikers may be more prone to use bikes to move downhill. To explore this hypothesis, we again plot the unbalanced stations but now using the elevation from the sea level for the size of the circle. Comparing this map to the one before, we can confirm our hypothesis, namely that unbalanced stations with more departures than arrivals are the ones located uphill while unbalanced stations with more arrivals than departures are the ones located downhill. This observation provides useful insights for the Pronto system. Indeed, in case of the opening of new stations, we can already foresee whether the station would be unbalanced or not and take this into account for handling the logistics. With this bivariate analysis, we learnt a lot about the users' behaviour. In the next video, we will go one step further and perform a multivariate analysis hence take more than two variables into account to find relations between them.","tags":"Data Exploration","url":"/Data Exploration/2022-09-05-bivariate-analysis.html","loc":"/Data Exploration/2022-09-05-bivariate-analysis.html"},{"title":"Multivariate Analysis","text":"Multivariate Analysis Welcome to the fourth tutorial video on data exploration. In the former videos, we concentrated on uni- and bivariate analysis. In this video, we conclude the exploration of the bike-sharing dataset with a more sophisticated multivariate analysis, complementing the analyses presented before. At the end of the last video, we validated that the elevation plays a major role in explaining why some stations are unbalanced with respect to arrivals and departures. In other terms, the elevation embodies the 'cost' that a user has to face when moving between stations that are at different levels. This aspect, that is the cost of reaching nodes within a network, is a general problem that is faced in network analysis for identifying the best routing. For this reason, we continue the previous analysis by focusing on the 'cost' of reaching stations without the need of using the elevation explicitly. By making this problem more generic, the proposed analysis can be applied also to other networks where there are 'costs' for traveling between nodes which influence routing. We redefine hence the previous problem setting as follows: each bike station represents a node of a network and each node is connected through an edge to all other nodes of the network. The single edges are bidirectional, and each direction has an associated cost. In our case, the cost is represented by the time required for traveling between two stations. We assume that given the same distance, the time required for moving between two stations is determined by the difference in elevation, hence, biking uphill requires more time than biking downhill. As a first step to perform this analysis, we compute the trip duration among unbalanced stations. An excerpt of these trip durations is provided in the table on the right. The table shows the cost hence, the time it takes to reach a given node when departing from another one and vice versa. In the example, we see that the cost to go from Cal Anderson Park, 11th Avenue & Pine Street to E Harrison Street & Broadway Avenue East is less expensive than taking the return way with a time difference of less than a minute. Hence, it is slightly faster to go in one direction than in the opposite direction. From this table, we plot a matrix depicting the difference of trip durations between each couple of unbalanced stations. The colour map indicates how much longer the median trip duration is from one station to the other, hence the higher the values, the redder the colour and as such, the bigger is the difference between the costs in the two directions. If the colour tends to a darker blue, the difference is negative such that it takes less time from station A to station B than the return way. To facilitate the visualization of the stations located in Capitol Hill, and therewith with a high elevation, the names of these stations are reported in bold letters. We observe that the connection from 12th Avenue & E Mercer Street to 2nd Avenue & Spring Street has the largest negative difference indicated by the cell with the most intense blue colour. In addition, we can see that the first one is located in Capitol Hill while the latter is located downtown. Similarly, the connection between Cal Anderson Park, 11th Avenue & Pine Street for example, which is also located in Capitol Hill, to Republican Street & Westlake Avenue North, located downtown, also shows a large negative difference. In fact, all stations located in Capitol Hill show a significant negative difference compared to the stations located downtown. This is consistent with the previous observation that the first group of stations are located at a higher elevation than the second group. Between the stations located downtown we can observe light - positive and negative - differences, which might be due to the particular relief characteristics and traffic organization of Seattle's downtown road network. Finally, between the stations located in Capitol Hill, we observe a slight positive difference, again likely revealing particular relief and traffic organization characteristics of that area. An exception is the large positive difference observed between 12th Avenue & E Mercer Street and 15th Avenue East & E Thomas Street. If we inspect the elevation of these two stations, we observe that there is a difference of 10m between them, which might at least partially explain their difference in trip duration as other pairs of stations exhibit a higher difference in altitude. Another interesting fact revealed during the bivariate analysis was the relationship between gender and the stations of arrival. In this analysis we want to further explore how this relationship evolves during the week. As a first step, we take the top ten stations of arrival on weekdays during the morning rush hour. We focus on this time window since we assume that it is used by bikers to go to work. For these stations, we check whether they are more popular among men or women. We can see that Pier 69, Alaskan Way & Clay Street is twice more popular for women than for men, whereas 9th Avenue North & Mercer Street is twice more popular for men than women. For all other stations, the difference is smaller, meaning that the popularity of a station of arrival in the morning rush hours is similar. If we compare this bar plot with the previous one in the former video we realize that the difference in ratio is significantly lower. This can signify that on weekdays, during rush hours, both men and women are going toward the same gathering places. To inspect this hypothesis, we now focus on stations of arrival during the weekend. The assumption is that during weekends bikers may go to different places than their work location, for example, to follow their personal interests and hobbies. For this reason, we use all stations of arrival that are reached on weekends. For these stations, we check whether they are more popular among men or women. With this analysis, we discover that during the weekend men and women have different favourite stations of arrival. Moreover, if we compare the last three bar plots, we discover that the ratio of men vs women of the top stations of arrival changes during the week. Indeed, we can see that on weekends there are in total seven stations where the ratio of women to men or the ratio of men to women is above 2. During weekdays this ratio was above 2 for only two stations. This seems to imply that, in absence of constraints like the location of the working place, gender plays a role on where bikers go. We complete this analysis by plotting the previously identified stations on the map. We distinguish between the following criteria: We use blue circles for the top 10 stations which are reached in the morning rush hours during weekdays, green circles for the top 10 stations which are reached by women during weekends and red circles for the top 10 stations which are reached by men during weekends. Note that red circles appear in a light red tone, close to orange while superposition of blue and red circles results in a dark red colour and superposition of green and blue circles results in a dark green colour. The distribution of circles of different colours confirms that the popular arrival stations during weekdays do not match with the popular stations reached by men and women during weekends. Further, it confirms that there are popular locations of arrival during weekends in the University district which were not present during weekdays. In order to further explain the differences observed during weekends, a more in-depth knowledge about the city and the shared habits of its inhabitants would be required, which falls outside the scope of this tutorial.","tags":"Data Exploration","url":"/Data Exploration/2022-09-05-multivariate-analysis.html","loc":"/Data Exploration/2022-09-05-multivariate-analysis.html"},{"title":"Univariate Analysis","text":"Univariate Analysis Welcome to the third video on data exploration. In this one, we will concentrate on the univariate analysis of the given data of the Pronto bike-sharing service. For this, we start by investigating the different variables individually. We know already that the dataset contains information on more than 140,000 single trips. By checking the first and last date of the trip data, we see that the information is given for exactly one year. Additionally, we can check how many single bikes and stations are given in the dataset. With more than 480 bikes at over 50 stations, we expect diverse riding behaviour with interesting inter-dependencies. As a first analysis, we concentrate on the single trip durations because the trip duration directly affects bike availability and the type of service that can be provided by Pronto. First, we visually check whether there are outliers in trip duration. For this, we plot the distribution of trip durations to spot trips that are much shorter or longer than the remaining ones. Please note that the y-axis is reported in logarithmic scale to improve the readability of the graph. We observe that bike usage can vary from a few minutes to 480 minutes, or 8h. The latter is probably a limitation due to the service hours of the Pronto service, that is, the maximum amount of time that a bike can be rented during one single day. By just looking at the distribution we cannot notice any anomaly. In a second step, we have a look at the users' age as we assume that it influences the bike usage behaviour for example due to health and working status. Consequently, it is relevant to explore the typical users' age. Similarly to the previous analysis, we analyse the age distribution to spot outliers and let potentially interesting insights emerge. We see that the youngest user is 16 years old, the oldest one 79. As indicated before, please note that these are only the subscribed users. It is possible that younger or older people use the bikes as well but are Short-Term Pass Holders for which no specific information is available. The biggest group of users is 28 years old, hence the mode of the distribution, while the average age is 35. Interestingly, we can notice that the distribution of the ages is not as smooth as expected: An unexpectedly high number of users is 28 years old. To inspect this anomaly, we proceed with an analysis of the birth years. By additionally looking at this distribution, we discover that the anomaly may depend on the users' declared birth year. The histogram indicates that bike rental seems to be quite common for users born in 1985 and 1987, but not for those born in 1986. It is unlikely that these peaks come from a 'baby-boom' in the Seattle area in these specific years. This may depend on a limited number of users born in those years which intensively used the service during the year for example, for a sports club with frequent commuters or a reoccurring city trip organized by some youth association. Unfortunately, the data does not include this type of information. In case you want to dig deeper into this anomaly, you could further analyse whether there are bikes that are frequently moving back and forth from or to the same locations in the same period of time for users that were born in 1985 and 1987. But this is beyond the scope of the univariate exploration. As already discussed in the former video, we also have a look at the gender distribution. Also in this case, we should note that we do not have the identity of the users of each ride. Consequently, we cannot exclude that this bar plot is biased by a limited number of men that used the service with an annual pass while more women use short-term passes. Nevertheless, it is a trend that is observed in many Western cities and is often linked to narrow and sometimes dangerous biking lanes. Finally, we have a look at the single locations where a bike was rented. The Pronto rental service relies on bike availability at each station. For this reason, it is interesting to explore how bikers used these stations and whether some stations are more popular than others. The bar plots shown here provide the top 10 most and least popular stations of arrival. As you can observe, there is a big difference in terms of trips toward these stations. This can lead to unbalanced stations, where extra bikes need to be continuously moved toward stations with missing bikes. This aspect will be further inspected later on in the tutorial. From this univariate analysis we already learnt several things about the biking - or at least the subscription – behaviour in Seattle. We know already that the average subscriber is male, around 30 and enjoys going to Pear 69. In the next video, we will expand our analysis to bivariate analysis in order to better understand how two variables influence each other.","tags":"Data Exploration","url":"/Data Exploration/2022-09-04-univariate-analysis.html","loc":"/Data Exploration/2022-09-04-univariate-analysis.html"},{"title":"Data Understanding and Preprocessing","text":"Data Understanding and Preprocessing Welcome to the second tutorial video of the AI starterkit on data exploration. In this video, we will introduce the data that will be used throughout the tutorial and explain how to preprocess your data before you can start with the statistical exploration. Throughout the tutorial, we will explore the datasets shared by Pronto, Seattle's bike sharing system. On the one hand, Pronto provides information about the single stations where you can take a bike. On the other hand, information about the single trips themselves are shared in an anonymized way. While we will have a brief look at the location data, the focus in this tutorial is on the trip data. Now, let's have a look at the data sets. We can see, that - besides the pure geolocation data given by latitude and longitude – quite some information is shared about the single stations. Each station can be uniquely identified either by its id, its name, defined by the street intersection where the station is located and more technically, by its terminal name. We also have details on the size of the station, given by the number of docks available and when the station was put in service, given by the variable \"online\". Finally, we also know about the station's elevation from sea level. With the geo-coordinates given, we can put the single stations on a map - probably the easiest way for humans to understand where the stations are located. From the map, we deduct that - location-wise - there are two groups of stations in Seattle. One is located in the University district, while the other is located downtown and in its surroundings for example around Capitol Hill. Now that we know where the single stations are based, let's have a look at the trips data in order to understand how the single trips can be linked to actual movements throughout Seattle. Even though the trip data is anonymized, still quite some information is given. For each trip, given by a unique \"trip id\", we know when and from where the trip started and where and when the bike was returned. The resulting trip duration is given in seconds. Further, every bike in use has a unique \"bike id\" such that we can also follow a single bike. Additionally, we learn something about the single bike users. We know whether it either was an Annual Member having a ride or a Short-Term Pass Holder with a ticket for 1 or 3 days. On top of that, the gender and year of birth of the bike user is shared. In order to make it possible to also place the single trips on a map later-on, we merge the geographical information namely latitude, longitude, and elevation from the stations with the information of the single trips. Note that we need to execute two merge commands: A first one for the departure station and a second one for the arrival station. An excerpt of the merged data that we will use in the rest of this Starter Kit is shown in the table on the right. For reasons of readability, we show the data here in a transposed manner such that each column provides the information for one single trip. Before proceeding with the further exploration, we first check the completeness of the data, hence where we observe missing values. We can see that at most more than 140,000 entries are given. Most variables seem to be complete, except gender and birthyear, which only contain about 87,000 entries. The reason for this is that Short-Term Pass Holders do not need to specify their age and gender, due to which this information is just missing in the data. This can be confirmed by calculating the percentage of data present for these two variables for the single user types. While for all Annual Members, gender and birthyear are given, it is not known for any of the Short-Term Pass Holders. For our later analysis we want to replace the missing values with meaningful entries. In order to see how to replace them, we first have a look at the existing values. For the Annual Pass Members, three different gender options are available: Female, Male, and Other. In order to be able to distinguish the missing values from the remaining ones later-on, we replace them by \"Unknown\". Interesting is the fact that almost 80% of all users are male. We will get back to this in the following video. In order to further prepare the data for subsequent analysis, we perform the following actions: First, we convert the variable tripduration which is currently given as a float variable, to a datetime.timedelta object and store it as new variable tripdurationTimeDelta. Further, we add a new variable tripdurationMinutes indicating the duration of a trip in minutes which we keep as a float value. For further analysis, it can be convenient to work with age instead of year of birth. Therefore, we convert the variable birthyear to an integer and create a new variable age. Finally, we extract from the starttime variable the month, the day of the week and the hour of the day, and use the starttime variable to index the data. The table presents an excerpt of the dataset after these transformations. Note that also here, each column indicates one trip. With this well-prepared dataset, we can start the actual data exploration in the next video.","tags":"Data Exploration","url":"/Data Exploration/2022-09-03-data-understanding-and-preprocessing.html","loc":"/Data Exploration/2022-09-03-data-understanding-and-preprocessing.html"},{"title":"Introduction","text":"Introduction Welcome to the video tutorial for the AI Starter Kit on data exploration. In this first video, we will discuss why data exploration is the first step in answering any data science question. Before starting to use complex algorithms to solve a data science problem, you need to gain a thorough understanding of the corresponding data and the underlying business challenge you are trying to address. Data exploration is the process through which you gain such an understanding. This will eventually enable you to derive viable working hypotheses related to the problem at hand, which is useful for the further analysis and modelling of the data. Throughout this process you will be faced with different types of variables, for example numerical or categorical, which require different types of treatment. In addition, looking at individual variables in isolation is not sufficient, as this does not consider the, often complex, interactions between different variables. Furthermore, you will need to identify which are the relevant variables in your dataset and, on the other hand, which ones worsen its outcome rather than improving the quality of your analysis. Finally, data exploration will help you to point out which data quality issues your dataset may have and which actions you should undertake to mitigate them. Amongst others, data exploration is useful to identify usual and unusual values for a variable but also, to study the evolution of a variable over time. It can be helpful to uncover significant relationships between different variables or to assess data quality and suitability. The goal behind this Starter Kit is to lay out a series of analyses that will teach you how to explore individual variables, look at how pairs of variables are related and study the complex interactions between groups of variables. You will learn how to conduct a quantitative and visual inspection of your dataset and you will be prepared for the next step in your advanced analytics workflow. In the tutorial of this AI Starter Kit, we follow a systematic approach to explore a dataset, based on the number of variables under consideration. We start by exploring individual variables in isolation, which is the simplest form of analysis and is called univariate analysis. We continue by studying pairs of variables, which is called bivariate analysis. And finally, we study the relationship between several variables, called multivariate analysis before we conclude on our findings. But before diving into this, in the next video, we will first introduce you to the dataset that we will use in this analysis and perform some basic preprocessing on it.","tags":"Data Exploration","url":"/Data Exploration/2022-09-02-introduction.html","loc":"/Data Exploration/2022-09-02-introduction.html"},{"title":"Sir Adas' series: The invisible war against women","text":"https://res.cloudinary.com/people-matters/image/upload/q_auto,f_auto/v1617632330/1617632330.jpg We have not done a lot of SirAdas blogging recently. The workload has taken the upper hand in our daily lives and then there was the insane war in Ukraine. History repeating again … few machos showing off with power and weapons. And as always, the victims are mostly women and children. However, there is another war which has been ongoing for centuries, namely the invisible violence against women. Occasionally we get reminded of it, either via some horrible news in the media or being confronted with it in our own lives. This time it was a regular post from Katrien Van Geystelen, the best voice coach I ever had. Typically, her posts are very cheerful, full of interesting insights and useful tips about how to best use your voice. Not this one! You can read the original post in Dutch here: https://www.linkedin.com/feed/update/urn:li:activity:6929768751680643072/ For those of you not speaking Dutch, here a very short summary of Katrien's post. She described how after attending a very inspirational evening event (as she brilliantly puts it, an event with very high \"we can\" content) for entrepreneurs, she and another female participant had to run scared through an underground parking being chased by 3 young men. Luckily, they managed to reach safely their cars on time. Katrien's story is so recognisable. I'm sure almost every woman has experienced something similar at least once in her life. I had to think about the last time I experienced something like this, about 4-5 years ago. I was at an offsite with my team somewhere in Limburg. In the late afternoon of the second day, my teammates were busy with some team building activities in the gardens of the domain where we were staying. I decided to go running. It was a warm summer day and I headed towards the forest bordering the domain dressed in jogging shorts. I was just entering the forest when I heard behind me the irritating noise of a motorcycle, which obviously had some problems with its exhaust. Very characteristic! The motorcycle overtook me and the guy driving it gave me some strange feeling. While passing me he turned his head back several times to look at me. A woman knows when it doesn't feel right! I continued running for several minutes somewhat alert and anxious. We had a walk in the forest the evening before when we arrived. Thus I knew that the black road through it was about a kilometre long and completely surrounded by very densely grown tall trees. So once inside the forest, nobody outside could see nor hear you anymore. All of a sudden I realised that something changed. What? I didn't hear anymore the irritating noise of badly working motorcycle exhaust. I thought this was strange. He didn't seem to drive very fast to so quickly disappear, but I could not see far enough since the road was turning. I took the turn and there some 200 meters ahead I could see the guy with the motorcycle standing still (waiting for me?) and looking towards me. It felt immediately as I needed to make the most important decision of my life. I had to admit that only for some very tiny split second I contemplated to just carry on with running. Then I just turned back sprinting faster and faster as I was hearing somewhere behind me the noise of badly working exhaust approaching … In despair, not being able to further increase my pace from exhaustion, I approached the end of the forest seeing through the trees my teammates far away fully occupied with their activities, without any awareness of my struggle. I started jumping (to be seen above the fence) and shouting as crazy. First they ignored me thinking I am just being funny, but at the end one of them run towards me from the other side of the fence. I was safe! At that moment the motorcycle passed next to me, and the guy turned back his head examining carefully my scarcely dressed in jogging shorts body … There I was, a successful professional woman, mature and assertive, leading a team of talented people often looking up to me for advice and support, transferred into a humiliated and scared child. Mother of 2 children, but not being able to even have a short running session in the countryside on my own. We had a team dinner that evening. I was still shaken by the afternoon incident, but when I tried to share my state of mind with my, predominantly male, teammates I was shocked by their indifference and disinterest. They wanted just to have fun, teasing and sharing jokes with each other. I felt as a paranoid female, overdramatising the facts. I started doubting myself, maybe it was not really dangerous. I just over-exaggerated it, I imagined it, …. Did I? No, I didn't! I was really very lucky! However, many women have been and will be much less lucky as described by Rebecca Solnit in her essay \"The longest war\" published in https://en.wikipedia.org/wiki/Men_Explain_Things_to_Me. I do not actually know how to finish my blog. I shared my story with Katrien in response to her post. This is what she wrote back to me: \"Yesterday my colleague and I went to the police office to report what happened. To be honest I can write a new story about that… We were not stimulated to report it at all. It's a shame!\" It is indeed a shame! The end of the longest war, as Rebecca Solnit defined it, is unfortunately not yet in sight …","tags":"Blog","url":"/Blog/20220525_the_invisible_war_against_women.html","loc":"/Blog/20220525_the_invisible_war_against_women.html"},{"title":"Key Take Away Messages","text":"Key Take Away Messages In this Starter Kit we have demonstrated how creative visualisations can already reveal interesting patterns and more elaborate insights in your data, even before any complex algorithm is applied. In particular, we have used bike counter data to illustrate how timeline plots, different types of heatmaps, streamgraph plots and scatterplots can be used bringing various kinds of insights to the surface. We showed how such visualisations could not only be used to explain certain characteristics of the data, such as some nodes having more crossings than others, but also to identify global trends, such as an increase in traffic over the years, and seasonal trends, such as fluctuating popularity within a year. Furthermore, we were able to recognize structural patterns, such as distinct weekday and weekend traffic patterns and by that detect outliers, such as weekend traffic patterns that occur on weekdays. Additionally, we discussed how the selection of colormaps can influence the visibility of given effects. These insights can help in formulating hypotheses to be validated further or can be used in subsequent analysis steps, such as feature engineering and data-driven modelling. We thank you for completing this video series and hope to welcome you in another AI Starter Kit tutorial.","tags":"Advanced Data Visualization","url":"/Advanced Data Visualization/2022-05-08-key-take-away-messages.html","loc":"/Advanced Data Visualization/2022-05-08-key-take-away-messages.html"},{"title":"Key Take Away Messages","text":"Key Take Away Messages In the video tutorial for this AI Starter Kit, we demonstrated forecasting the demand for a particular resource at a particular point in the future, illustrated on the case of energy demand forecasting. In doing so, we have taken you through the different steps in the data science workflow. We first performed a number of required data preprocessing steps in order to prepare the data for further analysis. This included fusing the electricity and weather data, requiring to resampling the former data to a lower sampling rate. Subsequently, we explored the fused dataset in order to better understand which factors influence the household energy consumption. To this end, we resorted to both visual and statistical exploration methods. Based on this, we could identify a number of yearly, weekly, and daily patterns, including particular behavior during weekends and some holiday periods. This information formed the basis for the feature extraction that followed next, such as the energy consumption a week before, the day of the week and the hour of the day. Finally, we served the extracted features to two different models, namely Random Forest Regression and Support Vector Regression to perform the actual forecasting. Through different experiments, we analyzed the influence of the training strategy, type of machine learning model and effect of data normalization on the model performance. Based on this, it became clear that achieving good prediction results depends on different factors, including the appropriate data preprocessing, the amount of available training data and the algorithm parametrisation. As next steps, we invite you to further explore each of these influences in more detail. Especially with respect to the hyperparameter tuning, it proves worth to further explore the different parameter settings and study how these changes impact the model performance. Furthermore, you can also try to adapt the different steps to the context of your own dataset. While the details of each of the steps might differ, the methodological steps we presented are typically the required phases you need to go through when solving a resource demand forecasting problem. We thank you for completing this video series and hope to welcome you in another AI Starter Kit tutorial. Additional information The video material in this website was developed in the context of the SKAIDive project , financially supported by the European Social Fund , the European Union and Flanders. For more information, please contact us at elucidatalab@sirris.be","tags":"Resource Demand Forecasting","url":"/Resource Demand Forecasting/2022-05-08-key-take-away-messages.html","loc":"/Resource Demand Forecasting/2022-05-08-key-take-away-messages.html"},{"title":"Data Modelling and Forecasting","text":"Data Modelling and Forecasting Now that we gained the necessary insights from the data, extracted a number of meaningful features, and gained some theoretical background on regression models, we will use the AI Starter Kit to discover the most important factors for training a machine learning model. More specifically, we will analyze the influence of the training strategy, type of machine learning model and effect of data normalization on the model performance. To evaluate the performance of the models the mean absolute error or MAE is used - a metric commonly used in literature for this purpose. This metric quantifies to what extent the model forecasts are close to the real values. As the name suggests, the mean absolute error is an average of the absolute errors. It holds that the lower the MAE of the model, the better its performance. As just mentioned, the training strategy, the machine learning model and the data normalization may influence the quality of the predictions. Before we start training a model, let us dive a bit deeper in these three influences. First of all, let us turn to the training strategy. The training data typically highly influences quality of the model. Therefore, in this starter kit, we will experiment with six different training strategies, to study their influence on the quality of the resulting model. First, we will use 1 month before the test month as training data. In the Starter Kit, several models are trained for the predictions of each individual month. Each model is trained on one month before the month we are making predictions for. A one-month gap is introduced between the training and the test month in order to avoid that the last day of the training set is also included in the first day of the test set. For example, the models that will make the predictions for April 2008 will be trained on the data from February 2008. Secondly, we will use 6 months before the test month as training data. This is thus highly similar to the above experiment, but with 6 months of training data. It still includes a 1-month gap. Or we can go back even further in time and use 1 year before the test month as training data. Similarly, we can use 1 month the year before as training data. This is similar to strategy number 1, but with a gap of 11 months. This way, the training data and test data are taken from the same month, one year apart. Further, we can also use all months before the test month as training data. For each test month, a model is trained using all the data prior to this (including a 1-month gap). This strategy simulates the scenario where the model is retrained as new data comes in. Since this requires training several models based on potentially large training sets, this might require a certain computational time. Finally, we experiment with training on the first year of data and make predictions on the rest. This strategy is commonly used for training these kinds of models. With these training strategies, we will see how strongly the amount of data but also the seasonal pattern will influence the quality of the model. As already introduced in the former video, we will train two different types of models, namely a Random Forest Regressor and a Support Vector Regressor. Besides these models, we will also use a simple benchmark model that we use to compare the performance of the models against. For the prediction of the energy of a certain day, this benchmark model simply takes the energy consumption of the previous day at the same time. Note that for this approach no model is built and consequently it does not include any training phase. Finally, a short note on normalization. We saw already in the video on data understanding that the scale for the outside temperature and global active power are quite different. This is also true for the remaining features that we introduced. Therefore, it might be necessary to normalize the data, that is, rescale it such that all input and output values are between 0 and 1. This is indeed a requirement for the correct training of some machine learning models. We will test in the Starter Kit whether it makes a difference for the models suggested. At first, let us run the baseline model as we will later on compare all model results to these results. The mean absolute error is 604.45. Putting this number into relation with the mean global active power of 1118, the error is comparably high. For reasons of comparability, each result will be shown in the table just below the interface. As discussed above, we first want to analyze the influence of the training strategy. Therefore, we train the Support Vector Regressor on strategy 1, so for 1 month of data, and on strategy 2, for 6 months of data. In this first experiment, we will not normalize the data beforehand. For both strategy 1 and 2, the mean absolute error is even bigger than for the benchmark model. Therefore, we are going to use the normalized data instead. With that, the model predictions improve and for both strategies, this results in better forecasts than the baseline model. We can do the same for the Random Forest Regressor. We train if for both strategies – namely strategy 1 and 2 – and on the normalized and non-normalized data. In all four cases, the results are better than those obtained by both the baseline model and the Support Vector Regressor. Now it is up to you. Train different models and find out which model returns the best results and which influences are the strongest. If you want, you can pause the video for this. These are the basic findings when training all models: Concerning the training strategy, we see that for the Support Vector Regressor the standard train-test split has the best performance, although using all months and using a one-year window before the test month as training data both have a similar performance. Furthermore, we clearly see that using a one-year window before the test month as training data leads to better performance than the other training strategies. We note that for both this \"one year window\" strategy and the standard train-test split, the training data is one year. The difference, however, is that for the former the training set changes, while for the latter it is fixed meaning that all the data from 2017 is used. A general trend that can be observed is that the performance increases as a larger training set is used. We therefore urge the user to look at the predictions made using the training strategy where all months before the test month are used as training data: we can expect that the predictions for the later months will be better than for the first months since more training data was used. However, this is hard to see by eye in the prediction plot directly. Regarding the chosen model, we can see that both the Support Vector Regressor and Random Forest Regressor outperform the simple baseline we set up. Further, the Random Forest Regressor outperforms the Support Vector Regressor for all training strategies. This shows the importance of knowing which model is best suited for your problem and testing different ones. Important to note though, is that no extensive hyperparameter tuning was performed, which could alter this observation. This is required for a proper validation of the algorithms and therefore we encourage the user to also experiment with altering the hyperparameters and study the influence on the results. Finally, normalizing the data, rescaling it such that the input and output variables all have values within a similar range - in this case, between 0 and 1 - is a common step when setting up machine learning models. This is because, depending on the model, they work much better with values of this order of magnitude. We see that normalization indeed greatly improves the predictions for the Support Vector Regressor but has little influence on the random forest regressor. Indeed, algorithms such as random forests and decision trees are not influenced by the scale of the input variables. We hope that you have gained more insights on how the training strategy, the type of machine learning model and data normalization influence the model performance and are familiar now with the usage of the interface. We suggest that you try a number of additional combinations of settings and think about in which way they influence the quality of the model. In the next video, we will summarize the key take away messages and provide you with a number of suggestions for additional experiments to gain additional insights. Additional information The video material in this website was developed in the context of the SKAIDive project , financially supported by the European Social Fund , the European Union and Flanders. For more information, please contact us at elucidatalab@sirris.be","tags":"Resource Demand Forecasting","url":"/Resource Demand Forecasting/2022-05-07-data-modelling-and-forecasting.html","loc":"/Resource Demand Forecasting/2022-05-07-data-modelling-and-forecasting.html"},{"title":"Key Take Away Messages","text":"Key Take Away Messages In the video tutorial for this AI Starter Kit, we have demonstrated techniques that can be used to preprocess time series data in order to improve its quality for further exploitation. To illustrate the respective methods and techniques, we have used a real-world dataset containing wind turbine data that exhibits the typical characteristics of industrial datasets, including noise, outliers, missing values, and seasonal patterns. First, we demonstrated how resampling and smoothing can be applied to better understand the behaviour of the time series. As we have shown, resampling techniques allow to reduce the temporal granularity, thereby revealing longer-term trends and hiding sharp, fast fluctuations in the signal. Smoothing on the other hand modifies the time series data such that individual points that are higher than the adjacent points - presumably caused by noise - are reduced, and points that are lower than the adjacent points are increased leading to a smoother signal. Subsequently, we showed how the quality of the data can be improved through normalization and outlier detection. Normalization rescales the range of a particular variable in order to make different variables with different ranges comparable. Outlier detection tried to identify the extreme values in the time series that deviate from other observations in the data. The detected outliers can subsequently be examined in more detail to investigate whether these were caused by measurement errors, in which case they can be removed, or if it concerns unexpected phenomena which are interesting to look into in more detail. Finally, another frequently occurring problem with time series data is that it suffers from missing data, caused by for example senor malfunctioning, communication errors, or due to outliers that were removed. We illustrated how to impute this missing data by means of 3 different techniques: linear interpolation, fleet-based imputation and pattern-based interpolation, and outlined what are the advantages and disadvantages of these techniques. The use of these methods allows to improve the quality of the data and to prepare it for further exploration, analysis and modelling purposes. This is a crucial step, as it will improve the completeness and reliability of the input data, and consequently the accuracy of your results. We thank you for completing this video series and hope to welcome you in another AI Starter Kit tutorial.","tags":"Time Series Preprocessing","url":"/Time Series Preprocessing/2022-05-07-key-take-away-messages.html","loc":"/Time Series Preprocessing/2022-05-07-key-take-away-messages.html"},{"title":"Key Take Away Messages","text":"Key Take Away Messages In the video tutorial for this AI Starter Kit, we covered the basics of using deep learning for remaining useful lifetime prediction, which is one of the central topics in predictive maintenance. We illustrated the approach on the use case of predicting the remaining useful life of aircraft engines based on run-to-failure data. In doing so, we have taken you through the different steps in the data science workflow. We started with the business and data understanding, which allowed us to gain more insights into the specific problem to solve and the structure of the data. Based on this information, we continued with the data preprocessing, in which we explained how to appropriately prepare the data. This included the construction of a training, test and validation set to learn and evaluate a model to predict whether an engine will fail within a certain number of operational cycles. In the video on data modelling and analysis, we showed you how to use deep learning to construct a model for remaining useful life prediction. In particular, we applied a deep learning approach called long short-term memory (LSTM) networks which allows the construction of a predictive model without the time-consuming feature engineering step. This voided the need to manually extract the characteristics from which the algorithm can learn. Finally, we also explained you how to experimentally validate the resulting model and compare the results of different models. While the use of deep learning in some cases might eliminate the need for manual feature engineering, it remains important to tune the models with the appropriate parameter settings, which requires the necessary effort. While the presented approach already gives promising results, there are still quite some improvements possible. Therefore, we recommend you to further try tuning the different parameters, and study the effect on the results. You can also continue to experiment with different network architectures, for example by altering the number of layers and nodes. If you want to gain deeper insights in remaining useful lifetime prediction, a good exercise is trying to cast the prediction problem as a regression or multi-class classification task instead of as a binary classification task. It might also be worth to experiment with the methodology on a larger dataset with a higher number of assets or to try to adapt the different steps to the context of your own dataset. While the details of each of the steps might differ, the methodological steps we presented are typically the required phases you need to go through when solving a remaining useful lifetime prediction model. We thank you for completing this video series and hope to welcome you in another AI Starter Kit tutorial.","tags":"Remaining Useful Life Prediction","url":"/Remaining Useful Life Prediction/2022-05-07-key-take-away-messages.html","loc":"/Remaining Useful Life Prediction/2022-05-07-key-take-away-messages.html"},{"title":"Data Modelling and Analysis","text":"Data Modelling and Analysis Welcome to the fourth and last video of the tutorial for the AI Starter Kit on remaining useful lifetime prediction! In this video, we will explain you in detail how a deep learning algorithm can be used to train a model that is able to determine if an aircraft engine is entering the last 30 cycles of its remaining useful lifetime. In the introductory video to Deep Learning, we discussed the difference between Machine Learning and Deep Learning. One of the main differences was given by the need of feature extraction. This can be a very time and knowledge demanding task. As we will concentrate on LSTMs in this AI starter kit, fortunately this step is taken care of by the deep learning algorithm itself. Nevertheless, some data modelling is necessary beforehand. As a first step in the modelling phase, we will prepare the data to serve as input for the LSTM network. When using LSTMs in the time-series domain, one important parameter to pick is the sequence length which is the window for LSTMs to look back in time. We discussed already in the data understanding video how differently the single variables for a given engine behave and consequently that the time when the degradation becomes visible is different for different engines. Hence, the window size chosen for the training data strongly influences the classification results. In order to model the data for training the algorithm, we first need to reshape the input information. So far, the data consists of the sequential measurements for each of the sensors and settings over time, engine per engine, meaning that one row per cycle and engine is given in the table. This format is however not so suitable for an LSTM model. Therefore, we create a matrix in the format (samples x sequence length x features), where 'samples' is the number of sequences that we will use for model training, ‘sequence length' is the look back window as discussed before and 'features' is the number of input values of each sequence at each time step. In our case, we selected all sensor values, as well as the 3 settings parameters and the (normalised) current cycle the machine is in at this point. With the data finally prepared, how can we now actually implement the LSTM architecture? Fortunately, there is no need to start from scratch but there are several open-source tools available that support you when building Deep Learning models. Keras is a deep learning library written in Python. It acts as an interface to the popular tensorflow library. This makes the implementation a lot more feasible. In the AI Starter Kit though, this is even simpler. Instead of implementing the Deep Learning network yourself, an easy-to-use interface was set up, to adjust the most important variables of the neural network. Once you decide that the parameters specified for building the model are correct, you simply push the \"Train model\" button at the bottom of the page and the whole implementation and model training is done automatically. Of course, it is also possible to modify the different parameters directly in the code. For more information on the meaning of each of the settings, you can have a look at the documentation page of Keras. Note that modifying the parameters outside of the scope that is defined in the interactive module can heavily influence the training time and the classification quality. So now it's time to build your own deep learning model. First of all, select the number of intermediate or so-called ‘hidden' layers. For this first experiment, we select one single intermediate layer only. We set its size to 30 neurons. This corresponds to a rather small network but will be sufficient for a first test. As discussed, we need to decide on the size of the look back window as well. In this first test, we will use a sequence length of 50 cycles. Furthermore, we add a dropout layer after each LSTM layer. The dropout consists in randomly setting a fraction rate of neurons to 0 at each update during training time. This helps preventing overfitting. We choose a value of 0.2 for this first experiment. Finally, we need to select the number of epochs to train. The epochs define the number of times to iterate over the training data arrays. The aim is that the model improves during each training epoch. In general, the more epochs, the better the results until we reach the given model's limit. Here, we will use 15 epochs. This model can now be trained. Now that we have a trained model, we can evaluate its performance. We will first evaluate it on the training data and subsequently on the test data. If both evaluations result in approximately the same score, this gives an indication that the model is generalizable to unseen datasets and thus not overfitting on the training data. In the interface, we switch to the tab Evaluation on the top of the animation to get more insights into the quality of the trained model. Pushing the button \"Evaluate model\" will lead to the evaluation of the last trained model. Depending on the use case, different evaluation metrics can be important. Therefore, we will on the one hand evaluate the model in function of accuracy, which measures the fraction of all instances that are correctly categorized. More formally, it is the ratio of the number of correct classifications to the total number of correct or incorrect classifications. On the other hand, we also show the so-called confusion matrix. The confusion matrix shows that the model is able to correctly classify that a specific engine is not going to fail within N cycles for almost all of the more than 12,000 samples. Vice versa, the model is able to correctly classify for almost all of the more than 3000 cases that a specific engine is going to fail within N cycles. In the summary table at the bottom of the page, several additional metrics are shown. Take your time to go through them once you run these experiments yourself. Note that this evaluation is performed on the training data. In order to evaluate the model against the unknown data, we continue to the next tab in the interface. In this case, the same evaluation as before is performed but on the test data. Also here, the results are promising. Now run an experiment yourself. For this, chose the following parameters and rerun the pipeline while pausing the video. Did it work properly? Congratulations! The only differences between the two experiments are the number of layers and their corresponding sizes. Which difference do you observe? Compare the results in the tables in both tabs - evaluation and testing. When comparing the two models, which one would you use and why? We hope that you have gained more insights in the key factors that influence the quality of a deep neural network and are familiar now with the usage of the interface. We suggest that you try different combinations of settings and think about in which way they influence the quality of the model. In the next video, we will summarize the key take away messages and provide you with a number of suggestions for additional experiments to gain additional insights.","tags":"Remaining Useful Life Prediction","url":"/Remaining Useful Life Prediction/2022-05-06-data-modelling-and-analysis.html","loc":"/Remaining Useful Life Prediction/2022-05-06-data-modelling-and-analysis.html"},{"title":"Data Science Theory - Advanced Regression Models","text":"Data Science Theory: Advanced Regression Models Before deciding on the most appropriate algorithm to solve a particular data science problem, a first step is to decide which type of task you are trying to solve. In order to do so, you usually need to start with finding the answer to a number of questions, based on the case under consideration. Without a clear understanding of the use case, even the best data science model will not help. A first question to answer in that respect is which type of outcome is expected from the use case owner. Is the aim to predict a category, such as ‘normal', ‘degrading' or ‘failed'? If yes, the next question to answer is whether labelled data is available or not. Labelled data is data for which examples are available that are annotated by a domain expert with the classes to predict. Put differently, for each data point or set of data points, a class is defined. Usually, the number of unique classes is rather small. This data will be used by the algorithm for training a model. Once trained, it can be evaluated on a test data set, for which the classes are known but will not be visible to the model. Evaluating the ratio of correctly predicted classes gives a measure of the quality of the model. Often used algorithms for classification are k-Nearest Neighbors, Decision Trees or Support Vector Machines. But what can we do if we do not have information available on possible classes? In that case, introducing a similarity between the examples that you have available makes it possible to cluster different data points into groups. These groups can then be used to gain deeper insights into the data, and in some cases can be mapped to particular classes. Partitioning-based clustering, hierarchical clustering or density-based clustering are often used techniques for this purpose. The situation is different in case a numerical value should be predicted. It is similar to the classification task, but the prediction range is continuous. For these so-called regression tasks, Ordinary Least Squares or Support Vector Regression are often used. If the goal is neither to predict a class nor a numerical value, but rather a future state, one typically turns to graphical modelling algorithms in order to predict these states. These techniques also allow one to include the available background knowledge into the modelling process. Examples of graphical modelling techniques are Hidden Markov Models and Bayesian Networks. To make the difference between the single categories a bit more clear, we discuss some examples: In case no labelled data is available, clustering of data points can provide insights in different modes in the data. An example is performance benchmarking of industrial assets. Let's assume the data to analyze comes from a wind turbine park. When looking at several measurements, like for example of the power curve, the wind speed, and the wind direction, we can identify different modes in which the single wind turbines are operating. In contrast, assume that we are interested in the expected power production of a particular wind turbine in the following days for which we have a weather forecast. We can use this information as input variables for a regression model and therewith predict the power production. If labels are attached to the gathered data, for example on the root cause of particular sensor failures, a classification algorithm can be used to train a model that is able to determine which fault is associated with a certain set of sensor readings. With this knowledge, we can decide which type of algorithms is suitable for the forecasting of electricity consumption. Let us use the decision tree for this: Do we want to predict a category? No, right, we are looking for a continuous value. So, we go to the right. And yes, we need to predict a value. Therefore, it is a regression task we are facing here. The question we want to answer is precisely: How much electricity will be consumed in the next hour, taking into account historical information regarding the electricity consumption and the outside temperature? There is a bunch of regression algorithms that can be used in various contexts. In our case, we have a comparably small feature set and all of them are numerical. Therefore, we will go for two commonly used algorithms for the prediction, namely Random Forest Regressors and Support Vector Regressors. We will introduce both algorithms in more detail in the remainder of this video. Random Forest Regression We start with Random Forest Regression. The base to build a random forest is a decision tree – which works similarly to the one we just used to determine which class of algorithms is suitable for the electricity forecasting. Since in a random forest, the model is defined by a combination of trees, it is a so-called ensemble method. Ensemble methods help improve machine learning results by combining several models. This approach allows the production of better predictive performance compared to a single model. From each decision tree a value is predicted, and the final prediction will be a weighted function of all predictions. Here we see a very simplistic version of a random forest regressor with only three decision trees. All trees are trained in parallel and each one will predict a value for a given set of input variables. The final prediction in this case would be the mean value of all predictions, ergo 10,67. In order to improve the performance of a model, you need to tune the algorithm's hyperparameters. Hyperparameters can be considered as the algorithm's settings, or put simply, the knobs that you can turn to gain a better result. These hyperparameters are tuned during the training phase by the data scientist. In the case of a random forest, the most important hyperparameters include the number of decision trees in the forest, the maximum depth of each of these trees, and the minimum number of examples and maximum number of features to consider when splitting an internal node in one of these trees. Support Vector Regression Another type of regression approach is support vector regression. While support vectors are mainly used in the field of classification, with some adaptions, it also works for regression tasks. It works similarly to an ordinary least squares regression where the linear regression line is targeted with the smallest overall deviation from the data points. This is very handy in case of linear dependencies and for clean data. But as soon as there are several outliers in the data set or the relation between the data points is non-linear, the quality of the model can decrease significantly. Especially in the context of industrial data, this can never be fully avoided. For Support Vector Regression a band of width epsilon ε is defined. We call that band the hyperplane . The aim is to search the hyperplane that includes most points while at the same time the sum of the distance of the outlying points may not exceed a given threshold. The training instances closest to the hyperplane that help define the margins are called Support Vectors . As for random forest regression, also support vector regression has a number of important hyperparameters that can be adjusted to optimize the performance. A first important hyperparameter is the choice for the type of kernel to use. A kernel is a set of mathematical functions that takes data as input and transform it into the required form. This kernel is used for finding a hyperplane in a higher dimensional space. The most widely used kernels include Linear, Non-Linear, Polynomial, Radial Basis Function (RBF) and Sigmoid. The selection of the type of kernel typically depends on the characteristics of the dataset. The cost parameter C tells the SVR optimization how much you want to avoid a wrong regression for each of the training examples. For large values of C, the optimization will choose a smaller-margin hyperplane if that hyperplane does a better job of getting all the training points predicted correctly, and vice versa. The size of this margin can be set by epsilon, which specifies the band within which no penalty is associated in the training loss function with points predicted within a distance epsilon from the actual value. Now that we gained some more knowledge on these two frequently used regression approaches, in the next video we will explain how to train a regression model for our household energy consumption prediction problem. Additional information The video material in this website was developed in the context of the SKAIDive project , financially supported by the European Social Fund , the European Union and Flanders. For more information, please contact us at elucidatalab@sirris.be","tags":"Resource Demand Forecasting","url":"/Resource Demand Forecasting/2022-05-06-data-science-theory-advanced-regression-models.html","loc":"/Resource Demand Forecasting/2022-05-06-data-science-theory-advanced-regression-models.html"},{"title":"Handling Missing Data","text":"Handling Missing Data In this video, we will finally use the insights from before and impute missing values in the dataset. Real-world industrial datasets often suffer from missing data due to several reasons. Missing data occurs often due to sensor malfunctioning or communication errors. In addition, if we have removed outliers as shown in the previous video, these will also show up as missing values in the data. In both cases, we can fill the missing values using so-called data imputation techniques. Multiple techniques exist and which one to choose depends on the data characteristics. The choice depends on the presence of trends, the length of the missing period, etc. In this video, we will present 3 different imputation techniques, namely Linear interpolation Fleet-based interpolation, and * Pattern-based imputation To evaluate the efficiency of the different methods, we will create a set of synthetic missing data events with varying durations. The main advantage of this approach is that it allows us to compare the outcome of the imputation procedure with the real, observed values. We start with the linear interpolation of values and interpolation based on the fleet median. Linear interpolation is a simple technique that is frequently used in case a small number of points are missing within a particular period. It simply connects the last point before and the first point after the missing data episode with a straight line. Fleet median, on the other hand, exploits the fleet-based aspect of our asset for imputing periods of missing data. For the dataset under investigation, we know the assets are co-located and are, therefore, exposed to similar conditions like for example the wind direction and speed. Hence, we can compute the median value of the wind speed for the turbines that do have data and use those values as an estimation of the missing values. Linear interpolation is sensitive to the event duration, meaning the longer the missing data event, the less likely it is that a linear interpolation will follow the real values. Fleet median interpolation may result in unexpected results if there are too few assets in the fleet or too few assets with non-missing values at the time. On top of that, as indicated before, the latter method is dependent on the different assets being co-located and exposed to similar conditions, which is a condition that might be too stringent in particular contexts. In the interactive starter kit, we can analyse how both methods perform for missing data events of different durations. We can also change the number of turbines that are considered for the fleet median interpolation and see how that affects the accuracy of the prediction. In our dataset, there are 4 turbines available, so you can choose between 1 and 3 turbines to use for the fleet median interpolation. The red trace corresponds to actual observed values. This means that the closer the blue or the green line, representing the linear and fleet mean interpolation respectively, are to this red line, the better the interpolation is. In order to understand the pros and cons of the methods, we perform some experiments: First, we decrease the duration of the missing data from 30 to 10 minutes, corresponding to one missing data point in the signal. We see that the linear interpolation approximates the original signal fairly well. When increasing the duration of missing data to 100, on the other hand, the interpolation deviates significantly from the original signal. Thus, the longer the period of missing data points, the worse the linear interpolation. Similarly, we can change the number of turbines for calculating the fleet median to a lower number. With only one turbine taken into account, also here the quality of the imputation drops but still returns a better approximation than the linear interpolation, especially for longer durations of missing data. Note, that this is only the case because we know that the wind turbines are co-located. In the starterkit, you can further inspect how the two methods are performing in different scenarios. When linear interpolation or fleet-based data imputation techniques do not lead to a sufficiently good approximation to the original signal, we can still use pattern-based imputation. In the following we illustrate this method on two attributes, namely the time series for temperature and wind speed. Pattern-based interpolation performs well on signals that follow a predictable pattern, as is the case for signals that show a strong seasonal modulation. We can appreciate this by comparing the interpolation of the temperature and wind speed signals. The former, as we have seen before, follows a daily pattern, ergo night vs. day and a seasonal pattern hence, winter vs. summer and therewith also shows a high-quality pattern-based interpolation. The latter, - the wind speed - however, has a weaker seasonal modulation reflected in a less accurate interpolation. As the basis for this pattern-based interpolation, we can use seasonal ARIMA - an extension of the ARIMA model - for forecasting seasonal time-series data. More specifically, in our case we can use seasonal ARIMA for forecasting the evolution of the temperature based on the data coming from a single asset. Note how varying the duration of the missing data event affects the interpolation quality. You can also anticipate the start of the (synthetic) missing data event and see if that affects the prediction. Here, we first change the start of the missing data by 36 hours. The prediction fails to mimic the original values in the beginning. Indeed, this type of interpolation might be sensitive to the starting point for the forecasting. Here we used one year of data prior to the missing data event for the training of the ARIMA model. Using a longer period - including multiple seasonal patterns- nonetheless will improve the forecasting and avoid the aforementioned shortcomings. Furthermore, we analyse how strong the duration of missing data influences the quality of the interpolation. When increasing this duration to 24 hours, or put differently, when aiming to interpolate the data for a whole day, the pattern-based interpolation still returns fairly satisfactory results for the temperature values but does not predict the wind speed with sufficiently high quality. Therefore, keep in mind that it is important to understand the data before choosing an interpolation method.","tags":"Time Series Preprocessing","url":"/Time Series Preprocessing/2022-05-06-handling-missing-data.html","loc":"/Time Series Preprocessing/2022-05-06-handling-missing-data.html"},{"title":"Data Preprocessing","text":"Data Preprocessing Welcome to the fourth video of the tutorial for the AI Starter Kit on remaining useful lifetime prediction! In this video, we will explain you how the data needs to be preprocessed such that it can be served as input for a machine learning algorithm. Let's get back to the NASA Engine data and our use case. Before we can start with training a model to predict the remaining useful lifetime, a number of preparatory steps need to be taken. First of all, let us assign which type of problems we are dealing with. What precisely are we interested in? Is it about predicting a categorical value, which can take only a limited set of possible values, or a number? Indeed, it's not so easy to answer. It depends on how we define our use case: We can either ask: How long is the remaining useful lifetime? Then, a real number is looked for, and we deal with a so-called regression task. Alternatively, we can ask: Will the engine fail within the next 50 cycles? Then this boils down to a so-called binary classification task, in which the prediction is ‘yes' or ‘no'. The choice for one or the other task mainly depends on the business question you want to solve. In most cases, a binary classification problem is easier to handle for a Machine Learning algorithm and therefore, this is the question we will answer in this AI Starter Kit. However, the solution methodology is largely similar when you would opt for predicting the remaining useful lifetime using regression. In the previous video, we explained that for classification tasks, labelled data is essential. Therefore, as a first step, we need to create the binary labels: Has the machine failed within a given period of time or not? For the training data, we know that the final cycle per engine id is the time of failure. In order to determine the remaining number of cycles at each time point, first the maximum number of cycles per engine is determined. Subsequently, the current cycle number is subtracted from the maximal number of cycles to arrive at the number of cycles remaining at a particular time point. [Can we visualize this nicely on a screen on an example? à Show a timeline next to the dataframe, annotated with max/number of cycles/labels] We will add this value as a new variable to the data. However, this is not yet a binary label that can be used as input for the classification model we want to train. This is created by determining whether the calculated remaining useful lifetime is smaller than or equal to the threshold N – the period of time we want to predict the failure in. In this tutorial, we will use 30 cycles as a threshold, meaning that we aim to answer the question whether or not the engine will fail within the next 30 cycles at a particular point in time. This binary label can now be used as input value for the classification model. In order to decide on the goodness of the model, the test data needs the same type of label. In that case though, the final cycles per engine does not automatically determine the time of failure. How can we proceed instead? The ground truth data helps in this case. By joining the unlabelled test data and the ground truth data, we know the time of failure. Once done, we can analogously to the training data calculate the binary label for the test data. Don't forget, these labels will only be used for evaluating the model and are not shown to the algorithm beforehand. A second observation that can be made from the training data sample is that the scales of the values differ significantly across columns – both for sensor values and machine settings. This difference in the scale of the numbers could cause problems when the model needs to calculate the similarity between different cycles - namely the rows in the table - during modeling. To address this problem, we will normalize the data. The goal of normalization is to change the values of numeric columns in the dataset to use a common scale, without distorting differences in the ranges of values or losing information. The normalized values maintain the general distribution and ratios in the source data, while keeping values within a scale applied across all numeric columns used in the model. One of the most popular normalization techniques is so-called Min-Max normalization and that is also what we will use here. It scales the values of a variable to a range between 0 and 1 using this formula where 𝑋 represents the value to be normalized, 𝑋𝑚𝑖𝑛 is the minimum value of the variable in that column and 𝑋𝑚𝑎𝑥 is the maximum value for that variable We apply the normalization on both the training and test data set on all sensor measurement and setting variables. We will not rescale the engine id, as it should be seen as a categorical variable. Further, for the cycle variable we keep both the original and the scaled values. In the next video, we will go to the core of this tutorial, namely using a deep learning algorithm to train a model that is able to solve this binary classification problem.","tags":"Remaining Useful Life Prediction","url":"/Remaining Useful Life Prediction/2022-05-05-data-preprocessing.html","loc":"/Remaining Useful Life Prediction/2022-05-05-data-preprocessing.html"},{"title":"Outlier Detection","text":"Outlier Detection In the previous video we explored the data mainly in a visual manner. In this fourth video, we will discuss how to detect outliers in the data and how different methods can be used to improve the data quality. There are several different techniques for outlier detection in time-series. Here, we will focus on online outlier detection, that means the detection of an outlier as soon as it occurs, as opposed to an offline detection, which happens retrospectively. We present two different approaches for outlier detection using temperature and wind speed variables. A relatively simple and frequently used approach for outlier detection is based on the boxplot data distribution. For a given attribute, this method computes its interquartile range or IQR, which is the difference between the 25th and 75th percentiles. This value is then multiplied by a constant factor 𝛼 which determines how stringent the outlier detection is. A typical value for 𝛼 is 1.5, although this value can be adapted according to the level of stringiness desired. Indeed, larger values will push the outlier boundaries further and thereby reducing the number of detected outliers. The resulting value is subtracted from the 25th and added to the 75th percentiles to obtain the lower and upper fences, respectively, which define the thresholds beyond which a given value is labelled as an outlier. Considering the seasonal nature of the data, we should ensure that the outlier detection approach takes the impact of seasonality into account. It is known that the temperature has significant seasonal variation as it varies between day and night or between winter and summer for example and the same temperature in winter and in summer can be considered as outlier in one case, but not in the other. Therefore, the seasonal trend decomposition described in the former video is applied to the signal and the residuals are used as input for the outlier detection. In doing so, we only take the distance to the seasonal pattern into account for the outlier detection. In the example shown on the right, we identify outlier events based on a given 𝛼 value. We define outlier events as outliers that are consecutive in time. An additional input parameter allows to merge outlier events that are separated by less than a given amount of time. When using the most stringent alpha value - in our case – we detect a single outlier in the dataset: namely one of the turbines measured a temperature of -273 degrees for a period of time. Further, we can visualize the time series around the time of a given outlier event. The Flank duration parameter allows you to control the time window around the outlier for the visualization. The left figure shows the time series with the outlier event highlighted in blue, while the figure on the right shows the distribution of all the temperature residual values using a boxplot. Again, the points in blue indicate the outlier event depicted on the left. When increasing the flank duration, we clearly see that this measurement is an outlier. Of course, to identify an outlier with a temperature of -273 degrees, you don't need data science. Hence, We also have a look at another outlier detected with a smaller value for alpha, namely 1.5. In this case, the outlier is not as evident as before. On the 23rd of August, the temperature was much higher than on a normal day in August and warmer than the days before and after. An important aspect we want to discuss in this respect is the influence of outlier detection and removal on normalization. Normalization is a typical data pre-processing step where the range of a variable is standardized, meaning rescaled in order to make different variables with different ranges comparable. It is an important pre-processing step before the data is presented to a machine learning algorithm, as it ensures all variables have equal importance. Different normalization approaches exist. Examples are rescaling the values in the 0-1 range, known as min-max normalization, or removing the mean and scaling to unit variance, known as z-score or standard score normalization. Most of these approaches are sensitive to outliers: For example, in the case of min-max normalization, the minimum value is mapped to 0 and the maximum to 1 so obviously extreme outliers will have a large impact. In the starterkit, we can test these two normalization approaches on each of the attributes of the dataset. We can enable or disable the outlier removal in order to appreciate how this affects the normalization procedure. This effect is most striking when looking at the temperature attribute. If we do not remove the outliers, the min-max normalization is meaningless as 0 is mapped onto -273 degrees. All remaining values are then in the range between 0.8 and 1. When removing the outliers, the range between 0 and 1 is equally dense and the normalization reflects the seasonality in the data. A second approach is the so-called Fleet-based outlier detection. For detecting outliers of the power attribute, we will use this alternative approach. Note, that this is only for the purpose of demonstration and that we could also use the interquartile range-base outlier detection for the power attribute. The approach we will use is based on exploiting the fleet aspect, which is exemplified by wind turbines, which typically operate as part of a wind park in the same environment under similar environmental conditions. At each point in time, we compute the median power recorded by the fleet and we consider any value that deviates too much from that median value to be an outlier. To determine what constitutes too much deviation, we again consider the boxplot outlier definition. If a value is beyond 5 times the IQR from the 25th or 75th percentile, we consider the observation to be an outlier. To exclude periods during which all the turbines in the fleet were not operational, we only consider time points when at least 3 of the 4 turbines recorded a power production above 0. The starter kit allows us to explore all the detected outliers in the power attribute using this definition. We can change the Flank parameter in order to see a larger time window around the outlier. It can be observed that the outliers often happen in periods of time when the power of a given turbine dropped to 0 without it being the case for the remaining turbines. There are, nonetheless, other instances, when the produced power of a given turbine was - statistically speaking – above what would be expected given the behaviour of the remaining turbines. Note that the grey square highlights only the fleet outlier event in question. Other points in the visualization might also be labelled as outliers but they are not part of the same event. The fleet-based approach has the advantage of being able to capture outliers at a specific moment in time only relying on sensor values captured at that time. On the contrary, this approach can only be applied if the dataset contains a fleet of co-located assets, meaning that they are exposed to similar conditions. In the next video, we will discuss how we can impute missing data in the dataset and therewith improve the quality of the data.","tags":"Time Series Preprocessing","url":"/Time Series Preprocessing/2022-05-05-outlier-detection.html","loc":"/Time Series Preprocessing/2022-05-05-outlier-detection.html"},{"title":"Outlier Detection","text":"Outlier Detection Welcome to the fourth video in the tutorial on advanced visualisation. In this video, we will concentrate on outlier detection. In particular, at which days does the people's biking behaviour deviate from the expected one. The visualisations presented in the former videos already allowed us to identify outliers. We saw for example that the traffic at NorthWest 58th Street seems to follow other rules than the other nodes in the network. In this video, we will perform a more detailed analysis of outliers in order to identify them more precisely. This table presents another way of looking at the data. For each node we have 24 observations per day, corresponding to the hourly total number of bike passings through that node. As humans, we cannot easily interpret such a table by just staring at it. However, it is also difficult to visualise such data because it is multi-dimensional meaning that it has 24 values for 6 nodes for thousands of days. However, we can still visualise that data by first performing a so-called dimensionality reduction technique. One of these techniques is Principal Components Analysis or PCA. The idea behind PCA is to find a reduced set of axes that summarize the data. More concretely, PCA is a statistical procedure that transforms the multidimensional data provided as input into a reduced set of orthogonal - and with that uncorrelated - dimensions. These dimensions are called principal components. By using PCA, we can reduce the dimensionality of the data in the above table to two dimensions and plot the result using a scatterplot, as shown here. Scatterplots typically visualise two variables of a dataset along the X and Y axis, respectively. Additional information can also be visualised by changing the colour or size of the dots though. Scatterplots are useful to identify relationships between two variables, such as correlation, and to identify separate groups in the data which can be useful for subsequent clustering. The scatterplot shown here has the shape of an 'L' and seems thus to indicate that there are roughly two groups in the data, corresponding to the two line segments forming the shape of that letter. We can now check in the interactive Starter Kit whether these two groups are present for all nodes. From the above analyses, we might suspect different results for NorthWest 58th Street and 26th Avenue compared to the other nodes. Furthermore, the Starter Kit also offers the possibility to change the colormap indicating the day of the week. What kind of colormap would you use to visualise the different days of the week? Also here, some colorscales work better than others. One the one hand, we could use sequential (YlGn) or diverging (RdBu) colormaps. When applying these to the data, it is hard to distinguish the data from the different days. These colormaps work better for continuous data, such as the heatmaps that we showed in the previous video. For categorical data as in this figure, a qualitative colormap, like colorblind, works much better. As the name suggests, this colormap has the added advantage that it is very readable for people with varying form of colorblindness. When using the colorblind colormap for Fremont Bridge, we can clearly see the L-shape. On top of that, we see that the two arms mainly belong to either weekdays or weekends. The dots corresponding to Fridays seem to be closer to the weekend pattern though. When we select the data from NorthWest 58th Street, we get a significantly different picture. The two groups are not as clearly separated as before, but Saturdays and Sundays typically correspond to points higher in the figure. Getting back to Fremont Bridge, also here a couple of weekdays seem to behave like weekend days. We can observe some of these dots in the upper left part of the plot for all nodes. We suspect these latter dots to be outliers. We can automatically identify them by first applying a clustering algorithm for grouping the datapoints in both of the arms together. Then, we can check that the points in each of these two categories correspond to weekend days or weekdays, respectively. Those which do not follow this pattern can be considered outliers. To cluster and hence separate these points we use a Gaussian Mixture Model. It is a clustering technique particularly suited to oval-like shapes like those corresponding to the two categories we would like to separate. As training data, we only consider a single node at a time, but in general, the model can be trained on the data for all the nodes that exhibit the commute pattern. We only do this for those nodes where the two distinct groups could be identified, so not for NW 58th Street and 26th Avenue. As can be seen from the plot, the Gaussian mixture model is quite good in separating the two groups. With the colours indicating the cluster, we can easily identify the outliers, that is those days that are actually weekdays but that are assigned to the weekend day cluster. The heatmap plot shows the date corresponding to those outliers. By further checking the US federal holiday calendar, we can see whether the resulting dates corresponded to a holiday, in which case the name of the holiday is indicated on the Y axis. We can see that all outliers can be explained: either they are weekdays that are actually holidays or weekdays that are other special days, for example the Friday after Thanksgiving, the days before and after Christmas, and New Year's Eve. Note that not all holidays are identified as outliers, for example Thanksgiving should occur every fourth Thursday of November, but we did not identify it as an outlier in 2016, 2017 and 2018. This might be due to an inaccuracy of the model, which would require a more elaborate preprocessing of the data in order to resolve this, yet this is out of scope of this Starter Kit.","tags":"Advanced Data Visualization","url":"/Advanced Data Visualization/2022-05-05-outlier-detection.html","loc":"/Advanced Data Visualization/2022-05-05-outlier-detection.html"},{"title":"Statistical Data Exploration and Feature Engineering","text":"Statistical Data Exploration and Feature Engineering In the former video, we performed a visual data exploration. We could already gain quite some insights from the figures we showed. In this video, we will concentrate stronger on statistics in order to verify our findings. Finally, we will prepare the datasets for modelling purposes by extracting a number of distinguishing features which will serve as input for the machine learning algorithm. In order to find repetitive patterns in time series data, we can use autocorrelation. It provides the correlation between the time series and a delayed copy of itself. In case of a perfect periodicity as shown in the figure, the autocorrelation equals 1 if we delay one copy by the periodicity. In order to investigate the autocorrelation in case of the global active power, we visualize the autocorrelation with time lags of 1 day, 2 days, and so on. Note that we do not show the results of a delay of 0 days, since the correlation with the exact same day would trivially be 1. For bigger delays, a clear peak occurs after 1 day, indicating the consumption of a day is highly correlated with the consumption of the previous day. And more obviously, several peaks occur every 7 days, which confirms the existence of a weekly pattern. That means that the consumption of a day is highly correlated with the consumption of the same day one or several weeks earlier. In the data exploration video, we further saw that the temperature and the global active power show an opposite trend. In order to verify the strength of this relationship, we can compute the correlation between these two variables. For this purpose, we compute Pearson's correlation. The Starter Kit allows to calculate the correlation between the temperature and the global active power for different resampling rates. Why is this important? One reason is the time scale for changes. The temperature typically changes less drastically over time than energy consumption does. In the figure, on the diagonal, we see the correlation of the two time series with each other, resulting in a perfect correlation of 1 as expected. On the opposite, along the antidiagonal, we see the correlation between the global active power and the outside temperature. In case of a sampling rate of 1 hour, the negative correlation is weak with roughly -0.2. With an increased resampling rate of 1 week, the negative correlation is more evident with a value of -0.75. This confirms our insight from the visual inspection. With a larger sampling rate, the short-term fluctuations – for example from your washing machine – are smoothed out and leaves us with the seasonal pattern. From these insights, we can now start extracting features from the extended dataset that we can use later on for the electricity consumption forecasting. A feature is an individual measurable property of the phenomenon being observed. Choosing informative, discriminating, and independent features is a crucial step for building effective machine learning algorithms. Based on the previous visual and statistical observations, we will consider the following features to predict the energy consumption for the hour ahead: energy consumption of the day before because we saw that there was a peak in the autocorrelation with a delay of one day; energy consumption of the 7 days before due to the weekly pattern; the hour of the day, which will allow to distinguish between different periods of the day (i.e., night, morning, afternoon, evening); the day of the week, which will allow to distinguish between weekdays and weekend days whether the day is a holiday or not; the month of the year, which will allow to distinguish between yearly seasons; and finally, the temperature. That results in the final dataset that will be used as input for the machine learning algorithm to learn the model. Before discussing the modelling step in more detail, in the next video, we will provide a theoretical overview of the approaches chosen for the electricity consumption forecast. Additional information The video material in this website was developed in the context of the SKAIDive project , financially supported by the European Social Fund , the European Union and Flanders. For more information, please contact us at elucidatalab@sirris.be","tags":"Resource Demand Forecasting","url":"/Resource Demand Forecasting/2022-05-05-statistical-data-exploration-and-feature-engineering.html","loc":"/Resource Demand Forecasting/2022-05-05-statistical-data-exploration-and-feature-engineering.html"},{"title":"Deep Learning Theory","text":"Deep Learning Theory Data Science, Machine Learning, Deep Learning, Artificial intelligence… These words are often used interchangeably in different contexts and not always precisely. Therefore, before we continue with our particular use case, we will provide you with a brief introduction into the difference between \"traditional\" machine learning methods and Deep Learning-based techniques. Subsequently we will introduce you to the main neural network types that are used in deep learning, and explain you we selected the appropriate type for use in the context of remaining useful lifetime prediction. When starting on a data science problem, you usually need to start with finding the answer to a number of questions before starting the hands-on work on the data. Without understanding the use case, even the best data science model will not help. In most cases, a particular business problem can mapped onto one of the following data science tasks. A first question to answer in that respect is which type of outcome is expected from the use case owner. Is the aim to predict a category, such as ‘normal', ‘degrading' or ‘failed'? If yes, the next question to answer is whether labelled data is available or not. Labelled data is data for which examples are available that are annotated by a domain expert with one the classes to predict. Put differently, for each data point or set of data points, a class is defined. Usually, the number of unique classes is rather small. This data will be used by the algorithm for training a model. Once trained, it can be evaluated on a test data set, for which the classes are known but will not be visible to the model. Evaluating the ratio of correctly predicted classes gives a measure of the quality of the model. Often used algorithms for classification are k-Nearest Neighbors, Decision Trees or Support Vector Machines. But what can we do if we don't have information available on possible classes? In that case, introducing a similarity between the examples that you have available makes it possible to cluster different data points into groups. These groups can then be used to gain deeper insights into the data, and in some cases can be mapped to particular classes. Partition-base clustering, hierarchical clustering or density-based clustering are often used techniques for this purpose. The situation is different in case a numerical value should be predicted. It is similar to the classification task, but the prediction range is continuous. For these so-called regression tasks, Ordinary Least Squares or Support Vector Regression are often used. If the goal is neither to predict a class nor a numerical value, but rather a future state, one typically turns to graphical modelling algorithms order to predict these states. These techniques also allow one to include the available background knowledge into the modelling process. Examples of graphical modelling techniques are Hidden Markov Models and Bayesian Networks. To make the difference between the single categories a bit clearer, we discuss some examples: In case no labelled data is available, clustering of data points can provide insights in different modes in the data. An example is a performance benchmarking of industrial assets. Let's assume the data to analyze comes from a wind turbine park. When looking at several measurements, like for example of the power curve, the wind speed, and the wind direction, we can identify different modes in which the single wind turbines are operating. In contrast, assume that we are interested in the expected power production of a particular wind turbine in the following days for which we have a weather forecast. We can use this information as input variables for a regression model and therewith predict the power production. If labels are attached to the gathered data, for example on the root cause of particular sensor failures, a classification algorithm can be used to train a model that is able to determine which fault is associated with a certain set of sensor readings. When next to the data also background knowledge from a domain perspective is available, graphical models can for example be used to diagnose the reason behind a particular fault in industrial assets such as wind turbines. How do the above examples differ from Deep Learning and when does it make sense to use the one or the other? One of the major constraints in traditional machine learning is given by the fact that domain knowledge is usually needed in order to perform proper feature extraction. Consequently, feature engineering is typically a manual and time-consuming task, requiring the necessary domain knowledge. There is hardly any case in which a model can be applied to the data as it is and receive good results. In a number of cases, however, the domain knowledge on a particular problem setting is limited or the machine or process generating the data is highly complex, making it hard for an expert to manually come up with useful characteristics that can be used as features for a machine learning model. This is where deep learning comes into play. In deep learning, the feature extraction and selection step is basically performed by the algorithm itself. The technique is inspired by the neural networks in human brains. It should learn by itself how to combine the different neurons in the single layers in order to obtain the best results. The learning is performed by the adjustment of the weights between the neurons in single layers. In the figure, these weights are symbolized by the arrows going from one neuron in one layer to another neuron in the next layer. The final result is given by the output in the output layer, indicated in green in the figure. The textbook example is the classification of images of animals, to discern cats from dogs. Nowadays there is a whole spectrum of different model architectures and flavours of neurons depending on the type of task that is supposed to be solved. The choice for a particular type of network typically depends on the problem you want to solve, the characteristics of your input data and the availability of particular domain knowledge. All these factors influence amongst others the complexity of the network, its interpretability and the learning speed. One of the simplest models is a so-called feedforward neural network as shown in the figure. As the model learns by adjusting the weights between the layers, the weights can be seen as its memory. While such models perform well for quite a number of problems, one downside is that their memory is rather static, due to which they have a hard time in ‘remembering' patterns that happened a long time ago. Suppose that you need to learn a model to predict the sales of Christmas trees. It might be clear that hardly any Christmas tree will be sold in summer, and the main indicator for the sales this year will be the sales figures of the previous years in the month before Christmas. Remembering this long-term information is not easy for a feedforward neural network. One way to overcome this problem is the usage of so-called Long Short-Term Memory - or LSTM in short - networks. As the name already indicates, LSTM networks are able to adapt their memory in a more precise way than feedforward neural networks, and in this way are better able in deciding which information they need to remember for a longer period of time. LSTM networks are often used to solve tasks such as unsegmented, connected handwriting recognition, speech recognition and anomaly detection in network traffic or intrusion detection systems. Also in the field of remaining useful lifetime, the implementation of LSTM networks is reasonable as we deal with data for which the change over time of several variables is decisive. Our interactive starterkit offers the possibility to build your own LSTM model. It allows to gain a deeper understanding of the different parameters and examine how they influence the prediction result.","tags":"Remaining Useful Life Prediction","url":"/Remaining Useful Life Prediction/2022-05-04-deep-learning-theory.html","loc":"/Remaining Useful Life Prediction/2022-05-04-deep-learning-theory.html"},{"title":"Resampling, Smoothing and Seasonal Patterns","text":"Resampling, Smoothing and Seasonal Patterns Welcome to the third video of the tutorial for the AI Starter Kit on time-series pre-processing! In the previous video, we already had a look at the temperature values and saw a clear seasonal pattern. For active power and wind speed however, it's not so easy to answer if particular patterns are present because the data is noisier. In general, visualizing time-series data with a high temporal granularity might make it difficult to interpret and detect underlying patterns. Resampling techniques allow to reduce the temporal granularity, thereby revealing longer-term trends and hiding sharp, fast fluctuations in the signal. In the interactive starter kit we can select the amount of resampling by specifying the time unit and the number of time units to resample to. For example, we can resample the data to weekly values. For this, we set the time unit to \"Week\" and the number of time units to 1. Playing with these two inputs, we see how the level of detail of the time series changes and how that influences the insights that we can derive from it. In a first step, we select a small resampling factor, for example a daily resampling. We observe that the visualization of the temperature shows a seasonal pattern in the data, even though strong stochastic fluctuations dominate the overall picture. At this level of granularity, the visualization of the wind speed is not at all interpretable due to these fluctuations, especially when visualizing over such a long period of time. Specifically, if we are interested to visualize the long-term trend of the time series, we need to reduce the sampling rate. To this end, we need to increase the resampling rate to a weekly resampling. We then see how these high frequency patterns disappear and it becomes possible to analyse some long-term fluctuations in the power and wind speed variables. You can notably observe the strict correlation between the patterns of wind speed and active power, which might be obvious: the more wind, the higher the power that can be generated by the turbine. Note also that by increasing the resampling of the time series too much, most of the information it contains is discarded. It is also important to understand that resampling aggregates the data for the specified period. Hence, we need to specify how we want the data to be aggregated. For the wind speed and temperature data in our example, we can opt to aggregate the data using the median of the values within the selected period. Like this, we level out very high or very low values. Therefore, this statistic is quite robust against outliers that could be present in our dataset. For other quantities, we might consider other statistics. For example, if we were considering power production, the interesting value is the power produced per day or week. Consequently, it makes more sense to sum all the data samples rather than to average them. Does the sum function also make sense for the temperature or wind speed? Think about it and then try different resamplings by changing the aggregation function or resampling units used. As we can see, a simple resampling makes additional details explicit, and the degree of resampling allows drawing different insights. For example, with a weekly sampling rate the temperature plot still shows a clear seasonal pattern. However, we can also see that in each year the weekly temperature evolves in a different manner: the highest and lowest temperatures are located in different weeks. Further, the wind speed also seems to follow a seasonal pattern, albeit a less explicit one. We can notice that the wind speed is generally higher in winter, yet its evolution is much more dynamic than the one of temperature. Finally, the power production closely follows the wind speed profile, consistent with the latter being its main driver. Important to keep in mind when resampling your own data is that it can help to find seasonal patterns but that it can also hide patterns when the scale is too big. To make this more tangible, imagine that you resample the data to an annual scale. In our example, you will not see the seasonality anymore. When you perform resampling on your own data a domain expert typically can support you to find a good scale by reasoning about the underlying phenomenon from which the data originates. In many data sources, stochastic fluctuations can be seen on different temporal scales. Some of those might be small variations that may not be significant for what you want to detect. These types of fluctuations can be real, such as, due to sudden decreases in temperatures between summer nights, but can also be caused by inaccurate measurements of the sensor, for example. If such details are not important to consider for your analysis, you can remove them by smoothing. In our interactive Starter Kit, we explore three different smoothing approaches, namely the rolling window algorithm, Gaussian smoothing and Savgol filters. The easiest approach is the rolling window algorithm: The user defines a small, fixed period – the window. The algorithm runs over the data taking into account the consecutive time points covered within the window, and replaces each time point by an aggregated value computed within this window. Typically, the aggregation follows the mean or the median value. This has the effect of reducing short-term fluctuations while preserving longer-term trends. When using Gaussian smoothing, the method summarizes the values over a sliding window, just as the rolling window algorithm. But instead of calculating the mean or median value, a Gaussian function is used. The size of the sliding window is specified by the standard deviation of this Gaussian function, which is called a kernel. With this method, the values closest to the centre of the sliding window will have a stronger impact on the smoothing process. The impact of the remaining values is a function of the distance to the window centre. Finally, the Savgol filter is a popular filter in signal processing and is based on a convolution approach. It fits a low-degree polynomial to successive subsets of adjacent data points via the linear least-squares method. Note that also here a window is defined. In our Starter Kit, we use a polynomial of degree 2. Now that we know the general idea of smoothing, We can experiment with these different approaches in the interactive starter kit. At the top of the graph, we can select one of the explained methods. Using the time unit controls, we can explore how the level of detail resulting from the smoothing varies for different window sizes. Can you strike a good balance between too much and too little detail? This is a difficult question to answer, as it strictly depends on the goal of the analysis. Smoothing using a large time window, for example, several days, can be useful to detect slow, long-term changes to the time series. On the other hand, a shorter time window, let's say 12 hours, can be very efficient to analyse short-lasting changes in the signal by getting rid of fast transients in the signal. There are also some significant differences in the presented methods. If you chose the rolling median method and zoom in, you will be able to see that the signal appears 'shifted'. This is due to the fact that the rolling window implementation of the Python pandas library that was used, uses a look-ahead option, where the value at any given point in time is given by a time window ahead of it. Another interesting aspect is how the Gaussian method creates a signal that is characterized by smooth 'Gaussian-like' hills, a consequence of its Gaussian kernel-based nature. In sum, the Savgol and rolling window approaches create a signal that more closely follows the original signal. This is due to the fact that both operate using a local estimation approach: the latter method aggregates values over a local time window, while the former applies a least-squares regression over the specified time window. We previously discussed the seasonal patterns that can be observed in certain variables such as temperature. One technique that allows analysing the seasonal patterns in a time-series signal and to decompose the signal into different components is called Seasonal Trend Decomposition. This technique identifies cyclical patterns in the signal and decompose it into 3 components: First, the trend, which summarizes the long-term trend of the time-series in the considered time frame. A second component is the seasonal component: this is the part of the signal that can be attributed to repeating patterns in the signal. Finally, the residuals are the \"left-overs\" after subtracting the trend and the seasonal factors from the original signal. This is the component of the time-series that cannot be attributed to either the long-term trend evolution of the signal or the seasonal patterns. In the starter kit, we can analyse the seasonal trend decomposition over two periods: a seasonal period that covers the four yearly seasons and a daily period. If we select a seasonal decomposition over a sufficiently large period, we can clearly observe the yearly temperature pattern, with higher temperatures in the summer months and lower ones in the winter months. If you test a daily decomposition over the same period of time you will need to zoom-in on a short-time range and similarly see a pattern in the seasonal component, namely the 24-hour temperature cycle. You can also test how the seasonal trend decomposition performs on a less cyclical signal, such as power production. Indeed, since power production is mainly driven by the amount of wind and wind speed, each showing a weak seasonal modulation, power production can only be poorly expressed in terms of its seasonal component. In the next video, we will discuss how outliers can be detected in seasonal data with stochastic fluctuations.","tags":"Time Series Preprocessing","url":"/Time Series Preprocessing/2022-05-04-resampling-smoothing-and-seasonal-patterns.html","loc":"/Time Series Preprocessing/2022-05-04-resampling-smoothing-and-seasonal-patterns.html"},{"title":"Temporal and Seasonal Visualisations","text":"Temporal and Seasonal Visualisations Welcome to the third video of of the tutorial for the AI Starter Kit on advanced visualisation. In this video, we will concentrate on how to visualize time series data in the best way in order to underline seasonal effects. For this, we will introduce different kinds of visualisations and explain what are their strengths and weaknesses. Since we are dealing with time series data, we can first use a timeline visualisation and plot the average number of crossings per hour and node over time. Such visualisations are on the one hand useful to observe abrupt changes in the data but on the other hand, also to identify time-sensitive trends, such as seasonal variation, increasing or decreasing trends, etc. Further, they make it easy to recognize periods with missing data. The visualisation on the right just plots the total number of crossings per hour for each node. However, plotting the available data in this way results in a visualisation containing an excessive amount of information. Note for example that data of some nodes hides data of other nodes, a phenomenon called occlusion. The massive amount of information makes it difficult to identify interesting observations in the data such as possible seasonal patterns. In order to resolve this issue, it is possible to resample the data to a lower frequency like for example daily or weekly crossings. In the interactive Starter Kit, this can easily be done by changing the resampling rate in the corresponding dropdown box. The seasonal pattern is very clear when using monthly resampling, while it is not as clear when using daily and yearly resampling. For the former, the day-to-day variation makes it harder to see the average trend, while for the latter the seasonal patterns occur over a shorter time period than the time window we use for resampling. Using a monthly resampling we can verify some of our former assumptions: first of all, Fremont Bridge clearly is the most popular node. A seasonal pattern is clearly visible for Fremont Bridge, Spokane St, and 2nd Ave, where crossings seem to increase in summer and decrease in winter. Such a seasonal pattern might also be present for other, less popular nodes. Large differences in the number of crossings per hour and therewith the overall scale, make it hard to identify patterns for these lesser frequented nodes. We can make the data comparable across nodes by normalising it, that is by changing its numerical values in such a way that a common scale is used without distorting differences in ranges and without losing information. In the interactive Starter Kit, you can choose between two different types of scaling. You can either apply standard scaling - also called Z-score normalisation or standardisation - in which numerical variables are rescaled to have the properties of a standard normal distribution with zero mean and unit variance. Alternatively, you can choose to apply a Min-max normalisation, in which the data points are rescaled such that the minimum and maximum value correspond to 0 and 1, respectively. Both types of scaling methods have their advantages and disadvantages. The discussion on this lies beyond the scope of this Starter Kit. For the Min-max normalisation, all values are mapped in the range from 0 to 1 which makes it easy to compare the overall pattern of the single time series. We can see for our data that most of the crossings reach their corresponding maximal values in the summers of 2014 and 2018. One extreme outlier in this regard is the crossing at 26th Avenue. It was mainly frequented in the summer of 2014 but for the remaining years the values were much lower. As these extreme values are mapped to 1, it is hard to identify any pattern for the years after 2014. With the standard scaling, this problem still occurs as in this case the standard deviation dominates the maximal values. On the other hand, it makes it easier to compare the underlying distribution of the data. We observe that the distributions of most of the streets are very similar. This gives us a hint that the overall behaviour at the single nodes is similar with respect to the seasonal pattern. Note, that a standard scaling only makes sense for data with an underlying Gaussian distribution. An alternative to timeline plots is a heatmap plot. A heatmap is a visual representation of data that uses different colours to indicate varying intensities of values. It is useful to visualise large amounts of data and also, to reveal spatial or temporal patterns that are much harder to spot in numerical data. Many different types of heatmaps exist, like for example matrix heatmaps, calendar heatmaps, geographical heatmaps, or circular heatmaps. In this tutorial, we will start with a calendar heatmap to visualise time series data on a calendar view. The visualisation consists of a series of matrix heatmaps, that is, heatmaps where data is presented in a matrix-like form. In the case of our data, each year is plotted on a different heatmap row, starting from 2012 at the top and finishing with 2018 at the bottom. The columns in each individual heatmap row correspond to individual weeks within a year. The columns are in turn vertically divided in 7 rows, each corresponding to a different day of the week such that each square corresponds to a single day. The colour intensity of each square represents the total number of crossings for that day, which in this case are aggregated per day across all nodes. This type of visualisation provides several interesting insights: as the timeline visualisations already revealed, it confirms that bike traffic seems to follow a seasonal pattern. This easily becomes visible as the winter period, November to May, has lighter colours each year than the summer period, from May to November. Further, we see, that the last two weeks of the year are always less crowded, which is probably due to the holiday season. Regardless of the specific year and season, weekdays always see more cyclists than weekend days. This becomes visible as the last two rows in each matrix heatmap are always lighter than the top rows. On the basis of this visualisation, we would also like to discuss the importance of colormaps. In most data science projects, the colormap gets the least attention. But for this example, we can see that not all colormaps work equally well. For example, using a circular colormap like hsv does not intuitively indicate days with low and high numbers of crossings as the map starts and ends with red-like colors. RdBu does a better job in this respect, but it does have another issue. This is a so-called diverging colormap, which has a naturally implied zero-point. This type of colormaps works very well for data with positive and negative entries. In our case though, no natural zero-point in the middle is given such that with this colormap blue days have higher values and red days have lower values. Sequential color maps that continuously change from one color to another like YlGn work much better for this kind of data. Keep in mind though, that we all might associate different colours with different values. For this, let's have a look at a continuously changing colormap from red to green. Most people will begin to wonder when they first see the plot. We usually associate red with bad, dangerous or under-performing datapoints while green implies the opposite. This is not the case here though. On top of that, this type of colormap might also give problems to people with red–green color blindness. To summarize, in this particular example, changing from a less intense, lighter colour for low values to a darker, more intense colour for high values, grants the best interpretability. Similarly to the calendar heatmap over the full year, we can drill down further and look at whether we can observe some hourly patterns. Let's first investigate a single node, for example Fremont Bridge, and use a matrix heatmap in which each day of the week is represented as a row and each hour of the day is represented as a column. As before, the colour intensity indicates the amount of traffic for that day and hour. Note that such a heatmap requires summing the total traffic for each day per hour. This plot reveals the following insights: first of all, it confirms the different daily patterns for weekdays and weekends. For this, please have a look at the last two rows of the plot representing Saturdays and Sundays. They are quite different from the top 5 rows representing Monday to Friday. There is a clear bimodal pattern for weekdays, with heavier traffic in the morning and evening. This is not visible for weekend days but rather some increase in the midday hours. This bipolar pattern probably relates to people using their bikes to get to work and back home. This is in contrast to the weekend pattern, corresponding to recreational traffic, that is people biking for pleasure in their free time. In order to check whether we can see this commute pattern for all nodes, we can use the small multiples technique. A small multiple is a series of graphs of the same type, using the same scale, presented together at a small size with minimal details. The graphs in this view show different partitions of the same dataset, which makes it very easy to compare these to each other. We plot small multiples for the matrix heatmap, using the different nodes to partition the data. Keep in mind that you should not compare the brightness of the colours between different graphs, as we did not normalise the data beforehand. We can distinguish the commute pattern that we discovered previously, - though with different gradations – in all nodes except for the node on 58th Street. On that node, the graph depicts a different pattern, with a significantly higher number of bikes in the weekend around noon in comparison to weekdays, maybe due to local habits or comparably lower commuting on weekdays. We can investigate the commute pattern even further by taking the direction of the cyclists into consideration. For this, we will use a variant of an area plot - called a streamgraph – in which data is displayed around a central axis. In the present case, we will display the average number of crossings per hour at a node around the X axis, in order to discriminate traffic between the two directions. We will plot one direction above the X axis and the other direction below. The X axis itself will represent the time expressed as hours in a day. The plot presents streamgraph plots using the small multiples technique for the different days of the week. In the interactive Starter Kit, it is possible to select the different nodes, as well as to select the month for which to show the data. This plot clearly confirms that the pattern we observed is a real commute pattern for all nodes - except for NorthWest 58th Street. During the week, more cyclists are going through one of the two directions in the morning hours, whereas in the afternoon more cyclists are returning through the other direction. This pattern is absent for weekend days. NorthWest 58th Street does not have a morning and evening peak or a different pattern for weekdays versus the weekend. It looks more like the weekend days of other nodes for every day of the week. Note that this street is far from the city center, which may explain the absence of biking commuters. For the locations with a bipolar pattern, it is also interesting to investigate the difference between summer and winter. In summer, when the weather is rather good, the peaks in Spokane Street are located at over 150 crossings per hour. On the contrary, when plotting the same graph for winter, the maximal values are at 75 people per hour only. This indicates that still some people commute in winter per bike, but that some change to different means of transportation. In this video, we have seen how we can explain some of the habits of people of Seattle - and this only by means of smartly chosen visualisations. In the next video, we will show how we can use visualisations for outlier detection.","tags":"Advanced Data Visualization","url":"/Advanced Data Visualization/2022-05-04-temporal-and-seasonal-visualisations.html","loc":"/Advanced Data Visualization/2022-05-04-temporal-and-seasonal-visualisations.html"},{"title":"Visual Data Exploration","text":"Visual Data Exploration Welcome to the third video of the tutorial for the AI Starter Kit on resource demand forecasting! Let us now explore the extended dataset in order to understand which factors influence the household energy consumption. Based on this exploration, we will later on identify which features are useful for training a machine learning model to forecast energy consumption. Time plots are a common way for exploring time series data, such as the energy consumption that we consider here. We explore time plots in different time frames in order to highlight different seasonal patterns in the data. We start with a yearly plot, which allows us to identify global patterns. Then, a monthly plot allows us to zoom in and understand relationships between energy consumption in different weeks and days along the year. And finally, a weekly plot allows us to zoom in even further and identify daily and hourly patterns. We start with the yearly plot. The data ranges from January 2007 to end of 2010. We can easily see the yearly pattern for the global active power with peaks in winter and valleys in summer. With these settings though, we see the temperature values only as a flat line because they are about two magnitudes smaller. When only displaying the temperature, we see a similar pattern with high values in summer and low values in winter, as expected. In order to make the two-time series comparable, we normalize the data, such that the maximal value of each time series corresponds to 1 and the lowest one corresponds to zero. Like this, we can see that the two show an opposite evolution over the year. For the monthly data, we progress analogously. While we can again clearly see the annual effect, there is no clear pattern recurring every month. One interesting thing that can be observed is that strong dips in power consumption tend to happen at the same time in February and November in 2007 and 2008. Most likely, these are holidays which tend to happen at the same time for the household under investigation. This can be best seen when the sampling rate is changed. Instead of showing the data per day, we can have a look at the rolling mean of the active power over three days to see the pattern more clearly. We further explore the evolution of the energy consumption in a shorter time frame. We want to verify whether during a week, the energy consumption exhibits the same patterns. In the following plot, the user can inspect and compare the energy consumption in December of different years. We mark weekends in yellow and Christmas day in orange. What can you notice when you compare weekdays and weekends? And how about the holidays? Exactly, the mean consumption is higher on the weekends. The reason might be that more members of the household are at home during the weekends. Comparing the week around Christmas 2008 and 2009 shows interesting patterns. While the household was obviously at home and maybe even with guests in 2009, it looks like they were not around in 2008 for several days. Finally, we analyze the daily pattern. For this, rather than showing the resampled dataset, we use the original dataset, which provides consumption with a sampling rate of 1 minute and which can show more detailed patterns. Note that this means we are using the original units of kW. In the figure, we see how the power consumption follows a daily recurring pattern, with peaks in the morning and evening and troughs at night and noon – on weekdays. The 5th of December was a Saturday, and we can clearly see how the curve of that day deviates from the weekdays. During the night, we still see a low and more or less constant power usage, most likely due to appliances like refrigerators that run continuously. In the next video, we will discuss the statistical significance of the patterns detected and prepare the data for machine learning modelling by extracting meaningful characteristics or features. Additional information The video material in this website was developed in the context of the SKAIDive project , financially supported by the European Social Fund , the European Union and Flanders. For more information, please contact us at elucidatalab@sirris.be","tags":"Resource Demand Forecasting","url":"/Resource Demand Forecasting/2022-05-04-visual-data-exploration.html","loc":"/Resource Demand Forecasting/2022-05-04-visual-data-exploration.html"},{"title":"Data Preprocessing","text":"Data Preprocessing Welcome to the second video of the tutorial for the AI Starter Kit on resource demand forecasting! In this video, we will detail the dataset that we will use and perform the necessary preprocessing steps in order to prepare the data for further analysis. In this AI Starter Kit, we will work with a publicly available energy consumption dataset from the UCI repository. This dataset contains measurements of electric power consumption in one household, located near Paris, with a one-minute sampling rate over a period of almost 4 years. Different electrical quantities and sub-metering values are available, as you can see in the table. Let us first have a look at the single variables given in the table: First of all, and most importantly, the global active and reactive powers are given. The active power specifies the amount of energy the single electric receivers transform into mechanical work and heat. This useful effect is called 'active energy'. On the contrary, several receivers, such as motors, transformer, or inductive cookers need magnetic fields in order to work. This amount is called reactive power as these receivers are generally inductive: This means, they absorb energy from the network in order to create magnetic fields and return it while they disappear. This results in an additional electrical consumption that is not useful for the receivers. Further, the voltage and current intensity are given in Volts and Ampere, respectively. Finally, the submeterings provide us a more detailed insights where the power is consumed. Submetering 1 corresponds to the kitchen, containing mainly a dishwasher, an oven, and a microwave. Submetering 2 corresponds to the laundry room, containing a washing-machine, a tumble-drier, a refrigerator and a light. Lastly, submetering 3 corresponds to an electric water-heater and an air-conditioner. We expect, that the different submeterings obey different patterns: While an oven mainly consumes power during lunch and dinner time, a refrigerator has a more or less constant consumption during the entire day. As mentioned before, we will also take weather measurements into account. For this, we use measurements recorded close to the city of Paris, where the household is located, provided by the Wunderground website. It contains information on climate conditions on a 30-minutes base. The dataset contains the outside temperature and the dew point in degrees Celsius, the air humidity in percentage, the pressure at sea level in hPa, the visibility in km and the wind direction expressed in degrees. The weather data is measured by a set of connected sensors. For these, it is rare that the data availability is uninterrupted for such a long period. For this reason, we first check the general statistics for the data set. For most variables, the minimal value is -9999, which indicates missing values as also specified in the documentation of the data. We will replace these with so-called 'Not a Number' values or NaNs such that they will not interfere too much during data modelling. In a next step, we fuse the electricity data and the weather data. Note, that the temporal resolution of the two is not the same – the power data is measured every minute while the weather information is available only every 30 minutes. The integration of these two datasets thus requires careful consideration. Two options are possible: we can either upsample the weather data or downsample the household data. That means that we either linearly interpolate the information from weather data on a one-minute interval or aggregate the power data on bigger intervals. In this use case, the latter is more reasonable, as otherwise, we would need to find a way to impute missing data in the climate dataset. Let us first analyze the effect of downsampling the power data. Therefore, we calculate the rolling mean values of the original power over a given time window. You can choose between a sampling rate of 30 or 60 minutes, or 4 hours. The light blue time series shows the original data, the dark blue one the downsampled one. In all cases, the data looks smoother as the original data, as taking the mean value smooths out the major short-time power peaks but at the same time results in some loss of information. With the larger window size of 60 minutes this effect is stronger than for the smaller one. A window size of 4 hours arguably removes too much variation. Note that the weather data is available every 30 minutes such that in case of an hourly time window for the power data also the weather data has to be resampled. Since the information loss for the 60-minute interval is reasonable, we decide to proceed with this sampling rate. This also allows us to reduce the size of the dataset, which will reduce the computation time required for the analysis. In order to merge the two data sets, we transform the active power from a minutes-based power measurement given in kW to an hourly consumption given in Wh. Therefore, we sum the power over one hour and divide it by 60 to get the actual energy. By additionally multiplying by a factor of 1000, we change units from kWh to Wh. For the weather data, we proceed similarly by taking the mean temperature over one hour. With this, we can join the two data sets. For future analysis, we will only take the global consumption and temperature into account. Now that we have preprocessed the dataset, we will illustrate how to gain deeper insights in the data through both visual and statistical data exploration. In the next video, we will start with the visual exploration by means of time plots. Additional information The video material in this website was developed in the context of the SKAIDive project , financially supported by the European Social Fund , the European Union and Flanders. For more information, please contact us at elucidatalab@sirris.be","tags":"Resource Demand Forecasting","url":"/Resource Demand Forecasting/2022-05-03-data-preprocessing.html","loc":"/Resource Demand Forecasting/2022-05-03-data-preprocessing.html"},{"title":"Data Understanding","text":"Data Understanding Welcome to the second video of the tutorial for the AI Starter Kit on time-series pre-processing! In this video, we will detail the dataset that we will use and perform an initial data exploration. The time series dataset that we study in this Starter Kit is generated by the SCADA system of a set of wind turbines. SCADA stands for supervisory control and data acquisition. In modern turbines, such a data acquisition system can easily contain more than 100 sensors that keep track of amongst others temperature, pressure data, electrical quantities like currents and voltages, and vibrations. It is commonly used for performance monitoring and condition-based maintenance. The dataset originates from 4 wind turbines located in the Northeast of France. It spans a period of 4 years with a sampling rate of 10 minutes. Although the original SCADA system records statistics for more than 30 sensors, we will focus our attention only on wind speed, wind direction, temperature as well as the power that is generated by each of these turbines to illustrate a variety of time series pre-processing techniques. The table on the right-hand side shows an excerpt of the data, with the following attributes: The column Date_time with the timestamp of the measurement, in 10-minute increments the identifier of the turbine Power, which is the active power measurement in kW as effectively produced by the turbine. the outside temperature measurement in degrees Celsius the wind speed measurement in meters per second And finally the wind direction measurement in degrees The top rows show the average values for each turbine for the defined range. Note that in the case of wind direction, this is the circular average, which takes into account the circular nature of the data. That means that values are bound between 0 and 360 degrees, in which 0 degrees is identical to 360 degrees. In our interactive Starter Kit, you can define a range of values for a given attribute and see how the values in the remaining attributes change. For example, we can increase the wind speed to more than 17m/sec and see how that affects power production. Can you already spot any unexpected values in the dataset? Indeed, in the marked area in the figure, the values are significantly below those seen at other times. You can experiment yourself how the other variables influence the active power in the interactive Starter Kit. Now that we know which variables were measured, let's check some statistics. From the table we learn that in total 4 years of data are available, namely from January 2013 to the end of 2016. Further, we see that the number of data points per wind turbine differs. Most data points are available for the first turbine in the column, while the second turbine collected roughly 1000 data points less. This indicates already that for this latter turbine some data points are missing. In the interactive Starter Kit, we can plot the values of a given variable in time to see how their long-term trend looks like. In the graph shown at the right, we adjusted the timeframe by selecting it directly in the graph, such that also shorter-term fluctuations become visible. By clicking on the turbine names at the right-hand side of the graph, we can select a subset of the turbines. In the drop-down menu at the top of the graph, the variable to plot can be selected. What can we learn from this visual exploration? Let's first have a look at the temperature. Here we can easily spot the seasonal pattern due to summer and winter. Is this behaviour similar for the active power? Is there something unusual you can observe in the temperature values? We will come back to the answers to these questions later on in this Starter Kit and introduce you to a number of techniques to automatically detect this. In the next video, we will first concentrate on the smoothening and so-called resampling of the data points. See you there.","tags":"Time Series Preprocessing","url":"/Time Series Preprocessing/2022-05-03-data-understanding.html","loc":"/Time Series Preprocessing/2022-05-03-data-understanding.html"},{"title":"Data Understanding","text":"Data Understanding Welcome to the second video of the tutorial for the AI Starter Kit on remaining useful lifetime prediction! In this video, we will detail the dataset that will use and perform an initial data exploration to extract some first insights. In this AI Starter Kit, we will work with a publicly-available dataset from NASA. The data simulates run-to-failure data from aircraft engines. These engines are assumed to start with varying degrees of wear and manufacturing variation - but this information is unknown to the user. Furthermore, in this simulated data, the engines are assumed to be operating normally at the beginning and start to degrade at some point during operation. The degradation progresses and grows in magnitude with time. When a predefined threshold is reached, the engine is considered unsafe for further operation. In other words, the last operational cycle of the engine can be considered as the failure point of the corresponding engine – meaning that the remaining useful lifetime has decreased to zero. Before we can start with learning an actual machine learning model, it is crucial to understand the data itself. The data set consists of multiple time series with the \"cycle\" variable as time unit. For each engine, identified by the variable \"id\", a different number of cycles is captured as not all engines fail at the same time. Per cycle, the following information in gathered: On the one hand, 21 sensor readings given by the data points s1 to s21. On the other hand, additional information about the machine settings, given by setting1 to setting3. In machine learning experiments, a dataset is often split in a training set and a test set. This split allows one to quickly evaluate the performance of an algorithm. The training dataset is used to prepare a model, to train it. For evaluation, the test dataset can be understood as new data that is presented to the algorithm. It was not seen by the algorithm before and therefore the outcome is unknown. In our example, it is data from different engines for which it is unclear when they are going to fail, or put differently, what their remaining useful lifetime is. For the purpose of evaluation, the information about the actual failure of the test data set is collected in the so-called ground truth data. This information will not be visible to the algorithm but will only be used for calculating the quality of the model. Now let's have a look at the single sensor measurements. We see that the value range of the single measurements are quite different, without knowing in detail what they correspond to in a sense of physical measurement. In the graph, we see the first 50 entries of time series data collected from three different sensor channels for engine 18. All three show some fluctuations but no clear deviation from a mean value indicated by the gray dotted lines, that could be a sign of degradation in engine performance, are visible. With increasing observation time, all three time series deviate more or less strongly from the mean values observed in the first 50 data points given by the gray horizontal line, indicating the start of the degradation process of the engine. For different engines, the deviation starts at different times. For engine 18, the deviation starts approximately at time 100. For engine 31 though, hardly any deviation is visible in the same time range. Only when increasing the time range, the clear deviation becomes evident. The different starting points of degradation for the single engines indicate that the simulations are made for engines with different wear. In the next video, we will go into more detail into the data preprocessing phase, explaining how the data needs to be prepared such that it can be served as input for a machine learning learning algorithm. If you are not familiar with deep learning, we recommend you to first watch our introductory video on this topic, in which we discuss the difference between ‘traditional' Machine Learning algorithms and Deep Learning techniques. We will also provide a brief introduction to the different type of neural networks, amongst others the so-called Long Short-Term Memory networks or LSTMs for short, which is the type of network that we will use to solve this problem.","tags":"Remaining Useful Life Prediction","url":"/Remaining Useful Life Prediction/2022-05-03-data-understanding.html","loc":"/Remaining Useful Life Prediction/2022-05-03-data-understanding.html"},{"title":"Data Understanding and Preprocessing","text":"Data Understanding and Preprocessing Welcome to the second video of the tutorial for the AI Starter Kit on advanced visualisation. In this video, we will give an overview of the dataset that we will use in this Starter Kit. The data we use is publicly-available data from the City of Seattle. The dataset contains information about bike rides at specific places in the city. At each spot, the number of bikes going in each direction per hour is counted. In the table, you see a random sample of the data. The variable 'Node' refers to the single locations at which the bikes are counted, which is done in two directions. In a first step, we will add some additional variables to the table that will help us later on for the visualisations. From the timestamp column, we extract the date and time, respectively, the hour and the day of the week. Additionally, we replace all possible missing measurements, hence where the variable 'Total' is not set, with zeros. Now, we can extract some basic statistics from the data. For all locations, we have data for at least three and a half years. For one of the locations, Fremont Bridge, the data spans even over 6 years. We can also see that Spokane Street seems to be very popular with more than 1 million bike crossings in 4 and a half years while others only reach about 300.000 counts in the same time span. Before we start with the advanced visualisations, let's briefly check the location of the single nodes on the map. This allows us to verify whether they are close to important areas, like for example tourist or business areas, points-of-interest, etc. In our interactive Starter Kit, you can click on the red circles to see where the single nodes are located. Can you guess which marker is Spokane Street? Spokane Street is on clear bottleneck in Seattle's road network. It is one of the few points at which it is possible to cross from the city center to West Seattle, whereas people can choose many different roads in other areas of the city. Similarly, Fremont Bridge connects Queen Anne and Fremont which are also divided by a canal. In addition, NorthWest 58th Street, 39th Avenue and 26th Avenue are located furthest from the city center, which might explain why they see much less traffic. With this first simple visualisation, we can already explain some characteristics of the data. In the next video, we will concentrate on the temporal evolution and seasonal patterns of the data at the different locations.","tags":"Advanced Data Visualization","url":"/Advanced Data Visualization/2022-05-03-data-understanding-and-preprocessing.html","loc":"/Advanced Data Visualization/2022-05-03-data-understanding-and-preprocessing.html"},{"title":"Introduction","text":"Introduction Welcome to the tutorial for the AI Starter Kit on resource demand forecasting. In this first video we will provide an overview of the actual business case we are tackling. We will introduce the most important concepts and explain why it is beneficial for resource planning to know the expected demand in the future. Resource demand forecasting concerns accurately predicting the future need for a resource, typically using historical information. It is one of the most essential steps in resource demand management, which tries to ensure that sufficient resources are available to satisfy a fluctuating demand. Resource demand forecasting allows one to plan ahead and guarantee that sufficient resources are available when needed and to avoid costly countermeasures in case of shortage. Precisely forecasting the demand of a given resource can be beneficial for several purposes in a wide range of industrial contexts. When knowing the future demand, only the number of resources actually required need to be allocated to a specific task or process. The field ranges from forecasting the number of products that need to be manufactured based on historical sales figures, over predicting the parking demand or traffic density in a particular neighbourhood in order to control traffic in more intelligent ways, to estimating the amount of energy that will be consumed and produced for a particular city or region. Especially in the later field, the demand is influenced by a mixture of external factors such as the weather conditions. For energy suppliers it is more beneficial to buy electricity on the day-ahead market than on the spot market. Consequently, the more accurate the energy consumption can be predicted, the lower the cost. Given that more and more houses and appliances are enabled with a smart meter, an increasing amount of data becomes available that can enable and improve this prediction. Furthermore, these kinds of predictions also allow energy suppliers to better balance demand and supply and helps them to ensure proper grid operation, but also to avoid negative prices, for example in times of high solar irradiation. In this example we see the active power of a single household near Paris shown in green. In orange, the outside temperature is given. The two are strongly correlated – the demand increases with decreasing temperatures since most French households use electrical heating. Therefore, particularly in winter times, the grid has to be stable when facing high demands. In this Starter Kit, we will demonstrate how to effectively forecast the demand for this household for the future by taking external factors into account. In the following videos, we will first start by gaining some first insights in the data in front of us and perform some initial pre-processing steps to prepare the data for the analysis. Subsequently, we will illustrate you how to gain deeper insights by presenting a number of visual and statistical techniques in order to explore the data for identifying interesting patterns. In a next step, useful characteristics or features will be extracted from the dataset that can as serve as input for the forecasting models. Finally, in the last video, we will explain you how to correctly compare the performance of multiple forecasting models. Let us go ahead with the next video on data preparation, in which we will describe how to deal with the typical problems of raw data and how to perform data fusion when multiple sources of data are available. Additional information The video material in this website was developed in the context of the SKAIDive project , financially supported by the European Social Fund , the European Union and Flanders. For more information, please contact us at elucidatalab@sirris.be","tags":"Resource Demand Forecasting","url":"/Resource Demand Forecasting/2022-05-02-introduction.html","loc":"/Resource Demand Forecasting/2022-05-02-introduction.html"},{"title":"Introduction","text":"Introduction Welcome to the tutorial for the AI Starter Kit on advanced visualisation. We will introduce you to a number of more advanced visualisation techniques. You will learn how these can be used creatively to uncover more elaborate insights from your data than would be possible with the more general out-of-the-box visualisations and plots. Data visualisation is an important activity in several phases of a data science project. It allows you to understand several characteristics of a dataset, to discover interesting insights, to validate analysis results, to communicate these results to non-experts via intuitive dashboards, and so on. Popular visualisations often used by data scientists are boxplots, distribution plots, as well as bar and line charts. These are basic out-of-the-box visualisations that are generally applicable, but that are often hard to interpret for non-experts. And more importantly, they do not always reveal interesting insights. More advanced visualisations exploit the human eye's extraordinary visual pattern recognition abilities. A clever visualisation of data can already reveal interesting patterns and insights, even before any complex algorithm is applied. On top of that, they can help in formulating hypotheses to be validated further in the data analytics process or to identify features that can be useful for data-driven modelling. Before we dive deeper in these more advanced visualisation techniques, let us discuss some examples. many time series show some underlying periodicity or seasonality. Depending on the length and granularity of these time series, these seasonal effects are often only hardly visible. In this Starter Kit, we will introduce to you to a number of visualisations that make it very easy to identify for example monthly patterns. Another field where advanced visualisations can help is anomaly detection. Such anomalies appear for example when a machine is failing, and are especially hard to detect when it comes to multi-dimensional data originating from multiple sensors. In this Starter Kit, we will use advanced visualisations such as timeline plots, heatmaps, calendar maps, area plots and scatter plots to visually explore a dataset and gradually build up more knowledge about it. We use a publicly-available dataset that consists of bike counter data, which contains hourly information on the number of bikes that cross at six different spots in Seattle. By visualising this data, we will be able to: explain certain characteristics of the data, such as some nodes having more crossings than others, identify global trends, such as an increase in traffic over the years, and seasonal trends, such as fluctuating popularity within a year, recognize structural patterns, such as distinct weekday and weekend traffic patterns, and finally detect outliers, such as weekend traffic patterns that occur on weekdays. In the next video, we will explain the data in more detail before we continue in the following videos with a number of more advanced visualisation techniques to extract insights from this data.","tags":"Advanced Data Visualization","url":"/Advanced Data Visualization/2022-05-02-introduction.html","loc":"/Advanced Data Visualization/2022-05-02-introduction.html"},{"title":"Introduction","text":"Introduction Welcome to the tutorial for the AI Starter Kit on remaining useful lifetime prediction! In this first video we will provide an overview of the actual business case we're tackling. We will introduce the most important concepts and explain why it is beneficial for maintenance to know the remaining useful lifetime of a machine, a tool or any other industrial asset. Estimating the remaining useful lifetime of an asset can be beneficial for several purposes in a variety of industrial contexts. For one, it can offer support in the better scheduling of maintenance operations, for example for offshore wind turbines. For such assets, maintenance can only be performed under the right weather conditions. But also for other assets, maintenance is an important part of its lifecycle. Traditionally, maintenance is done in a corrective way, such that it is performed at the moment an asset has failed. In that case, the failing part is identified and rectified or replaced, to ensure that the asset can subsequently resume normal operation. Especially for strongly interdependent production lines or industrial assets operating in critical environments, an unplanned downtime is to be avoided and often costly. For this reason, more recently, for an increasing number of assets preventive maintenance is performed. In that case, maintenance tasks are scheduled at regular intervals, avoiding future asset failure to a maximum extent – but with the risk to replace assets that are still working correctly and still could for a while. Hence, the optimal time for replacement is wanted: The time when the asset still works correctly but will probably fail soon. And this is where the remaining useful lifetime comes into play. Nowadays, ever more assets are equipped with different types of sensors gathering data. This started the trend towards predictive maintenance, in which the maintenance moments are decided upon in a data-driven way. Therewith, maintenance is only performed when actually needed. This is a very broad domain which encompasses a variety of topics. Besides the estimation of the asset's remaining useful lifetime – the main topic of this AI starter kit – further related problems to solve for a fully predictive maintenance approach are failure prediction, detection, and diagnosis for root cause analysis. The remaining useful lifetime of industrial assets is defined as an estimate of the remaining time that an item, component, or system is estimated to function as expected. It is expressed as the number of hours, cycles, batches or any other quantity. In the figure we see datapoints collected from an arbitrary sensor in green. Let's say it measures the tool radius of a milling machine. With increasing time, the radius decreases as the tool wears out. At a certain point, the tool is considered too worn out to still further use it in production, which can influence the quality of the produced parts or lead to an unstable production process and consequent damage to the machine. The main questions thus is for how long the tool will it still be able to function properly? By applying Machine Learning algorithms, the continuation of this time series data can be forecasted, denoted in pink. From this forecast, the remaining useful lifetime can be estimated for a given asset. In this case, the machine should be maintained within the next 10 to 15 cycles. With this information, maintenance tasks can be scheduled accordingly – with a tool still functioning and without unexpected downtime. Hence, by estimating the remaining useful lifetime, we minimize the risk of failure and maintenance cost. An accurate prediction of an asset's lifetime can also help to optimise the operational efficiency of an asset, by more optimally planning the use of that asset before its end of life, or adapting its operational use to extend its useful lifetime. In manufacturing settings, where a downtime of the production line results in several operational problems and associated costs, remaining useful lifetime prediction can avoid unplanned downtime. In this AI Starter Kit, we will show its use to avoid safety-critical situations, by illustrating the solution methodology for predicting the remaining useful lifetime of aircrafts engines. To summarize, by applying predictive maintenance it is possible to: - better schedule maintenance operations - optimize operational efficiency - avoid unplanned downtime - anticipate and avoid safety-critical situations. This analysis can be performed for various machines, tools or processes as long as this degradation over time can be measured accordingly, just like resistance, length, temperature or similar measures. Predicting the remaining useful lifetime is typically very challenging for several reasons: First of all, usually a multitude of heterogenous sensor data is measured at several places in the machine. This data is often captured at a high-frequency, consisting of vibration data, acoustic emission, accelerometer data, and many more machine-internal parameters. Secondly, in some settings, also characteristics of the surrounding environment influencing the lifetime of the asset are captured. Further, typically, data that is gathered over a long period of time needs to be available to train the model. Given that industrial assets are often very complex – as they consist of both electrical and mechanical components - it generally is a non-trivial exercise to predict their end of life. On top of that, the data typically comes with a lot of variation, due to varying machine types operating in heterogenous operating conditions with different machine configurations. Therefore, the main challenge in this process is to extract meaningful characteristics that can be used to predict the end of life from the gathered data. This is complicated by the fact that usually very little domain expertise on the operating conditions of these assets is available. Additionally, the environment in which they operate is typically very dynamic. In this AI Starter Kit, we will guide you through a data-driven methodology based on deep learning to tackle this challenge. In the next video, we will concentrate on the data itself that we selected to illustrate the approach. We will explain which information is available and what we can learn from it.","tags":"Remaining Useful Life Prediction","url":"/Remaining Useful Life Prediction/2022-05-02-introduction.html","loc":"/Remaining Useful Life Prediction/2022-05-02-introduction.html"},{"title":"Introduction","text":"Introduction Welcome to the tutorial for the AI Starter Kit on time-series pre-processing. In this first video we will provide an overview of the actual challenge we're tackling. We will introduce the most important problems occurring in time series data and explain why it is beneficial to first improve your data before you start any other data-modelling tasks. Time series are characterised as a collection of data points obtained at successive times, most often with equal intervals between them. Since an increasing amount of assets is instrumented with sensors, this type of data is omnipresent, for example when monitoring industrial machinery or resulting from wearables tracking one's health state. Depending on the application domain, time series data is exploited for various purposes. It is used, amongst others, for profiling energy consumption of households predicting imminent failures of a production line, estimating the remaining useful lifetime of a machine, and many more industrial use cases. Typically, time series data cannot be used easily as-is by machine learning algorithms. This can be for example due to data quality issues such as missing values and sensor misreadings. Or because some algorithms are not fit to deal with the continuous nature of this type of data or the associated - often high – frequency with which it is gathered. To illustrate the various methods and techniques in this Starter Kit, we will use a publicly available real-world dataset that consists of sensor data measurements from wind turbines. It exhibits typical characteristics of industrial time series datasets as it is very noisy and contains outliers and missing values. Moreover, it exhibits seasonal patterns across multiple years, as the machine behaviour is influenced by the meteorological conditions. We will use this dataset to illustrate how resampling and smoothing can be applied to gain a better understanding of the behaviour of the time series, how the quality of the data can be improved through normalization and outlier detection, and * how missing data can be imputed in various ways. In the next video, we will perform some initial exploration on the dataset in order to gain some basic understanding of the data.","tags":"Time Series Preprocessing","url":"/Time Series Preprocessing/2022-05-02-introduction.html","loc":"/Time Series Preprocessing/2022-05-02-introduction.html"},{"title":"Giving circular economy a boost by improving refurbishment process of household appliances","text":"A small part of discarded household appliances that are now shredded could be reused or refurbished, or their components reused in the refurbishing process. Refurnished household appliances can offer people in energy poverty an alternative to energy-consuming appliances. Through advanced data analytics and cognitive and assistive operator support the efficiency of the refurbishing process can be increased. 97% of the household appliances are shredded after return or recycled as raw material. In most cases these devices are relatively recent, and up to 10% can be reused or refurbished in an ecologically and economically responsible manner. Moreover, between 15% and 30% of these machines contain functioning components that can be resold as spare parts or reused in the refurbishing process. Providing more refurnished appliances helps families in energy poverty to replace energy-consuming household appliances. BrefurbiSH: Increasing efficiency of refurbishing process With the support of Innoviris, Sirris, Circular.Brussels and BSH started in 2021 the BrefurbiSH project for increasing the efficiency of the refurbishing process of household appliances through advanced data analytics and cognitive and assistive operator support. The refurbishing sector is still a very manual labour-intensive sector where target group employees perform a very repetitive sequence of instructions or tasks. In the BrefubiSH project we would increase the efficiency of household refurbishment by providing extra support to the employees. We will gather appliance-specific data before, during and after the refurbishment to extract insights on which appliances are more prone to faults and what type of error is most common. These insights can be fed back to the design process of the same type of appliances as such that design choices can be made to facilitate the refurbishing of household appliances in the future. Data extracted during the refurbishment can be used to automatically build a refurbishing guideline that is needed to train new target group employees. The traditional refurbishing infrastructure will be enhanced using advanced data analytics and cognitive and assistive operator support. The efficiency will be validated through a pilot infrastructure installed at Circular.Brussels targeting two representative appliances of the BSH appliances, namely washing machines and dishwashers. More information can be found here . With the support of","tags":"Blog","url":"/Blog/20211117_giving_circular_economy_a_boost.html","loc":"/Blog/20211117_giving_circular_economy_a_boost.html"},{"title":"Interview 3E and EluciDATA Lab in the framework of the campaign \"AI to boost your SME\" by the FPS Economie","text":"In the framework of the campaign \"Artificial Intelligence to boost your SME\" by the FPS Economie, the EluciDATA Lab shared its experience with the company 3E in an interview. More information: here .","tags":"Blog","url":"/Blog/20210820_interview_3e_and_elucidata_lab.html","loc":"/Blog/20210820_interview_3e_and_elucidata_lab.html"},{"title":"Sir Adas' series: Ada Lovelace, the first programmer","text":"Programming is for men! With roughly 23 % 1 female workers in the IT sector in Belgium, this assumption seems to suggest itself. And still, the first person ever regarded to be a programmer was a woman – namely Ada Lovelace back in 1843. Supported by her mother – in her opinion to prevent her from developing her father's (Lord Byron) perceived insanity – she eagerly studied mathematics and logic. When meeting Charles Babbage and learning about his Analytical Engine she was enthusiastic about the idea of a machine that \"has no pretensions whatever to originate anything. It can do whatever we know how to order it to perform. It can follow analysis; but it has no power of anticipating any analytical relations or truths.\" Not only that she developed a method for calculating the Bernoulli number on the Analytical Engine (considered the first published algorithm ever, but unfortunately never built), it was her driving the work and believing in it. That given, she was not only a talented mathematician but also a visioner and innovation driver. And most importantly, recognized as such by others (amongst them many men) in a time and age when women were still seen as just a pretty adornment of the décor. On the international day of women in mathematics, we are happy to announce the birth of a new blog series, namely the SirAdas blog. Its mission is to spread and promote achievements of women passionate about science, technology and innovation. In other words, the modern Ada Lovelaces. Let's celebrate and cherish women's intelligence and creativity! Follow and contribute to our blog! Your SirAdas: Sarah, Anna, Tatiana, Caroline and Elena 1 https://werk.belgie.be/nl/publicaties/de-loonkloof-tussen-vrouwen-en-mannen-belgie-rapport-2021 2 http://www.fourmilab.ch/babbage/sketch.html","tags":"Blog","url":"/Blog/20210519_ada_lovelace_the_first_programmer.html","loc":"/Blog/20210519_ada_lovelace_the_first_programmer.html"},{"title":"Is traffic volume correlated to Covid-19 infections?","text":"This is the fifth blog in a series of blogs concerning the traffic situation in Brussels in times of Covid-19 restrictions. This work has been realized in the context of the MISTic project , executed by the EluciDATA Lab of Sirris with Macq and VUB as partners. Our previous analyses were based on a dataset of Brussels Mobility , which contains vehicle counts from 55 busy locations in the city of Brussels. During the first lockdown in March 2020, we discovered, as communicated in our first blog , that traffic had dramatically reduced overall, but less so on the small ring of Brussels, where peaks of up to 80% of retained traffic were observed. In the previous blog (number 4), we investigated the traffic volume evolution in relation to historical values since 2018, in order to consider the seasonality and holiday effects impacting traffic volumes. We observed that the traffic volumes in October 2020 were very close to normal (pre-Covid-19) conditions. Today's blog is inspired by a quote of virologist Steven Van Gucht in Het Journaal on December 9: \"…wat wel opvalt is dat we ons vaker verplaatsen… en daarvan weten we dat dat een vrij goede voorspelling is van het toekomstig aantal nieuwe besmettingen.\" [... what is striking is that we move more often... and we know that that is a pretty good prediction of the future number of new infections.] Steven Van Gucht Since vehicle counts data is closely connected to people's movements, it is worthwhile to investigate whether Steven Van Gucht's statement is valid, i.e., how well traffic intensity correlates with, or even more predicts peaks in Covid-19 infections. The blog reveals three main insights: 1) a certain threshold in traffic load needs to be exceeded in order to observe an effect on the number of Covid-19 related hospitalisations; 2) there is a clear difference in effect on traffic between residential areas and more business strategic locations; 3) peaks in Covid-19 hospitalisations follow peaks in traffic volumes with a very variable time offset depending on the time of the year. Less than 16 000 vehicles a day in Brussels controls the pandemic As a first use case, we consider the average daily vehicle counts for 50 busy roads in the Brussels Capital Region since the beginning of the Covid-19 pandemic in March 2020. In the figure below, the evolution of these vehicle counts is shown with a blue line. To smooth down the periodic drops due to weekend effects, a rolling window of the average per week is applied. The latter is also aligned with the way the Brussels Covid-19 hospitalisations are reported (orange line). Note that we use the number of new hospitalisations in this study, instead of the number of new infections. The hospitalisation rate also represents the presence of Covid-19, although it is typically lagging a bit behind the number of infections. The big advantage of this parameter is its stability since it is not affected by the varying testing strategy and capacity. Legend: DIY & garden shops open B2B companies open B2C shops open Contact professions open Pubs & restaurants closed Brussels: stricter curfew Non-essential shops can open again It is interesting to observe that a certain threshold in vehicle counts (around 16 000-17 000) had to be exceeded in order to provoke the reversal of increasing/decreasing trends in Covid-19 hospitalisations. The figure above shows clearly that during the first lockdown in 2020 traffic load decreased to nearly a third of its normal volume. A similar sudden drop in Covid-19 hospitalisations can be observed some weeks later in the beginning of April (mind the log scale). Remarkably, the Covid-19 hospitalisation numbers kept dropping until the beginning of July despite the clear gradual increase in vehicle counts. Subsequently, only in the end of June the vehicle counts clearly exceeded the threshold of 16 000 vehicles, finally resulting in a strong increase of Covid-19 hospitalisations in July. Similarly, the temporary fall in Covid-19 hospitalisations in the beginning of September can be explained by the valley below 16 000 in vehicle counts in the middle of the summer holidays. During the second lockdown, traffic counts were as well only temporary diving below the threshold, followed by a stagnation around the threshold value. As a result, the Covid-19 hospitalisations dropped only for a short period and even stagnated in the second half of December. Note that this threshold might deviate over time as it is depending on external factors as availability of mouth masks and people following the restrictions less stringently. Increase in hospitalizations in Flanders is also preceded by more traffic As an extension to the Brussels vehicle counts, we also consider vehicle counts from the Telraam dataset. Telraam is an open project where citizens can monitor traffic by attaching a small camera to the inside of a window, which counts vehicles passing by. Since any citizen can participate, a large(r) number of locations can be monitored. Apart from the different Belgian regions, the big difference with the Brussels vehicle counts data is that the Telraam data is typically originating from residential areas, with hotspots in Leuven, Antwerp and Ghent. The Brussels vehicle counts originate from busier and critical traffic locations (e.g., the small ring). A drawback of the Telraam project is that such low-cost hardware is not able to detect vehicles after sunset. So, to allow for a fair comparison over all seasons, we excluded all counts before 9:00 and after 17:00. Additionally, locations with too few data points inside the time interval of interest (March 2020 – December 2020) were filtered out. Finally, we identified 379 qualitative vehicle counting locations in Flanders. Based on this Telraam data, the same figure as in the Brussels case was constructed below. Focusing solely on the vehicle counts, one can observe a very similar traffic evolution up to the start of the second lockdown in November. However, in contrast to the Brussels numbers, a much steeper drop of vehicle counts was introduced in the beginning of November, leading to even less traffic than during the first lockdown. There can be different explanations for this phenomenon. Maybe Flanders is simply a better student than Brussels when it concerns following the Covid-19 restrictions and recommendations. Most probably, the difference could be attributed to the more residential character of the locations and the exclusion of the peak hours from the Telraam dataset. After all, this sudden drop does not seem to precede a very dramatic decrease in Covid-19 hospitalisations compared to the Brussels case. In general, one can observe again a very similar (lagging behind) behaviour of the vehicle counts and the Covid-19 hospitalisations. How much time is needed to see the effect of a traffic reduction? For both Brussels and Flanders, a delayed effect of the vehicle counts on the Covid-19 hospitalisations is observed. Although the window of delay between traffic and hospitalisation peaks seems very variable in the different periods, there is no doubt that there is some correlation between the vehicle counts evolution and the Covid-19 numbers. In order to support formally this statement, we calculated the Pearson correlation between the Covid-19 hospitalisations and the average vehicle counts for 3 so called \"periods of interest\". Each period of interest represents a different stage (in terms of peaks) in the Covid-19 hospitalisations and is annotated in the two images above. Since we are interested in discovering which time offset leads to the highest correlation for each period of interest, the Pearson correlation was calculated for a time offset between 7 and 60 days for each period. The most optimal Pearson correlation during the first period of interest (the blue bar below the two plots) was obtained for both regions for a delay of 40-42 days. This indicates that the reduction in traffic (i.e., in people's movement) needed 40-42 days before it took effect on the Covid-19 hospitalisations. For period of interest 2 (small peak during summer break), the offset with the highest Pearson correlation was too high to be reliable. We suppose that too many different factors impacted the hospitalisation rate in the summer period, which obviously cannot be captured solely by traffic counts. In period of interest 3 (the second major peak), the optimal Pearson correlation is obtained at a different offset in time for Brussels (36 days) and Flanders (22 days). Such a big difference in delay of effect can be attributed to the different nature of traffic data e.g., more business related in Brussels vs. residential in Flanders, more concentrated location counts in Brussels vs. fragmented ones in Flanders, etc. which are differently affected by Covid-19 measures. So, is Steven Van Gucht's claim correct? Based on this brief study, it is clear that an increasing trend in traffic volumes can serve as an early indicator for increasing numbers of Covid-19 infections as claimed by Steven Van Gucht. However, the increase of Covid-19 infections is caused by multitude of very diverse and dynamic factors which are not captured in traffic data, e.g., unpredictable human behaviour and evolution of the virus (e.g., evolving time between symptoms and hospitalisation ). Since it is very hard to take such effects into account, monitoring evolution in traffic volumes is not reliable for prediction but still very useful to raise an alarm of caution. Project subsidized by the Brussels-Capital Region - Innoviris.","tags":"Blog","url":"/Blog/20210205_is_traffic_volume_correlated_to.html","loc":"/Blog/20210205_is_traffic_volume_correlated_to.html"},{"title":"Sir Adas' series: Women cannot play chess!","text":"It is Corona times and there is not much to do in the evenings. Thus, we bought a Netflix subscription some months ago and The Queen's Gambit was one of the first things we enjoyed. The celebrated drama series is telling the story of the genius female chess player, wining all games against men - not many women in the game in the 1960s. Of course, she could only do this thanks to her drug and alcohol dependency. No woman can beat a man in an intellectual game without some help from outside, right! Around the same time, an article[1] appeared in the Knack magazine about Judit Polgár, the best female chess player of all times. On a Sunday afternoon, I was reading with admiration about how she and her 2 sisters enjoyed the same spartan upbringing of their father, who she defeated in chess for the first time at the age of five, how at the age of fifteen years and five months old, she became the youngest Grandmaster of all time, knocking the legendary Bobby Fischer off the throne how the reigning world champion at the time Garry Kasparov, openly shared his conviction that women would never reach the highest level due to \"the imperfections of the female psyche\" how in 2002's tournament \"Russia vs The Rest of the World\", Judit Polgár finally defeated Garry Kasparov, despite the so-called imperfections of the female psyche and how other world champions shared his fate, amongst them Magnus Carlsen, Viswanathan Anand, and Anatoli Karpov. However, my admiration transformed into indignation when I reached a passage talking about the Dutch grandmaster Johannes Donner. He wrote in 1968 a piece on women and chess in the women's magazine Avenue, concluding \"However painful it may be, we must not hesitate to face the truth: Women cannot play chess.\" Feeling really upset, I recited this piece of text to my Dutch husband. He demonstrated adequate empathy by exclaiming \"But, this was such a long time ago!\" REALLY? Even the sweetest man does not have a clue how it feels to go through life as a woman! I recalled an evening on one of our skis vacations some 3-4 years ago. My husband was playing chess with my son and was mercifully defeating him game after game - father instinct obviously does not have a chance against the eagerness to win. The situation was getting dramatic, not being able to bare any further the frustration of my child, my mother instinct made me suggest I play a match with my husband instead. In less than 15 minutes, I defeated him. He completely underestimated me. What he didn't know was that my sister and I also enjoyed very spartan upbringing and played often chess with my father, who was good at it. So, my husband was sitting there embarrassed, full of disbelief and wanted to immediately rematch. Well, my victory, reflected in the admiring eyes of my son, tasted so sweet! - \"No thank you, I am going to bed!\" Never played chess again with my husband … and now I know what to say next time this topic comes up in our family discussions: \"However painful it may be, we must not hesitate to face the truth: Bulgarian women are much better in chess than Dutch men!\" [1] https://www.knack.be/nieuws/geschiedenis/judit-polgar-de-beste-schaakster-aller-tijden-ze-eet-je-met-huid-en-haar-op-als-een-lieflijk-monster/article-longread-1684359.html","tags":"Blog","url":"/Blog/20210604_women_cannot_play_chess.html","loc":"/Blog/20210604_women_cannot_play_chess.html"},{"title":"Three new thematic ICON projects on AI.","text":"The EluciDATA Lab of Sirris successfully initiates three new thematic ICON projects on AI With the Flemish Policy Plan on Artificial Intelligence, which started in 2019, Flanders invests 32 million euros annually in Artificial Intelligence (AI). As part of this plan, VLAIO launched a special call for thematic ICON projects on Artificial Intelligence (AI-ICON) with the aim to bridge the gap between research results in the field of artificial intelligence and its applications in Flemish industry. ICON projects (Interdisciplinary Cooperative Research) allows multidisciplinary research teams of scientists and industry partners to work together to develop innovative solutions that then find their way into the market offerings of the participating partners and beyond. For the special ICON call on AI, the EluciDATA Lab of Sirris, identified 3 topics with a high industrial added value in Flanders, in collaboration with 3 consortia of academic and industrial partners: TRACY (Trace Analytics) aims to investigate how to optimally use the log data generated by industrial assets and refine existing AI and machine learning techniques targeted at time series analysis . To this end, TRACY will research how to handle the complexities of log data, e.g., the heterogeneity of the industrial assets , the lack of standardisation amongst log data and the scalable interactive visualisation of the heterogeneous data. The research is validated on complex industrial use cases as optimising the performance of compressors and decreasing the service cost of electrophotographic machines. The consortium consists of Xeikon, CMC, Datylon, I-Care and Yazzoom, and the EluciDATA Lab as a research partner. CONSCIOUS ( C o ntextual aNomaly deteCtIon for cOmplex indUstrial aSsets ) focusses on c ontext-aware anomaly detection in industrial machines and processes. In these complex environments, anomaly detection remains a big challenge caused by the highly dynamic conditions in which these assets operate . The overall objective is to research effective solutions to achieve a more accurate, robust, timely and interpretable anomaly detection in complex, heterogenous data from industrial assets by accounting for confounding contextual factors. The results will be validated on multiple real-world use cases in different domains. In this project, the EluciDATA Lab will collaborate with Skyline Communications, Duracell Batteries, I-care, Yazzoom, KU Leuven and University of Antwerp. ATWI ( H ybrid, multi-modal methodology for Automatic Tool Wear Inspection ) , aims to accurately analyze and predict the tool wear in metal cutting processes . Both direct (vision -based ) and non-direct (data-driven ) methods will be combined to develop a hybrid approach that defines new methods to extract relevant data for tool wear prediction, identif y the correlation between status of the tool-wear and underlying data , and construct an innovative integration of the multimodal information in production strategies . The multidisciplinary Sirris team with experts from the Precision Manufacturing team, the EluciDATA Lab and the Mechatronics team of Sirris will collaborate with Exmore, Goddeeris , Buysse, Melotte, Tisea and KU Leuven. Despite the highly-competitive call, all three proposals were selected for funding. This will allow the EluciDATA Lab to further support the industry in tackling its real-world challenges related to AI through joint industrial research for innovative solutions. More information: https://www.vlaio.be/nl/nieuws/10-miljoen-euro-vlaio-innovatiesteun-voor-artificiele-intelligentie-en-cybersecurity","tags":"Blog","url":"/Blog/20210104_three_new_thematic_icon_projects.html","loc":"/Blog/20210104_three_new_thematic_icon_projects.html"},{"title":"Run-up in Brussels Traffic Towards the 2nd Covid-19 Peak","text":"This is the fourth blog in a series of blogs concerning the traffic situation in Brussels in times of Covid-19 restrictions. This work has been realized in the context of the MISTic project executed by the EluciDATA Lab of Sirris with Macq and VUB as partners. Our analyses are based on a dataset of Brussels Mobility , which contains vehicle counts on 55 busy locations in the city of Brussels. During the first lockdown in March, we discovered in our first blog that traffic had dramatically reduced overall, but less so on the small ring o f Brussels, where peaks of up to 80% of retained traffic were observed. In the second blog , we focused on the evolution of traffic during the first wave of relaxations of the restrictions which started in April , and revealed some interesting traffic volume patterns. Our third blog third blog zoomed into the traffic situation on the small ring of Brussels since the beginning of the restrictions until the beginning of the summer , guided by some elucidating visual analytics. In this fourth blog, we bring an update of the development of the traffic in Brussels by comparing it with pre-Covid-19 outbreak times. To this end we use the historical vehicle count data available for 17 key locations in Brussels. We observe that the varying restrictions had, and still have, a lot of influence on the Brussels traffic. HOLIDAYS As one can expect, traffic is heavily influenced by the holiday periods. Students do not have to go to school, parents often take holiday as well, some leisure activities take a break, while other dedicated to holidays leisure activities arise, etc. This phenomenon makes it hard to understand whether a sudden drop in vehicle counts at the start of a holiday is due to the \"holiday effect\" or due to the ever-changing Covid-19 restrictions. To get some more insights, we constructed some weekly fingerprints (as introduced in our first blog ) in the figure below. In the first fingerprint, titled ‘Regular work-school weeks', we illustrate the average traffic volume per hour of each weekday in the city of Brussels. For this we aggregated data from 2018 until the first Belgian lockdown in March 2020, and excluded school and public holidays. Since we use this fingerprint as the baseline for the subsequent analyses, we indicate in its center that it represents 100% of the traffic volume. People Like Extended Weekends During Easter Holidays Next to the regular work-school week fingerprint on the first row, two similar fingerprints are constructed for the pre-Covid-19 Easter and summer holidays. In the Easter holiday one can, apart from the weekend, observe a decrease of traffic volume mainly on the beginning and the end of the work week. This might be caused by people taking an extended weekend, in particular around Easter Monday. This phenomenon is not observed during the summer holidays, where the decrease in traffic volume is nicely spread out. Less Summer Traffic Without Imposed Restrictions The second row of the above figure shows the traffic fingerprints for the Easter and summer holidays during the Covid-19 pandemic. Due to the imposed lockdown, we can observe an additional 50% decrease of traffic volume during Easter, compared to the baseline Easter holidays. Some months later, in the summer holidays, all transport restrictions were abolished by the government. But surprisingly, a reduction of still 18% of the traffic volume was encountered. This might be due to several reasons e.g. more teleworking officially encouraged by the government [ statbel ], less travel related to leisure activities as children's camps and visits to museums and other cultural events, no traffic linked to festivals, etc. MONTHLY COMPARISON Pre-Covid-19 Traffic Is Fluctuating The Brussels traffic volume is highly seasonal since it is influenced by weather conditions, holidays, yearly events, etc. The blue bars of the bar chart above illustrate this by visualizing the average weekly vehicle counts over the available locations for some consecutive months (aggregated since 2018 until the Covid-19 outbreak). We can observe the highest peak of traffic volume in May, and the lowest peak in July and August. October Traffic Reached almost Normal Pre-Covid-19 Volumes The grey bars in this bar chart show the same values, but during the Covid-19 pandemic. This enables us to compare the weekly counts with what we would expect in a Covid-19-free situation. Between each corresponding month, the difference (Δ) in percentage is shown. We can observe that the overall seasonal pattern was somehow retained, while the reduction in traffic volume compared to what we would expect, kept decreasing (except for a dive of 7% in July). This ended in a decrease of only 5% in traffic volume in October. So, traffic was getting very close to the pre-Covid-19 situation. This also indirectly indicates that the regular economic activities in Brussels were also gradually getting recovered. Unfortunately, not for long before the new restrictions were imposed. Rush for Taking a Holiday Break in July As mentioned in the previous section, there is a sudden relapse of the traffic volume recovery in July. In here the difference compared to pre-Covid-19 times increases from 21% to 28%, while we before only could observe a decreasing trend in these percentages. Moreover, this traffic volume drop/valley has completely disappeared in August. This might be explained by the fact that, once the restrictions were lifted shortly before the summer holidays, people decided to take holiday immediately in July. Project subsidized by the Brussels-Capital Region - Innoviris.","tags":"Blog","url":"/Blog/20201124_runup_in_brussels_traffic_towards.html","loc":"/Blog/20201124_runup_in_brussels_traffic_towards.html"},{"title":"Sir Adas' series: No laws can empower women career-wise without an adequate social system!","text":"I once had a very frightening, and at the same time weird, experience in Edinburgh. In the beginning of the nineties, I was on a short study visit in the Department of Computer Science. I hardly knew anybody there and my English was far from fluent. One evening, at the end of a movie night, a Brazilian colleague left me behind alone at some bus stop. By the time I realised I took the wrong bus, it was already driving in the darkness somewhere outside of Edinburgh. I panicked and tried to interact, pushing my lousy English to the extreme, with the bus driver. At a certain moment the bus stopped in the middle of the road and to my big surprise a police car, called by the bus driver as turned out later, was waiting for me. They were very friendly and just gave me a lift home. Next morning, still very frightened and humiliated, I was boiling with anger when I met my colleague. I told him that his behaviour was unacceptable. No man in my native country would leave a woman alone at night. Apologising, he told me this was also the case in Brazil. He had actually doubted whether to offer me to see me home, but he feared my reaction. It seems he had pretty bad experience with being accused of sexism when opening the door for a woman or offering to carry her bag or to accompany her on her way home in the evening. I never understood this overfocus on these, in my opinion superficial, aspects of women-men interaction. I actually do not mind at all if someone opens the door for me and carries my bag. I am a small woman and considering that most of the stuff in this world has been designed by men for the averaged man (don't get me started on this topic), I often find doors and other stuff much too big and heavy for me. However, I would certainly mind if I were interrupted during meetings (which was proven to happen to women much more often) and my professional opinion was not taken seriously. I grew up in communist Bulgaria. I do not recall any fuss in particular about feminism at the time, nor having the feeling of having less opportunities or being treated differently because of my gender. It was the cold war era and the iron curtain did not let much news on women rights in the West through. Not that we cared. We had other stuff to worry about e.g. paying the electricity bill or getting hold of a pair of winter shoes and numerous other daily life concerns. However, there were also several really good things implemented by the communist system e.g. good quality education accessible for everyone, very well organised child care including a more than 2 year maternity leave, up to 2 months paid sickness leave for mothers per year to be able to nurse their sick children and an excellent network of daycares and kindergartens. Not surprising that all women were working full time. I personally didn't know anyone who was a housewife. Women were teachers, secretaries, medical doctors, but also accountants, engineers and university professors. We are speaking about the eighties. It took my Dutch husband a while to recall that he encountered only one female professor, the statistics one, in his 5 year university study. When I started working in still communist Bulgaria, late eighties, as an assistant-professor in a technical university, half of our department of computer systems and also half of the sister department of electronics were women. The same was true for our students, the future engineers. Even the head of the electronics department was a woman. When in 2003, my daughter started her engineering study in computer science at the University of Ghent, she and another girl were the only 2 female students. I have never understood from where this difference comes between East (ex-communist) and West Europe. The first time I visited West Europe, some 30 years ago, I was impressed by the cool and assertive looking women I met in the streets, some dressed very masculine and radiating independence. It took me quite some time to figure out that these outer signs didn't always imply inner self confidence and ambition to achieve more in life than just being a good mother and housewife. Anyway, all this stuff was bothering me for quite some time. I was struggling to understand why I, as a woman born and raised in communist Europe, have a somewhat different understanding/expectations of what feminism should be about than fellow women here in the West. Until a colleague sent me a link to this article: https://mondediplo.com/2021/07/07women-socialism I do not have any intention to provide you with a condensed overview of the content of the article. It would be a shame to waste the richness and depth of its insights in such a way. Just read it for yourself! I would only dare to share my own personal take away. Practically, the women in communist Europe understood from very early on that the true feminism goes beyond fighting the patriarch bias. Namely, it is difficult for a woman to have a solid professional career when: she is expected to leave her newly born baby only after less than 3 months, the daycares are much too expensive, the schools close well before the normal working time for your male colleagues to schedule important meetings, the business travels usually start in the weekend, …! Or in other words, no laws can empower and guarantee equal rights and opportunities for women if we do not create a social system making this feasible.","tags":"Blog","url":"/Blog/20201118_sir_adas_empower_women.html","loc":"/Blog/20201118_sir_adas_empower_women.html"},{"title":"EluciDATA Lab  involved in TRAIL (Trusted AI Labs) – Walloon initiative to foster research and collaboration around AI","text":"TRAIL is an overall structure, which aims at fostering collaboration and research around AI in Wallonia and Brussels between universities and research centers in line with regional policies. In the last months, the EluciDATA Lab has been involved in the definition of this partnership and its service offering. The Lab will mainly be involved in activities related to the adoption of AI by industry. For more information, we invite you to have a look at the website of the partnership: https://trail.ac","tags":"Blog","url":"/Blog/20200910_elucidata_lab__involved_in.html","loc":"/Blog/20200910_elucidata_lab__involved_in.html"},{"title":"Unravelling volume patterns of Brussels traffic in times of Covid-19","text":"questions such as \"What is the effect from each relaxation of the restrictions on the Brussels traffic?\" and \"Do people consistently obey the restrictions?\". author: Michiel Dhont, Elena Tsiporkova, Tom Tourwé and Nicolás González-Deleito This is the second in a series of blogs, in the context of the MISTic project executed by the EluciDATA Lab of Sirris with Macq and VUB as partners, concerning the traffic situation in Brussels in times of Covid-19 restrictions. Our analysis is based on the open dataset of Brussels Mobility , which contains vehicle counts on 55 busy locations in the city of Brussels. In our first blog , we discovered that traffic has dramatically, but unevenly reduced in terms of locations (e.g. less reduction witnessed at the small ring of Brussels) and of time (e.g. evening peaks retained up to 80%). In this second blog, we focus on the evolution of traffic during the relaxations of the restrictions, and on providing answers to questions such as \"What is the effect from each relaxation of the restrictions on the Brussels traffic?\" and \"Do people consistently obey the restrictions?\". Traffic fingerprints Since the Covid-19 restrictions are gradually being relaxed, traffic is slowly returning to 'normal' and allows us to observe the emergence of traffic volumes associated to different activities. Following the approach from the first blog, we have generated characteristic weekly fingerprints for different time periods depicted in the upper row of the figure below. Each of these fingerprints depicts the hourly traffic intensity throughout the (averaged) week. Our baseline (100% traffic volume) is constructed by averaging over 5 'regular' work-school weeks, excluding the school holidays, in January and February 2020. The second fingerprint (Carnival holidays) refers to the school vacation at the end of February, while the remaining fingerprints average traffic intensities over the different phases of the quarantine period e.g. complete lockdown between 14 March and 17 April, followed by opening of the DIY & garden shops, re-starting of companies' activities, re-opening of all other shops. The bottom row of this figure depicts the result of subtracting each weekly ‘fingerprint' from the fingerprint immediately on its left. This highlights better the time slots where traffic has increased (blue) or decreased (orange) compared to the previous ‘fingerprint'. 18 th of April: opening of DIY shops might cause some non-essential traffic The opening of the DIY and garden shops resulted overall in 7% of traffic increase. Since Brussels is a major city with few DIY shops, we expected to see this increase mostly for some nodes only, and for sure not for nodes located in residential areas. However, we observed a similar increase for all nodes across the entire city. In addition, it appears the traffic volume also increased with a similar amount on Sunday, when shops are closed. All this seems to suggest that probably not all observed traffic increase is due to DIY-garden shopping. Perhaps people thought that DIY-shopping could be used as a valid pretext for doing other non-essential commutes? Notice also how the volume didn't increase consistently for all days: Friday even has a decrease in traffic volume. This Friday is Labour Day, an official bank holiday. 4 th of May: friends or family visits are preferred over going to work again In the next fingerprint, corresponding to the week in which B2B companies could restart their activities, we see 11% average increase in traffic volume. Even though more people were allowed to go to work again, we see the highest increase in traffic volume on Sunday. This is obviously not due work activities. This particular Sunday was Mother's Day, and people were allowed to meet with up to 4 other people. Many people seem to have seized this opportunity to go visit their relatives and friends. Note that the very clear increase of traffic on Friday is influenced by the reduced traffic on Labour Day of the week before, so this is probably not significant. 11 th of May: there's no rush to go shopping The most recent relaxation, namely the opening of all B2C shops, results only in a moderate increase of 7% in the overall traffic that seems to be quite nicely spread over all days of the week. It appears that not everybody rushed to benefit immediately from the shopping opportunities at the same time. This has helped to avoid very busy peak periods at the shopping areas. It is also interesting to note that no increase of traffic was observed on Sunday, 17 th of May. This indicates that people travelled as much as on the previous Sunday, which was Mother's Day. Does this mean that people visited their friends on both Sundays, and not their mothers? Or they just visited their mothers again complying strictly to the rule of 4? Project subsidized by the Brussels-Capital Region - Innoviris.","tags":"Blog","url":"/Blog/20200519_unravelling_volume_patterns_of_brussels.html","loc":"/Blog/20200519_unravelling_volume_patterns_of_brussels.html"},{"title":"Insightful blueprints of Brussels traffic emerge in times of covid-19","text":"In the context of an industrial PhD project sponsored by Innoviris , the EluciDATA Lab of Sirris joined forces with Macq and VUB to contribute to better traffic management solutions for the city of Brussels. To this end, we aim for developing an advanced trend analytics engine that facilitates understanding traffic situations in an accurate and situation-aware way. In January 2020, we started gathering publicly available data captured by Brussels Mobility at 55 locations in the city, without having the slightest suspicion that the escalation of the corona epidemic will deliver interesting data to analyse. The lockdown measures introduced on 13/03 imposed serious restrictions on the traffic in Brussels, permitting only limited commuting. In this way, the observed traffic after the introduction of the measures can be considered as an opportunity to derive some characteristic blueprints of the traffic in Brussels. Traffic volume unevenly affected by the lockdown As expected, the overall traffic volume in Brussels has dramatically reduced during the lockdown period. The figure below illustrates the percentage of traffic volume retained during the lockdown in comparison to the period before for all observed locations. For some key locations (e.g. Keizer Karellaan, Willebroekkaai and Troontunnel), more than 50% of the traffic was retained, while other locations (e.g. Lorrainedreef, Vleurgattunnel and Stefaniatunnel exit direction centrum) have seen a substantial reduction, up to 80%, of traffic volume in comparison to the pre-lockdown period. On average 40% of the traffic volume was retained during the lockdown. The blue dots in the image indicate the locations where the amount of traffic has decreased more than the average, while the red dots indicate a smaller decrease than the average. It is clear the small ring of Brussels retains more volume compared to the residential areas around it. This might mean the functional traffic in the city of Brussels is less reduced than the recreational traffic. However, focusing on the rush hour periods offers completely different figures. For instance, the left figure below depicts the percentage of traffic volume retained during the lockdown period for each location between 6:00 and 9:00 on workdays. It can be observed that the morning traffic for some locations (e.g. Kunst-Wettunnel, Troontunnel and Tervurentunnel) is in the range of 80% of its normal volume, i.e. it is only slightly reduced due to the covid-19 restrictions. Analogously, the evening traffic between 16:00 and 20:00 on workdays, delivers similar results as visualised in the right figure above. For instance, for the locations at Rogiertunnel, Kunst-Wettunnel and Troontunnel between 65% and 80% of the regular traffic volume has been retained. All the above is yet one more confirmation of how saturated the traffic in Brussels has become in the recent years. Even a lockdown situation, never experienced before, is not able to relieve the traffic intensity of the busiest locations in Brussels. It is also important to realise that those very hectic traffic locations in Brussels are probably not that much affected by school related as well as by recreational traffic. This explains to some extent why the lockdown does not affect the traffic volume so much for these locations. Fingerprinting of weekly traffic confirms compliance with lockdown measures In the plots below, we have extracted a weekly traffic intensity fingerprint for two monitored locations in the form of a circular heatmap. The circular heatmap depicts the days of the week as concentration circles starting with Monday in the inner circle, followed by Tuesday in the next circle and so on till placing Sunday in on the most outer one. The circles are divided in sectors of 1 hour. The darker the colour of a sector the more intense traffic has been observed. The plot depicts the intensity fingerprints for weeks 8 to 16 (17 th of February to 19 th of April) for the Tervurentunnel (in direction of the city centre) and the Troontunnel (direction Belliard). It is interesting to observe how the traffic intensity evolved across the weeks for each of the locations. Starting on Saturday (March the 14 th , week 11) restaurants had to close and some non-essential shops were not allowed to open during the weekend, which resulted in a somewhat reduced traffic during the weekend for this week. On Wednesday (March the 18 th ) at 12:00 in the following week (week 12), even more restrictions were applied, e.g. more shops had to close and non-essential travels by private vehicles were prohibited, which is clearly detectable in the heatmap of week 12. From week 13 onwards (lockdown), we obtain for each week a very similar profile. The similarity between those consecutive weeks indicates that citizens in Brussels continue to follow the measures as well as at the beginning, no relaxation can be observed. These latter profiles are very interesting since they offer a kind of blueprint for the amount of absolutely inevitable traffic in Brussels. Distilling traffic volumes associated to specific activities The collected data spans over a time period covering 3 different distinct traffic situations: Normal referring to regular work-school etc. weeks. Carnival holidays referring to the vacation (week 9), which excludes school-related traffic and some work-related traffic due to parents taking vacation. At the same time, those families have more time for recreational trips (e.g. city trips, sport, shopping, etc.). Lockdown weeks referring to the period of activity restrictions due to covid-19 measures, including only traffic related to work which cannot be performed via teleworking and other minimal essential traffic (e.g. shopping for food). In the figure below, 5 heatmaps are shown concerning traffic information of the Troontunnel. The top three heatmaps are showing the characteristic fingerprints of the three distinct traffic situations. For the normal and regular fingerprints, the traffic volumes represented are aggregated over several weeks, while the carnival fingerprint is based on the traffic counts of a single week (week 9). Comparing the characteristic fingerprints allows to disaggregate the traffic volume into separate intensities associated with different activities. For instance, by subtracting the vehicle counts during the carnival holidays from the counts of normal weeks, a new (fingerprint) heatmap is obtained, labelled with \"1\". The red sectors in the heatmap indicate more traffic volumes for the normal week when comparing with the vacation period and the blue sectors signify less traffic in the regular week, i.e. increase of traffic during the vacation for those time slots. For instance, the Troontunnel shows both increases and decreases of traffic during the carnival period in comparison to the regular weeks. The differences in vehicle counts are spread over the whole heatmap and do not seem to have daily recurring peaks. The latter might be because in general there is not a lot of school traffic in that area. If this is correct, we actually segregated the amount of traffic which is additional recreational traffic due to the hollidays. The fingerprint labeled with \"2\", resulting from subtracting lockdown traffic volumes from the corresponding ones during the vacation week, is more intriguing. Both fingerprints summarize traffic volumes not containing school-related travel. However, the vacation week still contains a lot of recreational traffic related to school children, which is not the case for the lockdown weeks. In addition, the vacation week also captures traffic related to people who switched to teleworking or are technically unemployed during the lockdown and also non-essential activities (e.g. non-food shopping, restaurant dinners and events). All those missing activities during the lockdown were captured in heatmap \"2\". In there, the red color denotes the amount of traffic volume reduced during the lockdown vs. the vacation week, while the blue color denotes an increase of traffic volume. Clear morning and evening red peaks can be observed, which signify substantial reduction of work-related traffic due to the covid-19 restrictions. Outside the morning and evening segments, more traffic is observed during the vacation week compared to the lockdown weeks, which can be attributed to reacreational and other 'non-essential' traffic, e.g. sport, restaurants, events, etc. The purpose of this analysis is to illustrate that due to the unique restrictions for covid-19, we are able to split the characteristic fingerprint of normal traffic into three subcategories. Or in other words, the normal heatmap can be disaggregated into the sum of heatmaps 1 , 2 and 3. Most of the present teleworkers are typically biking to work? Similarly, as for vehicle traffic, we have extracted a weekly intensity fingerprint for biking traffic since we also have access to bike counting data of the city of Brussels. The heatmaps for two locations during weeks 8-16, displayed below, show that the intensity of the biking traffic has increased in the lockdown weeks. This is not surprising since walking and biking is practically the only allowed outdoor activity. For example, in the Koolmijnkaai, a street in the city centre, we can see that after the covid-19 restrictions the clear morning and evening peaks disappeared, but the amount of traffic (most probably recreational) during the afternoon (especially in the weekend) increases. In the Franklin Rooseveltlaan (near to Ter Kamerenbos) this difference is even more pronounced. It is also interesting to observe the clear morning and evening peaks in the Koolmijnkaai, which must be mostly representing work commuting as there is no peak on Wednesday afternoon. Note that the morning and evening peaks have fully disappeared in the lockdown weeks. This raises the question whether most of the bikers switched to teleworking during the lockdown. Is speeding induced by too much traffic? Another interesting measure for studying Brussels traffic is of course speed. The figure below displays the relative difference in average speed, i.e. lockdown vs. normal period, for each of the 55 locations. For instance, an average slowdown of 14 km/h was detected in the Georges Henri tunnel, while an increase of 12 km/h, on average, was observed in the Troontunnel. It is normal to expect, considering the traffic in Brussels is very saturated, that for many locations the average speed has increased since the start of the covid-19 restrictions. For instance, during a regular workday, the Troontunnel is often clogged with traffic and the maximum allowed speed of 50km/h is hardly possible. It is interesting to observe that, although the morning and evening peaks of traffic volume in the Troontunnel have been reduced with only 21% in the lockdown, the average speed has increased to the legal maximum. Thus, it seems that the modest reduction of traffic volume is already sufficient to permit fluent traffic flow. It is also remarkable that during the lockdown weeks people seem to respect the legal speed limit more. For instance, the Georges Henri tunnel has a speed limit of 50 km/h which is rarely respected during normal times, as it can be observed from the plots above, showing that the average speed hovers around 70 km/h during the day. However, during the lockdown weeks the average speed dropped to the maximum allowed of 50km/h. The same phenomenon can be observed during the weekends for both mentioned locations. We believe there are 2 possible explanations for this phenomenon. Either the drivers are less stressed since the traffic is less hectic, there are almost no traffic jams, people are not in hurry for a work/school or any other appointment (e.g. reservation at a restaurant, training in sports club and music event), or the drivers who typically violate speed limits are not driving in Brussels during the lockdown. Or perhaps we are observing a combination of both? What's next? The covid-19 restrictions will be phased out in the next weeks/months by the government. It will be interesting to see how fast the population will react to this and whether we will return to the original traffic patterns. Project subsidized by the Brussels-Capital Region - Innoviris.","tags":"Blog","url":"/Blog/20200429_insightful_blueprints_of_brussels_traffic.html","loc":"/Blog/20200429_insightful_blueprints_of_brussels_traffic.html"},{"title":"How process modelling & data science will enhance your food processing business","text":"On 11/06, the EluciDATA Lab will be giving a presentation on how process modelling and data science can support companies active in the food processing business to enhance their business. In particular, it will be shown how modelling goes beyond the process and can include applications for understanding and optimizing the food process and the food supply chain and for performing a life cycle analysis. With this presentation, companies will get insights into how to start a data science project and get inspired via real-world cases from the food processing sector. This webinar is organised together with Technord & Agoria. More information: here .","tags":"Blog","url":"/Blog/20200603_how_process_modelling__data.html","loc":"/Blog/20200603_how_process_modelling__data.html"},{"title":"Flanders AI Forum on 23/01/2020","text":"On January 23 rd , Agoria, Voka, imec and Sirris with the support of VLAIO organized the first Flanders AI Forum with the presence of Hilde Crevits, Viceminister-president, Vlaams minister van Economie, Innovatie, Werk, Social economie en Landbouw, to present different initiatives in the framework of the AI impulsprogramme. Two initiatives of the EluciDATA Lab were presented: The SKAIDive project, which aims at creating an interactive Starter Kit Platform for the adoption of industrial AI and Data Science competencies by companies. The AI4DETAIL project aiming at building up and transfering knowledge related to gathering and exploiting in-the-field asset usage data. Interested in one of the initiatives? Get in touch with us !","tags":"Blog","url":"/Blog/20200124_flanders_ai_forum_on_23012020.html","loc":"/Blog/20200124_flanders_ai_forum_on_23012020.html"},{"title":"The Small Ring of Brussels is Getting Gradually Cured (i.e. back to normal stagnated traffic) from Covid-19","text":"\\ This is the third in a series of blogs, in the context of the MISTic project executed by the EluciDATA Lab of Sirris with Macq and VUB as partners, concerning the traffic situation in Brussels in times of Covid-19 restrictions. Our analysis is based on the open dataset of Brussels Mobility , which contains vehicle counts on 55 busy locations in the city of Brussels. In our first blog , we discovered that traffic had dramatically reduced overall, but less so on the small ring on Brussels, where peaks of up to 80% retained traffic were observed. In the second blog , we focused on the evolution of traffic during the first wave of relaxations of the restrictions, which revealed some interesting traffic volume patterns. This third blog is zooming into the traffic situation of the small ring of Brussels since the beginning of the restrictions until now, guided by some elucidating visual analytics. The small ring of Brussels is a sequence of many tunnels around the core center of Brussels, notoriously known for frequent traffic problems. Most of these tunnels are monitored by Brussels Mobility in terms of traffic flow per minute, average speed per minute and occupancy of the road (% of time the sensor is covered by a vehicle). We consider data for 16 of those locations from the beginning of the year until the present. Our aim is to identify and characterize traffic modes, i.e. distinct periods of time with similar traffic patterns, when considering all the 16 locations as one connected trajectory of tunnels. For this purpose, the dataset has been organized in such a way that the locations are sequentially ordered as they spatially appear in reality. Divergent Types of Traffic Our assumption is that during normal times the small ring goes through several different traffic modes which reoccur day after day. In order to identify those traffic modes, we have considered only the data before the lockdown (normal times) and were able to derive 5 distinct traffic modes by clustering along the time dimension. In order to understand the specific characteristics of each traffic mode in detail, we turn to visual analytics. In this case, we generate pairwise scatter plots of the flow, speed and road occupancy, averaged over all locations, and represent the clusters we obtain in different colours (see figure below). The scatter plots show well-formed curved patterns, which is the typical shape caused by traffic jams, i.e. high road occupancy results in lower vehicle speeds. The different traffic modes also appear well separated from each other with the exception of one, traffic mode B. Traffic mode A represents little traffic and high speeds on average, while traffic mode D seems to represent highly saturated traffic, resulting in lower average speeds. Traffic mode C contains points in time where a lot of traffic is present, but still high average speeds are possible. Traffic mode E denotes in general fluent traffic resulting in higher average speeds. Finally as mentioned above, traffic mode B is the least distinctive since it overlaps to some extent with traffic mode D. It seems to correspond to rather congested traffic at low average speeds. We can exploit the above traffic modes further in many different ways: They can be the basis for anomaly detection, e.g. we know at each moment in time what is the expected traffic mode, and so we can assume an anomaly occurs whenever a different traffic mode is observed at a specific time. They can be the basis for traffic prediction, e.g. given a specific moment in time and a specific location, we can predict the amount of traffic and its average speed. They can be used in subsequent visualizations to understand and interpret traffic evolution even better, as will be explained next. An Infinite Carpet of Traffic Modes We used the 5 traffic modes discussed above to construct a so-called label map by colouring the different time segments in a day with their respective traffic mode colours. If we do this for a sufficiently long time period as illustrated in the figure below, we obtain a colourful carpet, which offers a birds' eye view of the composition of the different traffic modes over the time period under investigation. On the y-axis we represent the different days of the year, while the x-axis represents the time within each day. Each cell is coloured with the colour of the traffic mode that was observed at that specific day and time, which allows to describe each of the traffic modes in a more intuitive way: Mode A : Night traffic (avg. occupancy 7%) Mode B : Morning rush hour peak (avg. occupancy 26%) Mode C : Day traffic outside rush hours (avg. occupancy 22%) Mode D : Evening rush hour peak (avg. occupancy 28%) Mode E : Early morning and late evening traffic (avg. occupancy 15%) It is interesting to observe that our algorithm considered the morning and evening peaks as different modes, despite people might expect that the rush hours look the same. This is because the traffic flow is almost mirrored (people who went to their work in the morning, usually take the reverse trajectory in the evening), plus there is on average a more recreational evening traffic. Note that the grey zones indicate missing data. In the ‘carpet of traffic modes' below, we can observe a clear repetitive pattern during the pre-Covid-19 period (indicated with ‘normal'). The morning peaks, evening peaks, weekend patterns, etc. are repeating each day and each week. Lockdown Traffic = Normal Night Traffic Our ‘carpet of traffic modes' is offering a very concise, but still rather granular, visualization which is ‘infinitely' expandable in time by rolling it forward over a certain period. It facilitates continuous monitoring of the traffic evolution in time e.g. detecting and interpreting recurring patterns during typical weeks (pre-Covid-19). It also allows for rapid detection of deviating behavior, i.e. which has not been observed in the past. We can easily capture the introduction of the first restrictions (14th of March, day 74) and the start of the lockdown (18th of March, day 78). Since then, mode A is predominantly present, which is typically the traffic mode corresponding to night traffic in normal conditions. Rush Hours are Emerging Another fascinating phenomenon which can be observed is the traffic mode evolution in time e.g. the gradual emergence of mode E already in the very beginning of the lockdown at around 7 p.m. and then slowly unrolling forward covering wider parts of the day. So, despite the expectation that there should be no difference in traffic intensities between the first four weeks of the lockdown, a small gradual increase can be observed. Subsequently, following on the gradual relaxation of the lockdown restrictions in May (days 122 until 152), we even observe almost constantly traffic mode E during the day. Eventually, mode C and even mode D (evening rush hour peak) are emerging during the typical rush hours, getting closer and closer to the normal situation, but not yet there. No Rush during Rush Hours Zooming into in the ‘carpet of traffic modes' for the most recent couple of days, it appears that the traffic intensity of the small ring of Brussels during the typical rush hours is gradually increasing towards the volumes of the normal day traffic outside rush hours (mode C). Thus, although the traffic volume is steadily increasing, it has not yet been recovered completely to stagnating rush hour volumes, luckily for the daily commuters. Project subsidized by the Brussels-Capital Region - Innoviris.","tags":"Blog","url":"/Blog/20200701_the_small_ring_of_brussels.html","loc":"/Blog/20200701_the_small_ring_of_brussels.html"},{"title":"Introduction","text":"Introduction Welcome to the tutorial for the AI Starter Kit on resource demand forecasting. In this first video we will provide an overview of the actual business case we are tackling. We will introduce the most important concepts and explain why it is beneficial for resource planning to know the expected demand in the future. Resource demand forecasting concerns accurately predicting the future need for a resource, typically using historical information. It is one of the most essential steps in resource demand management, which tries to ensure that sufficient resources are available to satisfy a fluctuating demand. Resource demand forecasting allows one to plan ahead and guarantee that sufficient resources are available when needed and to avoid costly countermeasures in case of shortage. Precisely forecasting the demand of a given resource can be beneficial for several purposes in a wide range of industrial contexts. When knowing the future demand, only the number of resources actually required need to be allocated to a specific task or process. The field ranges from forecasting the number of products that need to be manufactured based on historical sales figures, over predicting the parking demand or traffic density in a particular neighbourhood in order to control traffic in more intelligent ways, to estimating the amount of energy that will be consumed and produced for a particular city or region. Especially in the later field, the demand is influenced by a mixture of external factors such as the weather conditions. For energy suppliers it is more beneficial to buy electricity on the day-ahead market than on the spot market. Consequently, the more accurate the energy consumption can be predicted, the lower the cost. Given that more and more houses and appliances are enabled with a smart meter, an increasing amount of data becomes available that can enable and improve this prediction. Furthermore, these kinds of predictions also allow energy suppliers to better balance demand and supply and helps them to ensure proper grid operation, but also to avoid negative prices, for example in times of high solar irradiation. In this example we see the active power of a single household near Paris shown in green. In orange, the outside temperature is given. The two are strongly correlated – the demand increases with decreasing temperatures since most French households use electrical heating. Therefore, particularly in winter times, the grid has to be stable when facing high demands. In this Starter Kit, we will demonstrate how to effectively forecast the demand for this household for the future by taking external factors into account. In the following videos, we will first start by gaining some first insights in the data in front of us and perform some initial pre-processing steps to prepare the data for the analysis. Subsequently, we will illustrate you how to gain deeper insights by presenting a number of visual and statistical techniques in order to explore the data for identifying interesting patterns. In a next step, useful characteristics or features will be extracted from the dataset that can as serve as input for the forecasting models. Finally, in the last video, we will explain you how to correctly compare the performance of multiple forecasting models. Let us go ahead with the next video on data preparation, in which we will describe how to deal with the typical problems of raw data and how to perform data fusion when multiple sources of data are available. Additional information The video material in this website was developed in the context of the SKAIDive project , financially supported by the European Social Fund , the European Union and Flanders. For more information, please contact us at elucidatalab@sirris.be","tags":"Resource Demand Forecasting","url":"/Resource Demand Forecasting/2022-05-02-introduction.html","loc":"/Resource Demand Forecasting/2022-05-02-introduction.html"},{"title":"Introduction","text":"Introduction Welcome to the tutorial for the AI Starter Kit on advanced visualisation. We will introduce you to a number of more advanced visualisation techniques. You will learn how these can be used creatively to uncover more elaborate insights from your data than would be possible with the more general out-of-the-box visualisations and plots. Data visualisation is an important activity in several phases of a data science project. It allows you to understand several characteristics of a dataset, to discover interesting insights, to validate analysis results, to communicate these results to non-experts via intuitive dashboards, and so on. Popular visualisations often used by data scientists are boxplots, distribution plots, as well as bar and line charts. These are basic out-of-the-box visualisations that are generally applicable, but that are often hard to interpret for non-experts. And more importantly, they do not always reveal interesting insights. More advanced visualisations exploit the human eye's extraordinary visual pattern recognition abilities. A clever visualisation of data can already reveal interesting patterns and insights, even before any complex algorithm is applied. On top of that, they can help in formulating hypotheses to be validated further in the data analytics process or to identify features that can be useful for data-driven modelling. Before we dive deeper in these more advanced visualisation techniques, let us discuss some examples. many time series show some underlying periodicity or seasonality. Depending on the length and granularity of these time series, these seasonal effects are often only hardly visible. In this Starter Kit, we will introduce to you to a number of visualisations that make it very easy to identify for example monthly patterns. Another field where advanced visualisations can help is anomaly detection. Such anomalies appear for example when a machine is failing, and are especially hard to detect when it comes to multi-dimensional data originating from multiple sensors. In this Starter Kit, we will use advanced visualisations such as timeline plots, heatmaps, calendar maps, area plots and scatter plots to visually explore a dataset and gradually build up more knowledge about it. We use a publicly-available dataset that consists of bike counter data, which contains hourly information on the number of bikes that cross at six different spots in Seattle. By visualising this data, we will be able to: explain certain characteristics of the data, such as some nodes having more crossings than others, identify global trends, such as an increase in traffic over the years, and seasonal trends, such as fluctuating popularity within a year, recognize structural patterns, such as distinct weekday and weekend traffic patterns, and finally detect outliers, such as weekend traffic patterns that occur on weekdays. In the next video, we will explain the data in more detail before we continue in the following videos with a number of more advanced visualisation techniques to extract insights from this data.","tags":"Advanced Data Visualization","url":"/Advanced Data Visualization/2022-05-02-introduction.html","loc":"/Advanced Data Visualization/2022-05-02-introduction.html"},{"title":"Introduction","text":"Introduction Welcome to the tutorial for the AI Starter Kit on remaining useful lifetime prediction! In this first video we will provide an overview of the actual business case we're tackling. We will introduce the most important concepts and explain why it is beneficial for maintenance to know the remaining useful lifetime of a machine, a tool or any other industrial asset. Estimating the remaining useful lifetime of an asset can be beneficial for several purposes in a variety of industrial contexts. For one, it can offer support in the better scheduling of maintenance operations, for example for offshore wind turbines. For such assets, maintenance can only be performed under the right weather conditions. But also for other assets, maintenance is an important part of its lifecycle. Traditionally, maintenance is done in a corrective way, such that it is performed at the moment an asset has failed. In that case, the failing part is identified and rectified or replaced, to ensure that the asset can subsequently resume normal operation. Especially for strongly interdependent production lines or industrial assets operating in critical environments, an unplanned downtime is to be avoided and often costly. For this reason, more recently, for an increasing number of assets preventive maintenance is performed. In that case, maintenance tasks are scheduled at regular intervals, avoiding future asset failure to a maximum extent – but with the risk to replace assets that are still working correctly and still could for a while. Hence, the optimal time for replacement is wanted: The time when the asset still works correctly but will probably fail soon. And this is where the remaining useful lifetime comes into play. Nowadays, ever more assets are equipped with different types of sensors gathering data. This started the trend towards predictive maintenance, in which the maintenance moments are decided upon in a data-driven way. Therewith, maintenance is only performed when actually needed. This is a very broad domain which encompasses a variety of topics. Besides the estimation of the asset's remaining useful lifetime – the main topic of this AI starter kit – further related problems to solve for a fully predictive maintenance approach are failure prediction, detection, and diagnosis for root cause analysis. The remaining useful lifetime of industrial assets is defined as an estimate of the remaining time that an item, component, or system is estimated to function as expected. It is expressed as the number of hours, cycles, batches or any other quantity. In the figure we see datapoints collected from an arbitrary sensor in green. Let's say it measures the tool radius of a milling machine. With increasing time, the radius decreases as the tool wears out. At a certain point, the tool is considered too worn out to still further use it in production, which can influence the quality of the produced parts or lead to an unstable production process and consequent damage to the machine. The main questions thus is for how long the tool will it still be able to function properly? By applying Machine Learning algorithms, the continuation of this time series data can be forecasted, denoted in pink. From this forecast, the remaining useful lifetime can be estimated for a given asset. In this case, the machine should be maintained within the next 10 to 15 cycles. With this information, maintenance tasks can be scheduled accordingly – with a tool still functioning and without unexpected downtime. Hence, by estimating the remaining useful lifetime, we minimize the risk of failure and maintenance cost. An accurate prediction of an asset's lifetime can also help to optimise the operational efficiency of an asset, by more optimally planning the use of that asset before its end of life, or adapting its operational use to extend its useful lifetime. In manufacturing settings, where a downtime of the production line results in several operational problems and associated costs, remaining useful lifetime prediction can avoid unplanned downtime. In this AI Starter Kit, we will show its use to avoid safety-critical situations, by illustrating the solution methodology for predicting the remaining useful lifetime of aircrafts engines. To summarize, by applying predictive maintenance it is possible to: - better schedule maintenance operations - optimize operational efficiency - avoid unplanned downtime - anticipate and avoid safety-critical situations. This analysis can be performed for various machines, tools or processes as long as this degradation over time can be measured accordingly, just like resistance, length, temperature or similar measures. Predicting the remaining useful lifetime is typically very challenging for several reasons: First of all, usually a multitude of heterogenous sensor data is measured at several places in the machine. This data is often captured at a high-frequency, consisting of vibration data, acoustic emission, accelerometer data, and many more machine-internal parameters. Secondly, in some settings, also characteristics of the surrounding environment influencing the lifetime of the asset are captured. Further, typically, data that is gathered over a long period of time needs to be available to train the model. Given that industrial assets are often very complex – as they consist of both electrical and mechanical components - it generally is a non-trivial exercise to predict their end of life. On top of that, the data typically comes with a lot of variation, due to varying machine types operating in heterogenous operating conditions with different machine configurations. Therefore, the main challenge in this process is to extract meaningful characteristics that can be used to predict the end of life from the gathered data. This is complicated by the fact that usually very little domain expertise on the operating conditions of these assets is available. Additionally, the environment in which they operate is typically very dynamic. In this AI Starter Kit, we will guide you through a data-driven methodology based on deep learning to tackle this challenge. In the next video, we will concentrate on the data itself that we selected to illustrate the approach. We will explain which information is available and what we can learn from it.","tags":"Remaining Useful Life Prediction","url":"/Remaining Useful Life Prediction/2022-05-02-introduction.html","loc":"/Remaining Useful Life Prediction/2022-05-02-introduction.html"},{"title":"Introduction","text":"Introduction Welcome to the video tutorial for the AI Starter Kit on data exploration. In this first video, we will discuss why data exploration is the first step in answering any data science question. Before starting to use complex algorithms to solve a data science problem, you need to gain a thorough understanding of the corresponding data and the underlying business challenge you are trying to address. Data exploration is the process through which you gain such an understanding. This will eventually enable you to derive viable working hypotheses related to the problem at hand, which is useful for the further analysis and modelling of the data. Throughout this process you will be faced with different types of variables, for example numerical or categorical, which require different types of treatment. In addition, looking at individual variables in isolation is not sufficient, as this does not consider the, often complex, interactions between different variables. Furthermore, you will need to identify which are the relevant variables in your dataset and, on the other hand, which ones worsen its outcome rather than improving the quality of your analysis. Finally, data exploration will help you to point out which data quality issues your dataset may have and which actions you should undertake to mitigate them. Amongst others, data exploration is useful to identify usual and unusual values for a variable but also, to study the evolution of a variable over time. It can be helpful to uncover significant relationships between different variables or to assess data quality and suitability. The goal behind this Starter Kit is to lay out a series of analyses that will teach you how to explore individual variables, look at how pairs of variables are related and study the complex interactions between groups of variables. You will learn how to conduct a quantitative and visual inspection of your dataset and you will be prepared for the next step in your advanced analytics workflow. In the tutorial of this AI Starter Kit, we follow a systematic approach to explore a dataset, based on the number of variables under consideration. We start by exploring individual variables in isolation, which is the simplest form of analysis and is called univariate analysis. We continue by studying pairs of variables, which is called bivariate analysis. And finally, we study the relationship between several variables, called multivariate analysis before we conclude on our findings. But before diving into this, in the next video, we will first introduce you to the dataset that we will use in this analysis and perform some basic preprocessing on it.","tags":"Data Exploration","url":"/Data Exploration/2022-09-02-introduction.html","loc":"/Data Exploration/2022-09-02-introduction.html"},{"title":"Introduction","text":"Introduction Welcome to the tutorial for the AI Starter Kit on time-series pre-processing. In this first video we will provide an overview of the actual challenge we're tackling. We will introduce the most important problems occurring in time series data and explain why it is beneficial to first improve your data before you start any other data-modelling tasks. Time series are characterised as a collection of data points obtained at successive times, most often with equal intervals between them. Since an increasing amount of assets is instrumented with sensors, this type of data is omnipresent, for example when monitoring industrial machinery or resulting from wearables tracking one's health state. Depending on the application domain, time series data is exploited for various purposes. It is used, amongst others, for profiling energy consumption of households predicting imminent failures of a production line, estimating the remaining useful lifetime of a machine, and many more industrial use cases. Typically, time series data cannot be used easily as-is by machine learning algorithms. This can be for example due to data quality issues such as missing values and sensor misreadings. Or because some algorithms are not fit to deal with the continuous nature of this type of data or the associated - often high – frequency with which it is gathered. To illustrate the various methods and techniques in this Starter Kit, we will use a publicly available real-world dataset that consists of sensor data measurements from wind turbines. It exhibits typical characteristics of industrial time series datasets as it is very noisy and contains outliers and missing values. Moreover, it exhibits seasonal patterns across multiple years, as the machine behaviour is influenced by the meteorological conditions. We will use this dataset to illustrate how resampling and smoothing can be applied to gain a better understanding of the behaviour of the time series, how the quality of the data can be improved through normalization and outlier detection, and * how missing data can be imputed in various ways. In the next video, we will perform some initial exploration on the dataset in order to gain some basic understanding of the data.","tags":"Time Series Preprocessing","url":"/Time Series Preprocessing/2022-05-02-introduction.html","loc":"/Time Series Preprocessing/2022-05-02-introduction.html"},{"title":"Key Take Away Messages","text":"Key Take Away Messages In this Starter Kit we have demonstrated how creative visualisations can already reveal interesting patterns and more elaborate insights in your data, even before any complex algorithm is applied. In particular, we have used bike counter data to illustrate how timeline plots, different types of heatmaps, streamgraph plots and scatterplots can be used bringing various kinds of insights to the surface. We showed how such visualisations could not only be used to explain certain characteristics of the data, such as some nodes having more crossings than others, but also to identify global trends, such as an increase in traffic over the years, and seasonal trends, such as fluctuating popularity within a year. Furthermore, we were able to recognize structural patterns, such as distinct weekday and weekend traffic patterns and by that detect outliers, such as weekend traffic patterns that occur on weekdays. Additionally, we discussed how the selection of colormaps can influence the visibility of given effects. These insights can help in formulating hypotheses to be validated further or can be used in subsequent analysis steps, such as feature engineering and data-driven modelling. We thank you for completing this video series and hope to welcome you in another AI Starter Kit tutorial.","tags":"Advanced Data Visualization","url":"/Advanced Data Visualization/2022-05-08-key-take-away-messages.html","loc":"/Advanced Data Visualization/2022-05-08-key-take-away-messages.html"},{"title":"Key Take Away Messages","text":"Key Take Away Messages In the video tutorial for this AI Starter Kit, we have demonstrated techniques that can be used to preprocess time series data in order to improve its quality for further exploitation. To illustrate the respective methods and techniques, we have used a real-world dataset containing wind turbine data that exhibits the typical characteristics of industrial datasets, including noise, outliers, missing values, and seasonal patterns. First, we demonstrated how resampling and smoothing can be applied to better understand the behaviour of the time series. As we have shown, resampling techniques allow to reduce the temporal granularity, thereby revealing longer-term trends and hiding sharp, fast fluctuations in the signal. Smoothing on the other hand modifies the time series data such that individual points that are higher than the adjacent points - presumably caused by noise - are reduced, and points that are lower than the adjacent points are increased leading to a smoother signal. Subsequently, we showed how the quality of the data can be improved through normalization and outlier detection. Normalization rescales the range of a particular variable in order to make different variables with different ranges comparable. Outlier detection tried to identify the extreme values in the time series that deviate from other observations in the data. The detected outliers can subsequently be examined in more detail to investigate whether these were caused by measurement errors, in which case they can be removed, or if it concerns unexpected phenomena which are interesting to look into in more detail. Finally, another frequently occurring problem with time series data is that it suffers from missing data, caused by for example senor malfunctioning, communication errors, or due to outliers that were removed. We illustrated how to impute this missing data by means of 3 different techniques: linear interpolation, fleet-based imputation and pattern-based interpolation, and outlined what are the advantages and disadvantages of these techniques. The use of these methods allows to improve the quality of the data and to prepare it for further exploration, analysis and modelling purposes. This is a crucial step, as it will improve the completeness and reliability of the input data, and consequently the accuracy of your results. We thank you for completing this video series and hope to welcome you in another AI Starter Kit tutorial.","tags":"Time Series Preprocessing","url":"/Time Series Preprocessing/2022-05-07-key-take-away-messages.html","loc":"/Time Series Preprocessing/2022-05-07-key-take-away-messages.html"},{"title":"Key Take Away Messages","text":"Key Take Away Messages In the video tutorial for this AI Starter Kit, we demonstrated forecasting the demand for a particular resource at a particular point in the future, illustrated on the case of energy demand forecasting. In doing so, we have taken you through the different steps in the data science workflow. We first performed a number of required data preprocessing steps in order to prepare the data for further analysis. This included fusing the electricity and weather data, requiring to resampling the former data to a lower sampling rate. Subsequently, we explored the fused dataset in order to better understand which factors influence the household energy consumption. To this end, we resorted to both visual and statistical exploration methods. Based on this, we could identify a number of yearly, weekly, and daily patterns, including particular behavior during weekends and some holiday periods. This information formed the basis for the feature extraction that followed next, such as the energy consumption a week before, the day of the week and the hour of the day. Finally, we served the extracted features to two different models, namely Random Forest Regression and Support Vector Regression to perform the actual forecasting. Through different experiments, we analyzed the influence of the training strategy, type of machine learning model and effect of data normalization on the model performance. Based on this, it became clear that achieving good prediction results depends on different factors, including the appropriate data preprocessing, the amount of available training data and the algorithm parametrisation. As next steps, we invite you to further explore each of these influences in more detail. Especially with respect to the hyperparameter tuning, it proves worth to further explore the different parameter settings and study how these changes impact the model performance. Furthermore, you can also try to adapt the different steps to the context of your own dataset. While the details of each of the steps might differ, the methodological steps we presented are typically the required phases you need to go through when solving a resource demand forecasting problem. We thank you for completing this video series and hope to welcome you in another AI Starter Kit tutorial. Additional information The video material in this website was developed in the context of the SKAIDive project , financially supported by the European Social Fund , the European Union and Flanders. For more information, please contact us at elucidatalab@sirris.be","tags":"Resource Demand Forecasting","url":"/Resource Demand Forecasting/2022-05-08-key-take-away-messages.html","loc":"/Resource Demand Forecasting/2022-05-08-key-take-away-messages.html"},{"title":"Key Take Away Messages","text":"Key Take Away Messages In the video tutorial for this AI Starter Kit, we covered the basics of using deep learning for remaining useful lifetime prediction, which is one of the central topics in predictive maintenance. We illustrated the approach on the use case of predicting the remaining useful life of aircraft engines based on run-to-failure data. In doing so, we have taken you through the different steps in the data science workflow. We started with the business and data understanding, which allowed us to gain more insights into the specific problem to solve and the structure of the data. Based on this information, we continued with the data preprocessing, in which we explained how to appropriately prepare the data. This included the construction of a training, test and validation set to learn and evaluate a model to predict whether an engine will fail within a certain number of operational cycles. In the video on data modelling and analysis, we showed you how to use deep learning to construct a model for remaining useful life prediction. In particular, we applied a deep learning approach called long short-term memory (LSTM) networks which allows the construction of a predictive model without the time-consuming feature engineering step. This voided the need to manually extract the characteristics from which the algorithm can learn. Finally, we also explained you how to experimentally validate the resulting model and compare the results of different models. While the use of deep learning in some cases might eliminate the need for manual feature engineering, it remains important to tune the models with the appropriate parameter settings, which requires the necessary effort. While the presented approach already gives promising results, there are still quite some improvements possible. Therefore, we recommend you to further try tuning the different parameters, and study the effect on the results. You can also continue to experiment with different network architectures, for example by altering the number of layers and nodes. If you want to gain deeper insights in remaining useful lifetime prediction, a good exercise is trying to cast the prediction problem as a regression or multi-class classification task instead of as a binary classification task. It might also be worth to experiment with the methodology on a larger dataset with a higher number of assets or to try to adapt the different steps to the context of your own dataset. While the details of each of the steps might differ, the methodological steps we presented are typically the required phases you need to go through when solving a remaining useful lifetime prediction model. We thank you for completing this video series and hope to welcome you in another AI Starter Kit tutorial.","tags":"Remaining Useful Life Prediction","url":"/Remaining Useful Life Prediction/2022-05-07-key-take-away-messages.html","loc":"/Remaining Useful Life Prediction/2022-05-07-key-take-away-messages.html"},{"title":"Data Understanding and Preprocessing","text":"Data Understanding and Preprocessing Welcome to the second video of the tutorial for the AI Starter Kit on advanced visualisation. In this video, we will give an overview of the dataset that we will use in this Starter Kit. The data we use is publicly-available data from the City of Seattle. The dataset contains information about bike rides at specific places in the city. At each spot, the number of bikes going in each direction per hour is counted. In the table, you see a random sample of the data. The variable 'Node' refers to the single locations at which the bikes are counted, which is done in two directions. In a first step, we will add some additional variables to the table that will help us later on for the visualisations. From the timestamp column, we extract the date and time, respectively, the hour and the day of the week. Additionally, we replace all possible missing measurements, hence where the variable 'Total' is not set, with zeros. Now, we can extract some basic statistics from the data. For all locations, we have data for at least three and a half years. For one of the locations, Fremont Bridge, the data spans even over 6 years. We can also see that Spokane Street seems to be very popular with more than 1 million bike crossings in 4 and a half years while others only reach about 300.000 counts in the same time span. Before we start with the advanced visualisations, let's briefly check the location of the single nodes on the map. This allows us to verify whether they are close to important areas, like for example tourist or business areas, points-of-interest, etc. In our interactive Starter Kit, you can click on the red circles to see where the single nodes are located. Can you guess which marker is Spokane Street? Spokane Street is on clear bottleneck in Seattle's road network. It is one of the few points at which it is possible to cross from the city center to West Seattle, whereas people can choose many different roads in other areas of the city. Similarly, Fremont Bridge connects Queen Anne and Fremont which are also divided by a canal. In addition, NorthWest 58th Street, 39th Avenue and 26th Avenue are located furthest from the city center, which might explain why they see much less traffic. With this first simple visualisation, we can already explain some characteristics of the data. In the next video, we will concentrate on the temporal evolution and seasonal patterns of the data at the different locations.","tags":"Advanced Data Visualization","url":"/Advanced Data Visualization/2022-05-03-data-understanding-and-preprocessing.html","loc":"/Advanced Data Visualization/2022-05-03-data-understanding-and-preprocessing.html"},{"title":"Introduction","text":"Introduction Welcome to the tutorial for the AI Starter Kit on resource demand forecasting. In this first video we will provide an overview of the actual business case we are tackling. We will introduce the most important concepts and explain why it is beneficial for resource planning to know the expected demand in the future. Resource demand forecasting concerns accurately predicting the future need for a resource, typically using historical information. It is one of the most essential steps in resource demand management, which tries to ensure that sufficient resources are available to satisfy a fluctuating demand. Resource demand forecasting allows one to plan ahead and guarantee that sufficient resources are available when needed and to avoid costly countermeasures in case of shortage. Precisely forecasting the demand of a given resource can be beneficial for several purposes in a wide range of industrial contexts. When knowing the future demand, only the number of resources actually required need to be allocated to a specific task or process. The field ranges from forecasting the number of products that need to be manufactured based on historical sales figures, over predicting the parking demand or traffic density in a particular neighbourhood in order to control traffic in more intelligent ways, to estimating the amount of energy that will be consumed and produced for a particular city or region. Especially in the later field, the demand is influenced by a mixture of external factors such as the weather conditions. For energy suppliers it is more beneficial to buy electricity on the day-ahead market than on the spot market. Consequently, the more accurate the energy consumption can be predicted, the lower the cost. Given that more and more houses and appliances are enabled with a smart meter, an increasing amount of data becomes available that can enable and improve this prediction. Furthermore, these kinds of predictions also allow energy suppliers to better balance demand and supply and helps them to ensure proper grid operation, but also to avoid negative prices, for example in times of high solar irradiation. In this example we see the active power of a single household near Paris shown in green. In orange, the outside temperature is given. The two are strongly correlated – the demand increases with decreasing temperatures since most French households use electrical heating. Therefore, particularly in winter times, the grid has to be stable when facing high demands. In this Starter Kit, we will demonstrate how to effectively forecast the demand for this household for the future by taking external factors into account. In the following videos, we will first start by gaining some first insights in the data in front of us and perform some initial pre-processing steps to prepare the data for the analysis. Subsequently, we will illustrate you how to gain deeper insights by presenting a number of visual and statistical techniques in order to explore the data for identifying interesting patterns. In a next step, useful characteristics or features will be extracted from the dataset that can as serve as input for the forecasting models. Finally, in the last video, we will explain you how to correctly compare the performance of multiple forecasting models. Let us go ahead with the next video on data preparation, in which we will describe how to deal with the typical problems of raw data and how to perform data fusion when multiple sources of data are available. Additional information The video material in this website was developed in the context of the SKAIDive project , financially supported by the European Social Fund , the European Union and Flanders. For more information, please contact us at elucidatalab@sirris.be","tags":"Resource Demand Forecasting","url":"/Resource Demand Forecasting/2022-05-02-introduction.html","loc":"/Resource Demand Forecasting/2022-05-02-introduction.html"},{"title":"Introduction","text":"Introduction Welcome to the tutorial for the AI Starter Kit on advanced visualisation. We will introduce you to a number of more advanced visualisation techniques. You will learn how these can be used creatively to uncover more elaborate insights from your data than would be possible with the more general out-of-the-box visualisations and plots. Data visualisation is an important activity in several phases of a data science project. It allows you to understand several characteristics of a dataset, to discover interesting insights, to validate analysis results, to communicate these results to non-experts via intuitive dashboards, and so on. Popular visualisations often used by data scientists are boxplots, distribution plots, as well as bar and line charts. These are basic out-of-the-box visualisations that are generally applicable, but that are often hard to interpret for non-experts. And more importantly, they do not always reveal interesting insights. More advanced visualisations exploit the human eye's extraordinary visual pattern recognition abilities. A clever visualisation of data can already reveal interesting patterns and insights, even before any complex algorithm is applied. On top of that, they can help in formulating hypotheses to be validated further in the data analytics process or to identify features that can be useful for data-driven modelling. Before we dive deeper in these more advanced visualisation techniques, let us discuss some examples. many time series show some underlying periodicity or seasonality. Depending on the length and granularity of these time series, these seasonal effects are often only hardly visible. In this Starter Kit, we will introduce to you to a number of visualisations that make it very easy to identify for example monthly patterns. Another field where advanced visualisations can help is anomaly detection. Such anomalies appear for example when a machine is failing, and are especially hard to detect when it comes to multi-dimensional data originating from multiple sensors. In this Starter Kit, we will use advanced visualisations such as timeline plots, heatmaps, calendar maps, area plots and scatter plots to visually explore a dataset and gradually build up more knowledge about it. We use a publicly-available dataset that consists of bike counter data, which contains hourly information on the number of bikes that cross at six different spots in Seattle. By visualising this data, we will be able to: explain certain characteristics of the data, such as some nodes having more crossings than others, identify global trends, such as an increase in traffic over the years, and seasonal trends, such as fluctuating popularity within a year, recognize structural patterns, such as distinct weekday and weekend traffic patterns, and finally detect outliers, such as weekend traffic patterns that occur on weekdays. In the next video, we will explain the data in more detail before we continue in the following videos with a number of more advanced visualisation techniques to extract insights from this data.","tags":"Advanced Data Visualization","url":"/Advanced Data Visualization/2022-05-02-introduction.html","loc":"/Advanced Data Visualization/2022-05-02-introduction.html"},{"title":"Introduction","text":"Introduction Welcome to the tutorial for the AI Starter Kit on remaining useful lifetime prediction! In this first video we will provide an overview of the actual business case we're tackling. We will introduce the most important concepts and explain why it is beneficial for maintenance to know the remaining useful lifetime of a machine, a tool or any other industrial asset. Estimating the remaining useful lifetime of an asset can be beneficial for several purposes in a variety of industrial contexts. For one, it can offer support in the better scheduling of maintenance operations, for example for offshore wind turbines. For such assets, maintenance can only be performed under the right weather conditions. But also for other assets, maintenance is an important part of its lifecycle. Traditionally, maintenance is done in a corrective way, such that it is performed at the moment an asset has failed. In that case, the failing part is identified and rectified or replaced, to ensure that the asset can subsequently resume normal operation. Especially for strongly interdependent production lines or industrial assets operating in critical environments, an unplanned downtime is to be avoided and often costly. For this reason, more recently, for an increasing number of assets preventive maintenance is performed. In that case, maintenance tasks are scheduled at regular intervals, avoiding future asset failure to a maximum extent – but with the risk to replace assets that are still working correctly and still could for a while. Hence, the optimal time for replacement is wanted: The time when the asset still works correctly but will probably fail soon. And this is where the remaining useful lifetime comes into play. Nowadays, ever more assets are equipped with different types of sensors gathering data. This started the trend towards predictive maintenance, in which the maintenance moments are decided upon in a data-driven way. Therewith, maintenance is only performed when actually needed. This is a very broad domain which encompasses a variety of topics. Besides the estimation of the asset's remaining useful lifetime – the main topic of this AI starter kit – further related problems to solve for a fully predictive maintenance approach are failure prediction, detection, and diagnosis for root cause analysis. The remaining useful lifetime of industrial assets is defined as an estimate of the remaining time that an item, component, or system is estimated to function as expected. It is expressed as the number of hours, cycles, batches or any other quantity. In the figure we see datapoints collected from an arbitrary sensor in green. Let's say it measures the tool radius of a milling machine. With increasing time, the radius decreases as the tool wears out. At a certain point, the tool is considered too worn out to still further use it in production, which can influence the quality of the produced parts or lead to an unstable production process and consequent damage to the machine. The main questions thus is for how long the tool will it still be able to function properly? By applying Machine Learning algorithms, the continuation of this time series data can be forecasted, denoted in pink. From this forecast, the remaining useful lifetime can be estimated for a given asset. In this case, the machine should be maintained within the next 10 to 15 cycles. With this information, maintenance tasks can be scheduled accordingly – with a tool still functioning and without unexpected downtime. Hence, by estimating the remaining useful lifetime, we minimize the risk of failure and maintenance cost. An accurate prediction of an asset's lifetime can also help to optimise the operational efficiency of an asset, by more optimally planning the use of that asset before its end of life, or adapting its operational use to extend its useful lifetime. In manufacturing settings, where a downtime of the production line results in several operational problems and associated costs, remaining useful lifetime prediction can avoid unplanned downtime. In this AI Starter Kit, we will show its use to avoid safety-critical situations, by illustrating the solution methodology for predicting the remaining useful lifetime of aircrafts engines. To summarize, by applying predictive maintenance it is possible to: - better schedule maintenance operations - optimize operational efficiency - avoid unplanned downtime - anticipate and avoid safety-critical situations. This analysis can be performed for various machines, tools or processes as long as this degradation over time can be measured accordingly, just like resistance, length, temperature or similar measures. Predicting the remaining useful lifetime is typically very challenging for several reasons: First of all, usually a multitude of heterogenous sensor data is measured at several places in the machine. This data is often captured at a high-frequency, consisting of vibration data, acoustic emission, accelerometer data, and many more machine-internal parameters. Secondly, in some settings, also characteristics of the surrounding environment influencing the lifetime of the asset are captured. Further, typically, data that is gathered over a long period of time needs to be available to train the model. Given that industrial assets are often very complex – as they consist of both electrical and mechanical components - it generally is a non-trivial exercise to predict their end of life. On top of that, the data typically comes with a lot of variation, due to varying machine types operating in heterogenous operating conditions with different machine configurations. Therefore, the main challenge in this process is to extract meaningful characteristics that can be used to predict the end of life from the gathered data. This is complicated by the fact that usually very little domain expertise on the operating conditions of these assets is available. Additionally, the environment in which they operate is typically very dynamic. In this AI Starter Kit, we will guide you through a data-driven methodology based on deep learning to tackle this challenge. In the next video, we will concentrate on the data itself that we selected to illustrate the approach. We will explain which information is available and what we can learn from it.","tags":"Remaining Useful Life Prediction","url":"/Remaining Useful Life Prediction/2022-05-02-introduction.html","loc":"/Remaining Useful Life Prediction/2022-05-02-introduction.html"},{"title":"Introduction","text":"Introduction Description Most data mining and machine learning algorithms do not work well if you just feed them your raw data: such data often contains noise and the most relevant distinguishing information is often implicit. For example, raw sensor data just contains a timestamp and a corresponding value, while interesting aspects of it might be the trends contained within or the number of times a threshold is exceeded. Feature engineering is the process of extracting from the raw data the most relevant distinguishing characteristics that will be presented to the algorithm during modeling. It is a way of deriving new information from the existing data, so that its characteristics are more explicitly represented. The resulting features are eventually feed to the AI/ML algorithm of the model. In practice, the feature engineering step is achieved by the selection of the most appropriate parameters, or the composition of new features by manipulation, transformation and combination of the raw data. Feature engineering is one of the most important and creative steps in the data science workflow. However, there is no clearly-defined formal process for engineering features and, consequently, this requires a lot of creativity, a good understanding of the domain and of the available data, some trial-and-error, etc. It is also desirable to have upfront an idea of the modeling and analysis task for which you want to use the resulting features, as this might help you to identify relevant features. Business goal The overall goal of this Starter Kit is to present some advanced feature engineering steps related to feature construction and extraction . By keeping in mind what is your business question and what is the corresponding data science task , you will be able to derive valuable features that can be used in the next stage of your analysis. Application context Feature engineering is one of the steps in the data science workflow with the most decisive impact on the accuracy of the model you want to develop. It is the final step before actually training the model and it defines how the input data will be fed to the model. Starter Kit outline In this Starter Kit we use a dataset from the Regional Rail System of Pennsylvania. Before starting the feature extraction per se , we will first apply some basic preprocessing to the dataset and have a high-level overview of the data. We will then derive several interesting features from the dataset that can be used to characterise train rides and delays.","tags":"Feature Engineering","url":"/Feature Engineering/2023-04-01-introduction.html","loc":"/Feature Engineering/2023-04-01-introduction.html"},{"title":"Introduction","text":"Introduction Welcome to the tutorial for the AI Starter Kit on time-series pre-processing. In this first video we will provide an overview of the actual challenge we're tackling. We will introduce the most important problems occurring in time series data and explain why it is beneficial to first improve your data before you start any other data-modelling tasks. Time series are characterised as a collection of data points obtained at successive times, most often with equal intervals between them. Since an increasing amount of assets is instrumented with sensors, this type of data is omnipresent, for example when monitoring industrial machinery or resulting from wearables tracking one's health state. Depending on the application domain, time series data is exploited for various purposes. It is used, amongst others, for profiling energy consumption of households predicting imminent failures of a production line, estimating the remaining useful lifetime of a machine, and many more industrial use cases. Typically, time series data cannot be used easily as-is by machine learning algorithms. This can be for example due to data quality issues such as missing values and sensor misreadings. Or because some algorithms are not fit to deal with the continuous nature of this type of data or the associated - often high – frequency with which it is gathered. To illustrate the various methods and techniques in this Starter Kit, we will use a publicly available real-world dataset that consists of sensor data measurements from wind turbines. It exhibits typical characteristics of industrial time series datasets as it is very noisy and contains outliers and missing values. Moreover, it exhibits seasonal patterns across multiple years, as the machine behaviour is influenced by the meteorological conditions. We will use this dataset to illustrate how resampling and smoothing can be applied to gain a better understanding of the behaviour of the time series, how the quality of the data can be improved through normalization and outlier detection, and * how missing data can be imputed in various ways. In the next video, we will perform some initial exploration on the dataset in order to gain some basic understanding of the data.","tags":"Time Series Preprocessing","url":"/Time Series Preprocessing/2022-05-02-introduction.html","loc":"/Time Series Preprocessing/2022-05-02-introduction.html"},{"title":"Key Take Away Messages","text":"Key Take Away Messages Congratulations! You completed the tutorial accompanying the AI Starter Kit on data exploration. In this Starter Kit we have demonstrated how appropriate statistical and visualisation techniques can help a data scientist in discovering interesting patterns and gathering insights without applying any complex algorithm. We demonstrated how to explore a dataset by starting from a univariate analysis by means of which we could already get a clear view on who Pronto's customers are with respect to age and gender, where they like to go, and for how long the single bikes are usually in use. By subsequently adding more information and combining different variables, we were able to reveal relationships between gender and most frequently visited stations but also how the bikes are used for different purposes, like a strong commuter's pattern during the week and recreational usage on the weekends. Finally, via a multivariate analysis, we figured out how the elevation influences the biking behaviour. By introducing a cost to the single trips, we could explain why at some stations more bike trips are started while at other stations, more bike trips end. By applying comparably easy visualisations sometimes more insights can be gathered than by applying a sophisticated machine learning model. We thank you for completing this video series and hope to welcome you in another AI Starter Kit tutorial.","tags":"Data Exploration","url":"/Data Exploration/2022-09-08-key-take-away-messages.html","loc":"/Data Exploration/2022-09-08-key-take-away-messages.html"},{"title":"Key Take Away Messages","text":"Key Take Away Messages In the video tutorial for this AI Starter Kit, we have demonstrated techniques that can be used to preprocess time series data in order to improve its quality for further exploitation. To illustrate the respective methods and techniques, we have used a real-world dataset containing wind turbine data that exhibits the typical characteristics of industrial datasets, including noise, outliers, missing values, and seasonal patterns. First, we demonstrated how resampling and smoothing can be applied to better understand the behaviour of the time series. As we have shown, resampling techniques allow to reduce the temporal granularity, thereby revealing longer-term trends and hiding sharp, fast fluctuations in the signal. Smoothing on the other hand modifies the time series data such that individual points that are higher than the adjacent points - presumably caused by noise - are reduced, and points that are lower than the adjacent points are increased leading to a smoother signal. Subsequently, we showed how the quality of the data can be improved through normalization and outlier detection. Normalization rescales the range of a particular variable in order to make different variables with different ranges comparable. Outlier detection tried to identify the extreme values in the time series that deviate from other observations in the data. The detected outliers can subsequently be examined in more detail to investigate whether these were caused by measurement errors, in which case they can be removed, or if it concerns unexpected phenomena which are interesting to look into in more detail. Finally, another frequently occurring problem with time series data is that it suffers from missing data, caused by for example senor malfunctioning, communication errors, or due to outliers that were removed. We illustrated how to impute this missing data by means of 3 different techniques: linear interpolation, fleet-based imputation and pattern-based interpolation, and outlined what are the advantages and disadvantages of these techniques. The use of these methods allows to improve the quality of the data and to prepare it for further exploration, analysis and modelling purposes. This is a crucial step, as it will improve the completeness and reliability of the input data, and consequently the accuracy of your results. We thank you for completing this video series and hope to welcome you in another AI Starter Kit tutorial.","tags":"Time Series Preprocessing","url":"/Time Series Preprocessing/2022-05-07-key-take-away-messages.html","loc":"/Time Series Preprocessing/2022-05-07-key-take-away-messages.html"},{"title":"Key Take Away Messages","text":"Key Take Away Messages In the video tutorial for this AI Starter Kit, we demonstrated forecasting the demand for a particular resource at a particular point in the future, illustrated on the case of energy demand forecasting. In doing so, we have taken you through the different steps in the data science workflow. We first performed a number of required data preprocessing steps in order to prepare the data for further analysis. This included fusing the electricity and weather data, requiring to resampling the former data to a lower sampling rate. Subsequently, we explored the fused dataset in order to better understand which factors influence the household energy consumption. To this end, we resorted to both visual and statistical exploration methods. Based on this, we could identify a number of yearly, weekly, and daily patterns, including particular behavior during weekends and some holiday periods. This information formed the basis for the feature extraction that followed next, such as the energy consumption a week before, the day of the week and the hour of the day. Finally, we served the extracted features to two different models, namely Random Forest Regression and Support Vector Regression to perform the actual forecasting. Through different experiments, we analyzed the influence of the training strategy, type of machine learning model and effect of data normalization on the model performance. Based on this, it became clear that achieving good prediction results depends on different factors, including the appropriate data preprocessing, the amount of available training data and the algorithm parametrisation. As next steps, we invite you to further explore each of these influences in more detail. Especially with respect to the hyperparameter tuning, it proves worth to further explore the different parameter settings and study how these changes impact the model performance. Furthermore, you can also try to adapt the different steps to the context of your own dataset. While the details of each of the steps might differ, the methodological steps we presented are typically the required phases you need to go through when solving a resource demand forecasting problem. We thank you for completing this video series and hope to welcome you in another AI Starter Kit tutorial. Additional information The video material in this website was developed in the context of the SKAIDive project , financially supported by the European Social Fund , the European Union and Flanders. For more information, please contact us at elucidatalab@sirris.be","tags":"Resource Demand Forecasting","url":"/Resource Demand Forecasting/2022-05-08-key-take-away-messages.html","loc":"/Resource Demand Forecasting/2022-05-08-key-take-away-messages.html"},{"title":"Key Take Away Messages","text":"Key Take Away Messages In the video tutorial for this AI Starter Kit, we covered the basics of using deep learning for remaining useful lifetime prediction, which is one of the central topics in predictive maintenance. We illustrated the approach on the use case of predicting the remaining useful life of aircraft engines based on run-to-failure data. In doing so, we have taken you through the different steps in the data science workflow. We started with the business and data understanding, which allowed us to gain more insights into the specific problem to solve and the structure of the data. Based on this information, we continued with the data preprocessing, in which we explained how to appropriately prepare the data. This included the construction of a training, test and validation set to learn and evaluate a model to predict whether an engine will fail within a certain number of operational cycles. In the video on data modelling and analysis, we showed you how to use deep learning to construct a model for remaining useful life prediction. In particular, we applied a deep learning approach called long short-term memory (LSTM) networks which allows the construction of a predictive model without the time-consuming feature engineering step. This voided the need to manually extract the characteristics from which the algorithm can learn. Finally, we also explained you how to experimentally validate the resulting model and compare the results of different models. While the use of deep learning in some cases might eliminate the need for manual feature engineering, it remains important to tune the models with the appropriate parameter settings, which requires the necessary effort. While the presented approach already gives promising results, there are still quite some improvements possible. Therefore, we recommend you to further try tuning the different parameters, and study the effect on the results. You can also continue to experiment with different network architectures, for example by altering the number of layers and nodes. If you want to gain deeper insights in remaining useful lifetime prediction, a good exercise is trying to cast the prediction problem as a regression or multi-class classification task instead of as a binary classification task. It might also be worth to experiment with the methodology on a larger dataset with a higher number of assets or to try to adapt the different steps to the context of your own dataset. While the details of each of the steps might differ, the methodological steps we presented are typically the required phases you need to go through when solving a remaining useful lifetime prediction model. We thank you for completing this video series and hope to welcome you in another AI Starter Kit tutorial.","tags":"Remaining Useful Life Prediction","url":"/Remaining Useful Life Prediction/2022-05-07-key-take-away-messages.html","loc":"/Remaining Useful Life Prediction/2022-05-07-key-take-away-messages.html"},{"title":"Key Take Away Messages","text":"Key Take Away Messages Congratulations! You completed the tutorial accompanying the AI Starter Kit on data exploration. In this Starter Kit we have demonstrated how appropriate statistical and visualisation techniques can help a data scientist in discovering interesting patterns and gathering insights without applying any complex algorithm. We demonstrated how to explore a dataset by starting from a univariate analysis by means of which we could already get a clear view on who Pronto's customers are with respect to age and gender, where they like to go, and for how long the single bikes are usually in use. By subsequently adding more information and combining different variables, we were able to reveal relationships between gender and most frequently visited stations but also how the bikes are used for different purposes, like a strong commuter's pattern during the week and recreational usage on the weekends. Finally, via a multivariate analysis, we figured out how the elevation influences the biking behaviour. By introducing a cost to the single trips, we could explain why at some stations more bike trips are started while at other stations, more bike trips end. By applying comparably easy visualisations sometimes more insights can be gathered than by applying a sophisticated machine learning model. We thank you for completing this video series and hope to welcome you in another AI Starter Kit tutorial.","tags":"Data Exploration","url":"/Data Exploration/2022-09-08-key-take-away-messages.html","loc":"/Data Exploration/2022-09-08-key-take-away-messages.html"},{"title":"Key Take Away Messages","text":"Key Take Away Messages In this Starter Kit we have demonstrated how creative visualisations can already reveal interesting patterns and more elaborate insights in your data, even before any complex algorithm is applied. In particular, we have used bike counter data to illustrate how timeline plots, different types of heatmaps, streamgraph plots and scatterplots can be used bringing various kinds of insights to the surface. We showed how such visualisations could not only be used to explain certain characteristics of the data, such as some nodes having more crossings than others, but also to identify global trends, such as an increase in traffic over the years, and seasonal trends, such as fluctuating popularity within a year. Furthermore, we were able to recognize structural patterns, such as distinct weekday and weekend traffic patterns and by that detect outliers, such as weekend traffic patterns that occur on weekdays. Additionally, we discussed how the selection of colormaps can influence the visibility of given effects. These insights can help in formulating hypotheses to be validated further or can be used in subsequent analysis steps, such as feature engineering and data-driven modelling. We thank you for completing this video series and hope to welcome you in another AI Starter Kit tutorial.","tags":"Advanced Data Visualization","url":"/Advanced Data Visualization/2022-05-08-key-take-away-messages.html","loc":"/Advanced Data Visualization/2022-05-08-key-take-away-messages.html"},{"title":"Key Take Away Messages","text":"Key Take Away Messages In the video tutorial for this AI Starter Kit, we have demonstrated techniques that can be used to preprocess time series data in order to improve its quality for further exploitation. To illustrate the respective methods and techniques, we have used a real-world dataset containing wind turbine data that exhibits the typical characteristics of industrial datasets, including noise, outliers, missing values, and seasonal patterns. First, we demonstrated how resampling and smoothing can be applied to better understand the behaviour of the time series. As we have shown, resampling techniques allow to reduce the temporal granularity, thereby revealing longer-term trends and hiding sharp, fast fluctuations in the signal. Smoothing on the other hand modifies the time series data such that individual points that are higher than the adjacent points - presumably caused by noise - are reduced, and points that are lower than the adjacent points are increased leading to a smoother signal. Subsequently, we showed how the quality of the data can be improved through normalization and outlier detection. Normalization rescales the range of a particular variable in order to make different variables with different ranges comparable. Outlier detection tried to identify the extreme values in the time series that deviate from other observations in the data. The detected outliers can subsequently be examined in more detail to investigate whether these were caused by measurement errors, in which case they can be removed, or if it concerns unexpected phenomena which are interesting to look into in more detail. Finally, another frequently occurring problem with time series data is that it suffers from missing data, caused by for example senor malfunctioning, communication errors, or due to outliers that were removed. We illustrated how to impute this missing data by means of 3 different techniques: linear interpolation, fleet-based imputation and pattern-based interpolation, and outlined what are the advantages and disadvantages of these techniques. The use of these methods allows to improve the quality of the data and to prepare it for further exploration, analysis and modelling purposes. This is a crucial step, as it will improve the completeness and reliability of the input data, and consequently the accuracy of your results. We thank you for completing this video series and hope to welcome you in another AI Starter Kit tutorial.","tags":"Time Series Preprocessing","url":"/Time Series Preprocessing/2022-05-07-key-take-away-messages.html","loc":"/Time Series Preprocessing/2022-05-07-key-take-away-messages.html"},{"title":"Key Take Away Messages","text":"Key Take Away Messages In the video tutorial for this AI Starter Kit, we covered the basics of using deep learning for remaining useful lifetime prediction, which is one of the central topics in predictive maintenance. We illustrated the approach on the use case of predicting the remaining useful life of aircraft engines based on run-to-failure data. In doing so, we have taken you through the different steps in the data science workflow. We started with the business and data understanding, which allowed us to gain more insights into the specific problem to solve and the structure of the data. Based on this information, we continued with the data preprocessing, in which we explained how to appropriately prepare the data. This included the construction of a training, test and validation set to learn and evaluate a model to predict whether an engine will fail within a certain number of operational cycles. In the video on data modelling and analysis, we showed you how to use deep learning to construct a model for remaining useful life prediction. In particular, we applied a deep learning approach called long short-term memory (LSTM) networks which allows the construction of a predictive model without the time-consuming feature engineering step. This voided the need to manually extract the characteristics from which the algorithm can learn. Finally, we also explained you how to experimentally validate the resulting model and compare the results of different models. While the use of deep learning in some cases might eliminate the need for manual feature engineering, it remains important to tune the models with the appropriate parameter settings, which requires the necessary effort. While the presented approach already gives promising results, there are still quite some improvements possible. Therefore, we recommend you to further try tuning the different parameters, and study the effect on the results. You can also continue to experiment with different network architectures, for example by altering the number of layers and nodes. If you want to gain deeper insights in remaining useful lifetime prediction, a good exercise is trying to cast the prediction problem as a regression or multi-class classification task instead of as a binary classification task. It might also be worth to experiment with the methodology on a larger dataset with a higher number of assets or to try to adapt the different steps to the context of your own dataset. While the details of each of the steps might differ, the methodological steps we presented are typically the required phases you need to go through when solving a remaining useful lifetime prediction model. We thank you for completing this video series and hope to welcome you in another AI Starter Kit tutorial.","tags":"Remaining Useful Life Prediction","url":"/Remaining Useful Life Prediction/2022-05-07-key-take-away-messages.html","loc":"/Remaining Useful Life Prediction/2022-05-07-key-take-away-messages.html"},{"title":"Key Take Away Messages","text":"Key Take Away Messages Congratulations! You completed the tutorial accompanying the AI Starter Kit on data exploration. In this Starter Kit we have demonstrated how appropriate statistical and visualisation techniques can help a data scientist in discovering interesting patterns and gathering insights without applying any complex algorithm. We demonstrated how to explore a dataset by starting from a univariate analysis by means of which we could already get a clear view on who Pronto's customers are with respect to age and gender, where they like to go, and for how long the single bikes are usually in use. By subsequently adding more information and combining different variables, we were able to reveal relationships between gender and most frequently visited stations but also how the bikes are used for different purposes, like a strong commuter's pattern during the week and recreational usage on the weekends. Finally, via a multivariate analysis, we figured out how the elevation influences the biking behaviour. By introducing a cost to the single trips, we could explain why at some stations more bike trips are started while at other stations, more bike trips end. By applying comparably easy visualisations sometimes more insights can be gathered than by applying a sophisticated machine learning model. We thank you for completing this video series and hope to welcome you in another AI Starter Kit tutorial.","tags":"Data Exploration","url":"/Data Exploration/2022-09-08-key-take-away-messages.html","loc":"/Data Exploration/2022-09-08-key-take-away-messages.html"},{"title":"Key Take Away Messages","text":"Key Take Away Messages In this Starter Kit we have demonstrated how creative visualisations can already reveal interesting patterns and more elaborate insights in your data, even before any complex algorithm is applied. In particular, we have used bike counter data to illustrate how timeline plots, different types of heatmaps, streamgraph plots and scatterplots can be used bringing various kinds of insights to the surface. We showed how such visualisations could not only be used to explain certain characteristics of the data, such as some nodes having more crossings than others, but also to identify global trends, such as an increase in traffic over the years, and seasonal trends, such as fluctuating popularity within a year. Furthermore, we were able to recognize structural patterns, such as distinct weekday and weekend traffic patterns and by that detect outliers, such as weekend traffic patterns that occur on weekdays. Additionally, we discussed how the selection of colormaps can influence the visibility of given effects. These insights can help in formulating hypotheses to be validated further or can be used in subsequent analysis steps, such as feature engineering and data-driven modelling. We thank you for completing this video series and hope to welcome you in another AI Starter Kit tutorial.","tags":"Advanced Data Visualization","url":"/Advanced Data Visualization/2022-05-08-key-take-away-messages.html","loc":"/Advanced Data Visualization/2022-05-08-key-take-away-messages.html"},{"title":"Key Take Away Messages","text":"Key Take Away Messages In the video tutorial for this AI Starter Kit, we demonstrated forecasting the demand for a particular resource at a particular point in the future, illustrated on the case of energy demand forecasting. In doing so, we have taken you through the different steps in the data science workflow. We first performed a number of required data preprocessing steps in order to prepare the data for further analysis. This included fusing the electricity and weather data, requiring to resampling the former data to a lower sampling rate. Subsequently, we explored the fused dataset in order to better understand which factors influence the household energy consumption. To this end, we resorted to both visual and statistical exploration methods. Based on this, we could identify a number of yearly, weekly, and daily patterns, including particular behavior during weekends and some holiday periods. This information formed the basis for the feature extraction that followed next, such as the energy consumption a week before, the day of the week and the hour of the day. Finally, we served the extracted features to two different models, namely Random Forest Regression and Support Vector Regression to perform the actual forecasting. Through different experiments, we analyzed the influence of the training strategy, type of machine learning model and effect of data normalization on the model performance. Based on this, it became clear that achieving good prediction results depends on different factors, including the appropriate data preprocessing, the amount of available training data and the algorithm parametrisation. As next steps, we invite you to further explore each of these influences in more detail. Especially with respect to the hyperparameter tuning, it proves worth to further explore the different parameter settings and study how these changes impact the model performance. Furthermore, you can also try to adapt the different steps to the context of your own dataset. While the details of each of the steps might differ, the methodological steps we presented are typically the required phases you need to go through when solving a resource demand forecasting problem. We thank you for completing this video series and hope to welcome you in another AI Starter Kit tutorial. Additional information The video material in this website was developed in the context of the SKAIDive project , financially supported by the European Social Fund , the European Union and Flanders. For more information, please contact us at elucidatalab@sirris.be","tags":"Resource Demand Forecasting","url":"/Resource Demand Forecasting/2022-05-08-key-take-away-messages.html","loc":"/Resource Demand Forecasting/2022-05-08-key-take-away-messages.html"},{"title":"Key Take Away Messages","text":"Key Take Away Messages In the video tutorial for this AI Starter Kit, we covered the basics of using deep learning for remaining useful lifetime prediction, which is one of the central topics in predictive maintenance. We illustrated the approach on the use case of predicting the remaining useful life of aircraft engines based on run-to-failure data. In doing so, we have taken you through the different steps in the data science workflow. We started with the business and data understanding, which allowed us to gain more insights into the specific problem to solve and the structure of the data. Based on this information, we continued with the data preprocessing, in which we explained how to appropriately prepare the data. This included the construction of a training, test and validation set to learn and evaluate a model to predict whether an engine will fail within a certain number of operational cycles. In the video on data modelling and analysis, we showed you how to use deep learning to construct a model for remaining useful life prediction. In particular, we applied a deep learning approach called long short-term memory (LSTM) networks which allows the construction of a predictive model without the time-consuming feature engineering step. This voided the need to manually extract the characteristics from which the algorithm can learn. Finally, we also explained you how to experimentally validate the resulting model and compare the results of different models. While the use of deep learning in some cases might eliminate the need for manual feature engineering, it remains important to tune the models with the appropriate parameter settings, which requires the necessary effort. While the presented approach already gives promising results, there are still quite some improvements possible. Therefore, we recommend you to further try tuning the different parameters, and study the effect on the results. You can also continue to experiment with different network architectures, for example by altering the number of layers and nodes. If you want to gain deeper insights in remaining useful lifetime prediction, a good exercise is trying to cast the prediction problem as a regression or multi-class classification task instead of as a binary classification task. It might also be worth to experiment with the methodology on a larger dataset with a higher number of assets or to try to adapt the different steps to the context of your own dataset. While the details of each of the steps might differ, the methodological steps we presented are typically the required phases you need to go through when solving a remaining useful lifetime prediction model. We thank you for completing this video series and hope to welcome you in another AI Starter Kit tutorial.","tags":"Remaining Useful Life Prediction","url":"/Remaining Useful Life Prediction/2022-05-07-key-take-away-messages.html","loc":"/Remaining Useful Life Prediction/2022-05-07-key-take-away-messages.html"},{"title":"Key Take Away Messages","text":"Key Take Away Messages Congratulations! You completed the tutorial accompanying the AI Starter Kit on data exploration. In this Starter Kit we have demonstrated how appropriate statistical and visualisation techniques can help a data scientist in discovering interesting patterns and gathering insights without applying any complex algorithm. We demonstrated how to explore a dataset by starting from a univariate analysis by means of which we could already get a clear view on who Pronto's customers are with respect to age and gender, where they like to go, and for how long the single bikes are usually in use. By subsequently adding more information and combining different variables, we were able to reveal relationships between gender and most frequently visited stations but also how the bikes are used for different purposes, like a strong commuter's pattern during the week and recreational usage on the weekends. Finally, via a multivariate analysis, we figured out how the elevation influences the biking behaviour. By introducing a cost to the single trips, we could explain why at some stations more bike trips are started while at other stations, more bike trips end. By applying comparably easy visualisations sometimes more insights can be gathered than by applying a sophisticated machine learning model. We thank you for completing this video series and hope to welcome you in another AI Starter Kit tutorial.","tags":"Data Exploration","url":"/Data Exploration/2022-09-08-key-take-away-messages.html","loc":"/Data Exploration/2022-09-08-key-take-away-messages.html"},{"title":"Key Take Away Messages","text":"Key Take Away Messages In this Starter Kit we have demonstrated how creative visualisations can already reveal interesting patterns and more elaborate insights in your data, even before any complex algorithm is applied. In particular, we have used bike counter data to illustrate how timeline plots, different types of heatmaps, streamgraph plots and scatterplots can be used bringing various kinds of insights to the surface. We showed how such visualisations could not only be used to explain certain characteristics of the data, such as some nodes having more crossings than others, but also to identify global trends, such as an increase in traffic over the years, and seasonal trends, such as fluctuating popularity within a year. Furthermore, we were able to recognize structural patterns, such as distinct weekday and weekend traffic patterns and by that detect outliers, such as weekend traffic patterns that occur on weekdays. Additionally, we discussed how the selection of colormaps can influence the visibility of given effects. These insights can help in formulating hypotheses to be validated further or can be used in subsequent analysis steps, such as feature engineering and data-driven modelling. We thank you for completing this video series and hope to welcome you in another AI Starter Kit tutorial.","tags":"Advanced Data Visualization","url":"/Advanced Data Visualization/2022-05-08-key-take-away-messages.html","loc":"/Advanced Data Visualization/2022-05-08-key-take-away-messages.html"},{"title":"Key Take Away Messages","text":"Key Take Away Messages In the video tutorial for this AI Starter Kit, we have demonstrated techniques that can be used to preprocess time series data in order to improve its quality for further exploitation. To illustrate the respective methods and techniques, we have used a real-world dataset containing wind turbine data that exhibits the typical characteristics of industrial datasets, including noise, outliers, missing values, and seasonal patterns. First, we demonstrated how resampling and smoothing can be applied to better understand the behaviour of the time series. As we have shown, resampling techniques allow to reduce the temporal granularity, thereby revealing longer-term trends and hiding sharp, fast fluctuations in the signal. Smoothing on the other hand modifies the time series data such that individual points that are higher than the adjacent points - presumably caused by noise - are reduced, and points that are lower than the adjacent points are increased leading to a smoother signal. Subsequently, we showed how the quality of the data can be improved through normalization and outlier detection. Normalization rescales the range of a particular variable in order to make different variables with different ranges comparable. Outlier detection tried to identify the extreme values in the time series that deviate from other observations in the data. The detected outliers can subsequently be examined in more detail to investigate whether these were caused by measurement errors, in which case they can be removed, or if it concerns unexpected phenomena which are interesting to look into in more detail. Finally, another frequently occurring problem with time series data is that it suffers from missing data, caused by for example senor malfunctioning, communication errors, or due to outliers that were removed. We illustrated how to impute this missing data by means of 3 different techniques: linear interpolation, fleet-based imputation and pattern-based interpolation, and outlined what are the advantages and disadvantages of these techniques. The use of these methods allows to improve the quality of the data and to prepare it for further exploration, analysis and modelling purposes. This is a crucial step, as it will improve the completeness and reliability of the input data, and consequently the accuracy of your results. We thank you for completing this video series and hope to welcome you in another AI Starter Kit tutorial.","tags":"Time Series Preprocessing","url":"/Time Series Preprocessing/2022-05-07-key-take-away-messages.html","loc":"/Time Series Preprocessing/2022-05-07-key-take-away-messages.html"},{"title":"Key Take Away Messages","text":"Key Take Away Messages In the video tutorial for this AI Starter Kit, we demonstrated forecasting the demand for a particular resource at a particular point in the future, illustrated on the case of energy demand forecasting. In doing so, we have taken you through the different steps in the data science workflow. We first performed a number of required data preprocessing steps in order to prepare the data for further analysis. This included fusing the electricity and weather data, requiring to resampling the former data to a lower sampling rate. Subsequently, we explored the fused dataset in order to better understand which factors influence the household energy consumption. To this end, we resorted to both visual and statistical exploration methods. Based on this, we could identify a number of yearly, weekly, and daily patterns, including particular behavior during weekends and some holiday periods. This information formed the basis for the feature extraction that followed next, such as the energy consumption a week before, the day of the week and the hour of the day. Finally, we served the extracted features to two different models, namely Random Forest Regression and Support Vector Regression to perform the actual forecasting. Through different experiments, we analyzed the influence of the training strategy, type of machine learning model and effect of data normalization on the model performance. Based on this, it became clear that achieving good prediction results depends on different factors, including the appropriate data preprocessing, the amount of available training data and the algorithm parametrisation. As next steps, we invite you to further explore each of these influences in more detail. Especially with respect to the hyperparameter tuning, it proves worth to further explore the different parameter settings and study how these changes impact the model performance. Furthermore, you can also try to adapt the different steps to the context of your own dataset. While the details of each of the steps might differ, the methodological steps we presented are typically the required phases you need to go through when solving a resource demand forecasting problem. We thank you for completing this video series and hope to welcome you in another AI Starter Kit tutorial. Additional information The video material in this website was developed in the context of the SKAIDive project , financially supported by the European Social Fund , the European Union and Flanders. For more information, please contact us at elucidatalab@sirris.be","tags":"Resource Demand Forecasting","url":"/Resource Demand Forecasting/2022-05-08-key-take-away-messages.html","loc":"/Resource Demand Forecasting/2022-05-08-key-take-away-messages.html"},{"title":"Data Preprocessing","text":"Data Preprocessing Welcome to the second video of the tutorial for the AI Starter Kit on resource demand forecasting! In this video, we will detail the dataset that we will use and perform the necessary preprocessing steps in order to prepare the data for further analysis. In this AI Starter Kit, we will work with a publicly available energy consumption dataset from the UCI repository. This dataset contains measurements of electric power consumption in one household, located near Paris, with a one-minute sampling rate over a period of almost 4 years. Different electrical quantities and sub-metering values are available, as you can see in the table. Let us first have a look at the single variables given in the table: First of all, and most importantly, the global active and reactive powers are given. The active power specifies the amount of energy the single electric receivers transform into mechanical work and heat. This useful effect is called 'active energy'. On the contrary, several receivers, such as motors, transformer, or inductive cookers need magnetic fields in order to work. This amount is called reactive power as these receivers are generally inductive: This means, they absorb energy from the network in order to create magnetic fields and return it while they disappear. This results in an additional electrical consumption that is not useful for the receivers. Further, the voltage and current intensity are given in Volts and Ampere, respectively. Finally, the submeterings provide us a more detailed insights where the power is consumed. Submetering 1 corresponds to the kitchen, containing mainly a dishwasher, an oven, and a microwave. Submetering 2 corresponds to the laundry room, containing a washing-machine, a tumble-drier, a refrigerator and a light. Lastly, submetering 3 corresponds to an electric water-heater and an air-conditioner. We expect, that the different submeterings obey different patterns: While an oven mainly consumes power during lunch and dinner time, a refrigerator has a more or less constant consumption during the entire day. As mentioned before, we will also take weather measurements into account. For this, we use measurements recorded close to the city of Paris, where the household is located, provided by the Wunderground website. It contains information on climate conditions on a 30-minutes base. The dataset contains the outside temperature and the dew point in degrees Celsius, the air humidity in percentage, the pressure at sea level in hPa, the visibility in km and the wind direction expressed in degrees. The weather data is measured by a set of connected sensors. For these, it is rare that the data availability is uninterrupted for such a long period. For this reason, we first check the general statistics for the data set. For most variables, the minimal value is -9999, which indicates missing values as also specified in the documentation of the data. We will replace these with so-called 'Not a Number' values or NaNs such that they will not interfere too much during data modelling. In a next step, we fuse the electricity data and the weather data. Note, that the temporal resolution of the two is not the same – the power data is measured every minute while the weather information is available only every 30 minutes. The integration of these two datasets thus requires careful consideration. Two options are possible: we can either upsample the weather data or downsample the household data. That means that we either linearly interpolate the information from weather data on a one-minute interval or aggregate the power data on bigger intervals. In this use case, the latter is more reasonable, as otherwise, we would need to find a way to impute missing data in the climate dataset. Let us first analyze the effect of downsampling the power data. Therefore, we calculate the rolling mean values of the original power over a given time window. You can choose between a sampling rate of 30 or 60 minutes, or 4 hours. The light blue time series shows the original data, the dark blue one the downsampled one. In all cases, the data looks smoother as the original data, as taking the mean value smooths out the major short-time power peaks but at the same time results in some loss of information. With the larger window size of 60 minutes this effect is stronger than for the smaller one. A window size of 4 hours arguably removes too much variation. Note that the weather data is available every 30 minutes such that in case of an hourly time window for the power data also the weather data has to be resampled. Since the information loss for the 60-minute interval is reasonable, we decide to proceed with this sampling rate. This also allows us to reduce the size of the dataset, which will reduce the computation time required for the analysis. In order to merge the two data sets, we transform the active power from a minutes-based power measurement given in kW to an hourly consumption given in Wh. Therefore, we sum the power over one hour and divide it by 60 to get the actual energy. By additionally multiplying by a factor of 1000, we change units from kWh to Wh. For the weather data, we proceed similarly by taking the mean temperature over one hour. With this, we can join the two data sets. For future analysis, we will only take the global consumption and temperature into account. Now that we have preprocessed the dataset, we will illustrate how to gain deeper insights in the data through both visual and statistical data exploration. In the next video, we will start with the visual exploration by means of time plots. Additional information The video material in this website was developed in the context of the SKAIDive project , financially supported by the European Social Fund , the European Union and Flanders. For more information, please contact us at elucidatalab@sirris.be","tags":"Resource Demand Forecasting","url":"/Resource Demand Forecasting/2022-05-03-data-preprocessing.html","loc":"/Resource Demand Forecasting/2022-05-03-data-preprocessing.html"},{"title":"Outlier Detection","text":"Outlier Detection Welcome to the fourth video in the tutorial on advanced visualisation. In this video, we will concentrate on outlier detection. In particular, at which days does the people's biking behaviour deviate from the expected one. The visualisations presented in the former videos already allowed us to identify outliers. We saw for example that the traffic at NorthWest 58th Street seems to follow other rules than the other nodes in the network. In this video, we will perform a more detailed analysis of outliers in order to identify them more precisely. This table presents another way of looking at the data. For each node we have 24 observations per day, corresponding to the hourly total number of bike passings through that node. As humans, we cannot easily interpret such a table by just staring at it. However, it is also difficult to visualise such data because it is multi-dimensional meaning that it has 24 values for 6 nodes for thousands of days. However, we can still visualise that data by first performing a so-called dimensionality reduction technique. One of these techniques is Principal Components Analysis or PCA. The idea behind PCA is to find a reduced set of axes that summarize the data. More concretely, PCA is a statistical procedure that transforms the multidimensional data provided as input into a reduced set of orthogonal - and with that uncorrelated - dimensions. These dimensions are called principal components. By using PCA, we can reduce the dimensionality of the data in the above table to two dimensions and plot the result using a scatterplot, as shown here. Scatterplots typically visualise two variables of a dataset along the X and Y axis, respectively. Additional information can also be visualised by changing the colour or size of the dots though. Scatterplots are useful to identify relationships between two variables, such as correlation, and to identify separate groups in the data which can be useful for subsequent clustering. The scatterplot shown here has the shape of an 'L' and seems thus to indicate that there are roughly two groups in the data, corresponding to the two line segments forming the shape of that letter. We can now check in the interactive Starter Kit whether these two groups are present for all nodes. From the above analyses, we might suspect different results for NorthWest 58th Street and 26th Avenue compared to the other nodes. Furthermore, the Starter Kit also offers the possibility to change the colormap indicating the day of the week. What kind of colormap would you use to visualise the different days of the week? Also here, some colorscales work better than others. One the one hand, we could use sequential (YlGn) or diverging (RdBu) colormaps. When applying these to the data, it is hard to distinguish the data from the different days. These colormaps work better for continuous data, such as the heatmaps that we showed in the previous video. For categorical data as in this figure, a qualitative colormap, like colorblind, works much better. As the name suggests, this colormap has the added advantage that it is very readable for people with varying form of colorblindness. When using the colorblind colormap for Fremont Bridge, we can clearly see the L-shape. On top of that, we see that the two arms mainly belong to either weekdays or weekends. The dots corresponding to Fridays seem to be closer to the weekend pattern though. When we select the data from NorthWest 58th Street, we get a significantly different picture. The two groups are not as clearly separated as before, but Saturdays and Sundays typically correspond to points higher in the figure. Getting back to Fremont Bridge, also here a couple of weekdays seem to behave like weekend days. We can observe some of these dots in the upper left part of the plot for all nodes. We suspect these latter dots to be outliers. We can automatically identify them by first applying a clustering algorithm for grouping the datapoints in both of the arms together. Then, we can check that the points in each of these two categories correspond to weekend days or weekdays, respectively. Those which do not follow this pattern can be considered outliers. To cluster and hence separate these points we use a Gaussian Mixture Model. It is a clustering technique particularly suited to oval-like shapes like those corresponding to the two categories we would like to separate. As training data, we only consider a single node at a time, but in general, the model can be trained on the data for all the nodes that exhibit the commute pattern. We only do this for those nodes where the two distinct groups could be identified, so not for NW 58th Street and 26th Avenue. As can be seen from the plot, the Gaussian mixture model is quite good in separating the two groups. With the colours indicating the cluster, we can easily identify the outliers, that is those days that are actually weekdays but that are assigned to the weekend day cluster. The heatmap plot shows the date corresponding to those outliers. By further checking the US federal holiday calendar, we can see whether the resulting dates corresponded to a holiday, in which case the name of the holiday is indicated on the Y axis. We can see that all outliers can be explained: either they are weekdays that are actually holidays or weekdays that are other special days, for example the Friday after Thanksgiving, the days before and after Christmas, and New Year's Eve. Note that not all holidays are identified as outliers, for example Thanksgiving should occur every fourth Thursday of November, but we did not identify it as an outlier in 2016, 2017 and 2018. This might be due to an inaccuracy of the model, which would require a more elaborate preprocessing of the data in order to resolve this, yet this is out of scope of this Starter Kit.","tags":"Advanced Data Visualization","url":"/Advanced Data Visualization/2022-05-05-outlier-detection.html","loc":"/Advanced Data Visualization/2022-05-05-outlier-detection.html"},{"title":"Outlier Detection","text":"Outlier Detection In the previous video we explored the data mainly in a visual manner. In this fourth video, we will discuss how to detect outliers in the data and how different methods can be used to improve the data quality. There are several different techniques for outlier detection in time-series. Here, we will focus on online outlier detection, that means the detection of an outlier as soon as it occurs, as opposed to an offline detection, which happens retrospectively. We present two different approaches for outlier detection using temperature and wind speed variables. A relatively simple and frequently used approach for outlier detection is based on the boxplot data distribution. For a given attribute, this method computes its interquartile range or IQR, which is the difference between the 25th and 75th percentiles. This value is then multiplied by a constant factor 𝛼 which determines how stringent the outlier detection is. A typical value for 𝛼 is 1.5, although this value can be adapted according to the level of stringiness desired. Indeed, larger values will push the outlier boundaries further and thereby reducing the number of detected outliers. The resulting value is subtracted from the 25th and added to the 75th percentiles to obtain the lower and upper fences, respectively, which define the thresholds beyond which a given value is labelled as an outlier. Considering the seasonal nature of the data, we should ensure that the outlier detection approach takes the impact of seasonality into account. It is known that the temperature has significant seasonal variation as it varies between day and night or between winter and summer for example and the same temperature in winter and in summer can be considered as outlier in one case, but not in the other. Therefore, the seasonal trend decomposition described in the former video is applied to the signal and the residuals are used as input for the outlier detection. In doing so, we only take the distance to the seasonal pattern into account for the outlier detection. In the example shown on the right, we identify outlier events based on a given 𝛼 value. We define outlier events as outliers that are consecutive in time. An additional input parameter allows to merge outlier events that are separated by less than a given amount of time. When using the most stringent alpha value - in our case – we detect a single outlier in the dataset: namely one of the turbines measured a temperature of -273 degrees for a period of time. Further, we can visualize the time series around the time of a given outlier event. The Flank duration parameter allows you to control the time window around the outlier for the visualization. The left figure shows the time series with the outlier event highlighted in blue, while the figure on the right shows the distribution of all the temperature residual values using a boxplot. Again, the points in blue indicate the outlier event depicted on the left. When increasing the flank duration, we clearly see that this measurement is an outlier. Of course, to identify an outlier with a temperature of -273 degrees, you don't need data science. Hence, We also have a look at another outlier detected with a smaller value for alpha, namely 1.5. In this case, the outlier is not as evident as before. On the 23rd of August, the temperature was much higher than on a normal day in August and warmer than the days before and after. An important aspect we want to discuss in this respect is the influence of outlier detection and removal on normalization. Normalization is a typical data pre-processing step where the range of a variable is standardized, meaning rescaled in order to make different variables with different ranges comparable. It is an important pre-processing step before the data is presented to a machine learning algorithm, as it ensures all variables have equal importance. Different normalization approaches exist. Examples are rescaling the values in the 0-1 range, known as min-max normalization, or removing the mean and scaling to unit variance, known as z-score or standard score normalization. Most of these approaches are sensitive to outliers: For example, in the case of min-max normalization, the minimum value is mapped to 0 and the maximum to 1 so obviously extreme outliers will have a large impact. In the starterkit, we can test these two normalization approaches on each of the attributes of the dataset. We can enable or disable the outlier removal in order to appreciate how this affects the normalization procedure. This effect is most striking when looking at the temperature attribute. If we do not remove the outliers, the min-max normalization is meaningless as 0 is mapped onto -273 degrees. All remaining values are then in the range between 0.8 and 1. When removing the outliers, the range between 0 and 1 is equally dense and the normalization reflects the seasonality in the data. A second approach is the so-called Fleet-based outlier detection. For detecting outliers of the power attribute, we will use this alternative approach. Note, that this is only for the purpose of demonstration and that we could also use the interquartile range-base outlier detection for the power attribute. The approach we will use is based on exploiting the fleet aspect, which is exemplified by wind turbines, which typically operate as part of a wind park in the same environment under similar environmental conditions. At each point in time, we compute the median power recorded by the fleet and we consider any value that deviates too much from that median value to be an outlier. To determine what constitutes too much deviation, we again consider the boxplot outlier definition. If a value is beyond 5 times the IQR from the 25th or 75th percentile, we consider the observation to be an outlier. To exclude periods during which all the turbines in the fleet were not operational, we only consider time points when at least 3 of the 4 turbines recorded a power production above 0. The starter kit allows us to explore all the detected outliers in the power attribute using this definition. We can change the Flank parameter in order to see a larger time window around the outlier. It can be observed that the outliers often happen in periods of time when the power of a given turbine dropped to 0 without it being the case for the remaining turbines. There are, nonetheless, other instances, when the produced power of a given turbine was - statistically speaking – above what would be expected given the behaviour of the remaining turbines. Note that the grey square highlights only the fleet outlier event in question. Other points in the visualization might also be labelled as outliers but they are not part of the same event. The fleet-based approach has the advantage of being able to capture outliers at a specific moment in time only relying on sensor values captured at that time. On the contrary, this approach can only be applied if the dataset contains a fleet of co-located assets, meaning that they are exposed to similar conditions. In the next video, we will discuss how we can impute missing data in the dataset and therewith improve the quality of the data.","tags":"Time Series Preprocessing","url":"/Time Series Preprocessing/2022-05-05-outlier-detection.html","loc":"/Time Series Preprocessing/2022-05-05-outlier-detection.html"},{"title":"Data Preprocessing","text":"Data Preprocessing Welcome to the fourth video of the tutorial for the AI Starter Kit on remaining useful lifetime prediction! In this video, we will explain you how the data needs to be preprocessed such that it can be served as input for a machine learning algorithm. Let's get back to the NASA Engine data and our use case. Before we can start with training a model to predict the remaining useful lifetime, a number of preparatory steps need to be taken. First of all, let us assign which type of problems we are dealing with. What precisely are we interested in? Is it about predicting a categorical value, which can take only a limited set of possible values, or a number? Indeed, it's not so easy to answer. It depends on how we define our use case: We can either ask: How long is the remaining useful lifetime? Then, a real number is looked for, and we deal with a so-called regression task. Alternatively, we can ask: Will the engine fail within the next 50 cycles? Then this boils down to a so-called binary classification task, in which the prediction is ‘yes' or ‘no'. The choice for one or the other task mainly depends on the business question you want to solve. In most cases, a binary classification problem is easier to handle for a Machine Learning algorithm and therefore, this is the question we will answer in this AI Starter Kit. However, the solution methodology is largely similar when you would opt for predicting the remaining useful lifetime using regression. In the previous video, we explained that for classification tasks, labelled data is essential. Therefore, as a first step, we need to create the binary labels: Has the machine failed within a given period of time or not? For the training data, we know that the final cycle per engine id is the time of failure. In order to determine the remaining number of cycles at each time point, first the maximum number of cycles per engine is determined. Subsequently, the current cycle number is subtracted from the maximal number of cycles to arrive at the number of cycles remaining at a particular time point. [Can we visualize this nicely on a screen on an example? à Show a timeline next to the dataframe, annotated with max/number of cycles/labels] We will add this value as a new variable to the data. However, this is not yet a binary label that can be used as input for the classification model we want to train. This is created by determining whether the calculated remaining useful lifetime is smaller than or equal to the threshold N – the period of time we want to predict the failure in. In this tutorial, we will use 30 cycles as a threshold, meaning that we aim to answer the question whether or not the engine will fail within the next 30 cycles at a particular point in time. This binary label can now be used as input value for the classification model. In order to decide on the goodness of the model, the test data needs the same type of label. In that case though, the final cycles per engine does not automatically determine the time of failure. How can we proceed instead? The ground truth data helps in this case. By joining the unlabelled test data and the ground truth data, we know the time of failure. Once done, we can analogously to the training data calculate the binary label for the test data. Don't forget, these labels will only be used for evaluating the model and are not shown to the algorithm beforehand. A second observation that can be made from the training data sample is that the scales of the values differ significantly across columns – both for sensor values and machine settings. This difference in the scale of the numbers could cause problems when the model needs to calculate the similarity between different cycles - namely the rows in the table - during modeling. To address this problem, we will normalize the data. The goal of normalization is to change the values of numeric columns in the dataset to use a common scale, without distorting differences in the ranges of values or losing information. The normalized values maintain the general distribution and ratios in the source data, while keeping values within a scale applied across all numeric columns used in the model. One of the most popular normalization techniques is so-called Min-Max normalization and that is also what we will use here. It scales the values of a variable to a range between 0 and 1 using this formula where 𝑋 represents the value to be normalized, 𝑋𝑚𝑖𝑛 is the minimum value of the variable in that column and 𝑋𝑚𝑎𝑥 is the maximum value for that variable We apply the normalization on both the training and test data set on all sensor measurement and setting variables. We will not rescale the engine id, as it should be seen as a categorical variable. Further, for the cycle variable we keep both the original and the scaled values. In the next video, we will go to the core of this tutorial, namely using a deep learning algorithm to train a model that is able to solve this binary classification problem.","tags":"Remaining Useful Life Prediction","url":"/Remaining Useful Life Prediction/2022-05-05-data-preprocessing.html","loc":"/Remaining Useful Life Prediction/2022-05-05-data-preprocessing.html"},{"title":"Data Understanding","text":"Data Understanding Welcome to the second video of the tutorial for the AI Starter Kit on remaining useful lifetime prediction! In this video, we will detail the dataset that will use and perform an initial data exploration to extract some first insights. In this AI Starter Kit, we will work with a publicly-available dataset from NASA. The data simulates run-to-failure data from aircraft engines. These engines are assumed to start with varying degrees of wear and manufacturing variation - but this information is unknown to the user. Furthermore, in this simulated data, the engines are assumed to be operating normally at the beginning and start to degrade at some point during operation. The degradation progresses and grows in magnitude with time. When a predefined threshold is reached, the engine is considered unsafe for further operation. In other words, the last operational cycle of the engine can be considered as the failure point of the corresponding engine – meaning that the remaining useful lifetime has decreased to zero. Before we can start with learning an actual machine learning model, it is crucial to understand the data itself. The data set consists of multiple time series with the \"cycle\" variable as time unit. For each engine, identified by the variable \"id\", a different number of cycles is captured as not all engines fail at the same time. Per cycle, the following information in gathered: On the one hand, 21 sensor readings given by the data points s1 to s21. On the other hand, additional information about the machine settings, given by setting1 to setting3. In machine learning experiments, a dataset is often split in a training set and a test set. This split allows one to quickly evaluate the performance of an algorithm. The training dataset is used to prepare a model, to train it. For evaluation, the test dataset can be understood as new data that is presented to the algorithm. It was not seen by the algorithm before and therefore the outcome is unknown. In our example, it is data from different engines for which it is unclear when they are going to fail, or put differently, what their remaining useful lifetime is. For the purpose of evaluation, the information about the actual failure of the test data set is collected in the so-called ground truth data. This information will not be visible to the algorithm but will only be used for calculating the quality of the model. Now let's have a look at the single sensor measurements. We see that the value range of the single measurements are quite different, without knowing in detail what they correspond to in a sense of physical measurement. In the graph, we see the first 50 entries of time series data collected from three different sensor channels for engine 18. All three show some fluctuations but no clear deviation from a mean value indicated by the gray dotted lines, that could be a sign of degradation in engine performance, are visible. With increasing observation time, all three time series deviate more or less strongly from the mean values observed in the first 50 data points given by the gray horizontal line, indicating the start of the degradation process of the engine. For different engines, the deviation starts at different times. For engine 18, the deviation starts approximately at time 100. For engine 31 though, hardly any deviation is visible in the same time range. Only when increasing the time range, the clear deviation becomes evident. The different starting points of degradation for the single engines indicate that the simulations are made for engines with different wear. In the next video, we will go into more detail into the data preprocessing phase, explaining how the data needs to be prepared such that it can be served as input for a machine learning learning algorithm. If you are not familiar with deep learning, we recommend you to first watch our introductory video on this topic, in which we discuss the difference between ‘traditional' Machine Learning algorithms and Deep Learning techniques. We will also provide a brief introduction to the different type of neural networks, amongst others the so-called Long Short-Term Memory networks or LSTMs for short, which is the type of network that we will use to solve this problem.","tags":"Remaining Useful Life Prediction","url":"/Remaining Useful Life Prediction/2022-05-03-data-understanding.html","loc":"/Remaining Useful Life Prediction/2022-05-03-data-understanding.html"},{"title":"Data Understanding","text":"Data Understanding Welcome to the second video of the tutorial for the AI Starter Kit on time-series pre-processing! In this video, we will detail the dataset that we will use and perform an initial data exploration. The time series dataset that we study in this Starter Kit is generated by the SCADA system of a set of wind turbines. SCADA stands for supervisory control and data acquisition. In modern turbines, such a data acquisition system can easily contain more than 100 sensors that keep track of amongst others temperature, pressure data, electrical quantities like currents and voltages, and vibrations. It is commonly used for performance monitoring and condition-based maintenance. The dataset originates from 4 wind turbines located in the Northeast of France. It spans a period of 4 years with a sampling rate of 10 minutes. Although the original SCADA system records statistics for more than 30 sensors, we will focus our attention only on wind speed, wind direction, temperature as well as the power that is generated by each of these turbines to illustrate a variety of time series pre-processing techniques. The table on the right-hand side shows an excerpt of the data, with the following attributes: The column Date_time with the timestamp of the measurement, in 10-minute increments the identifier of the turbine Power, which is the active power measurement in kW as effectively produced by the turbine. the outside temperature measurement in degrees Celsius the wind speed measurement in meters per second And finally the wind direction measurement in degrees The top rows show the average values for each turbine for the defined range. Note that in the case of wind direction, this is the circular average, which takes into account the circular nature of the data. That means that values are bound between 0 and 360 degrees, in which 0 degrees is identical to 360 degrees. In our interactive Starter Kit, you can define a range of values for a given attribute and see how the values in the remaining attributes change. For example, we can increase the wind speed to more than 17m/sec and see how that affects power production. Can you already spot any unexpected values in the dataset? Indeed, in the marked area in the figure, the values are significantly below those seen at other times. You can experiment yourself how the other variables influence the active power in the interactive Starter Kit. Now that we know which variables were measured, let's check some statistics. From the table we learn that in total 4 years of data are available, namely from January 2013 to the end of 2016. Further, we see that the number of data points per wind turbine differs. Most data points are available for the first turbine in the column, while the second turbine collected roughly 1000 data points less. This indicates already that for this latter turbine some data points are missing. In the interactive Starter Kit, we can plot the values of a given variable in time to see how their long-term trend looks like. In the graph shown at the right, we adjusted the timeframe by selecting it directly in the graph, such that also shorter-term fluctuations become visible. By clicking on the turbine names at the right-hand side of the graph, we can select a subset of the turbines. In the drop-down menu at the top of the graph, the variable to plot can be selected. What can we learn from this visual exploration? Let's first have a look at the temperature. Here we can easily spot the seasonal pattern due to summer and winter. Is this behaviour similar for the active power? Is there something unusual you can observe in the temperature values? We will come back to the answers to these questions later on in this Starter Kit and introduce you to a number of techniques to automatically detect this. In the next video, we will first concentrate on the smoothening and so-called resampling of the data points. See you there.","tags":"Time Series Preprocessing","url":"/Time Series Preprocessing/2022-05-03-data-understanding.html","loc":"/Time Series Preprocessing/2022-05-03-data-understanding.html"},{"title":"Data Understanding and Preprocessing","text":"Data Understanding and Preprocessing Welcome to the second tutorial video of the AI starterkit on data exploration. In this video, we will introduce the data that will be used throughout the tutorial and explain how to preprocess your data before you can start with the statistical exploration. Throughout the tutorial, we will explore the datasets shared by Pronto, Seattle's bike sharing system. On the one hand, Pronto provides information about the single stations where you can take a bike. On the other hand, information about the single trips themselves are shared in an anonymized way. While we will have a brief look at the location data, the focus in this tutorial is on the trip data. Now, let's have a look at the data sets. We can see, that - besides the pure geolocation data given by latitude and longitude – quite some information is shared about the single stations. Each station can be uniquely identified either by its id, its name, defined by the street intersection where the station is located and more technically, by its terminal name. We also have details on the size of the station, given by the number of docks available and when the station was put in service, given by the variable \"online\". Finally, we also know about the station's elevation from sea level. With the geo-coordinates given, we can put the single stations on a map - probably the easiest way for humans to understand where the stations are located. From the map, we deduct that - location-wise - there are two groups of stations in Seattle. One is located in the University district, while the other is located downtown and in its surroundings for example around Capitol Hill. Now that we know where the single stations are based, let's have a look at the trips data in order to understand how the single trips can be linked to actual movements throughout Seattle. Even though the trip data is anonymized, still quite some information is given. For each trip, given by a unique \"trip id\", we know when and from where the trip started and where and when the bike was returned. The resulting trip duration is given in seconds. Further, every bike in use has a unique \"bike id\" such that we can also follow a single bike. Additionally, we learn something about the single bike users. We know whether it either was an Annual Member having a ride or a Short-Term Pass Holder with a ticket for 1 or 3 days. On top of that, the gender and year of birth of the bike user is shared. In order to make it possible to also place the single trips on a map later-on, we merge the geographical information namely latitude, longitude, and elevation from the stations with the information of the single trips. Note that we need to execute two merge commands: A first one for the departure station and a second one for the arrival station. An excerpt of the merged data that we will use in the rest of this Starter Kit is shown in the table on the right. For reasons of readability, we show the data here in a transposed manner such that each column provides the information for one single trip. Before proceeding with the further exploration, we first check the completeness of the data, hence where we observe missing values. We can see that at most more than 140,000 entries are given. Most variables seem to be complete, except gender and birthyear, which only contain about 87,000 entries. The reason for this is that Short-Term Pass Holders do not need to specify their age and gender, due to which this information is just missing in the data. This can be confirmed by calculating the percentage of data present for these two variables for the single user types. While for all Annual Members, gender and birthyear are given, it is not known for any of the Short-Term Pass Holders. For our later analysis we want to replace the missing values with meaningful entries. In order to see how to replace them, we first have a look at the existing values. For the Annual Pass Members, three different gender options are available: Female, Male, and Other. In order to be able to distinguish the missing values from the remaining ones later-on, we replace them by \"Unknown\". Interesting is the fact that almost 80% of all users are male. We will get back to this in the following video. In order to further prepare the data for subsequent analysis, we perform the following actions: First, we convert the variable tripduration which is currently given as a float variable, to a datetime.timedelta object and store it as new variable tripdurationTimeDelta. Further, we add a new variable tripdurationMinutes indicating the duration of a trip in minutes which we keep as a float value. For further analysis, it can be convenient to work with age instead of year of birth. Therefore, we convert the variable birthyear to an integer and create a new variable age. Finally, we extract from the starttime variable the month, the day of the week and the hour of the day, and use the starttime variable to index the data. The table presents an excerpt of the dataset after these transformations. Note that also here, each column indicates one trip. With this well-prepared dataset, we can start the actual data exploration in the next video.","tags":"Data Exploration","url":"/Data Exploration/2022-09-03-data-understanding-and-preprocessing.html","loc":"/Data Exploration/2022-09-03-data-understanding-and-preprocessing.html"},{"title":"Introduction","text":"Introduction Welcome to the tutorial for the AI Starter Kit on advanced visualisation. We will introduce you to a number of more advanced visualisation techniques. You will learn how these can be used creatively to uncover more elaborate insights from your data than would be possible with the more general out-of-the-box visualisations and plots. Data visualisation is an important activity in several phases of a data science project. It allows you to understand several characteristics of a dataset, to discover interesting insights, to validate analysis results, to communicate these results to non-experts via intuitive dashboards, and so on. Popular visualisations often used by data scientists are boxplots, distribution plots, as well as bar and line charts. These are basic out-of-the-box visualisations that are generally applicable, but that are often hard to interpret for non-experts. And more importantly, they do not always reveal interesting insights. More advanced visualisations exploit the human eye's extraordinary visual pattern recognition abilities. A clever visualisation of data can already reveal interesting patterns and insights, even before any complex algorithm is applied. On top of that, they can help in formulating hypotheses to be validated further in the data analytics process or to identify features that can be useful for data-driven modelling. Before we dive deeper in these more advanced visualisation techniques, let us discuss some examples. many time series show some underlying periodicity or seasonality. Depending on the length and granularity of these time series, these seasonal effects are often only hardly visible. In this Starter Kit, we will introduce to you to a number of visualisations that make it very easy to identify for example monthly patterns. Another field where advanced visualisations can help is anomaly detection. Such anomalies appear for example when a machine is failing, and are especially hard to detect when it comes to multi-dimensional data originating from multiple sensors. In this Starter Kit, we will use advanced visualisations such as timeline plots, heatmaps, calendar maps, area plots and scatter plots to visually explore a dataset and gradually build up more knowledge about it. We use a publicly-available dataset that consists of bike counter data, which contains hourly information on the number of bikes that cross at six different spots in Seattle. By visualising this data, we will be able to: explain certain characteristics of the data, such as some nodes having more crossings than others, identify global trends, such as an increase in traffic over the years, and seasonal trends, such as fluctuating popularity within a year, recognize structural patterns, such as distinct weekday and weekend traffic patterns, and finally detect outliers, such as weekend traffic patterns that occur on weekdays. In the next video, we will explain the data in more detail before we continue in the following videos with a number of more advanced visualisation techniques to extract insights from this data.","tags":"Advanced Data Visualization","url":"/Advanced Data Visualization/2022-05-02-introduction.html","loc":"/Advanced Data Visualization/2022-05-02-introduction.html"},{"title":"Introduction","text":"Introduction Welcome to the tutorial for the AI Starter Kit on remaining useful lifetime prediction! In this first video we will provide an overview of the actual business case we're tackling. We will introduce the most important concepts and explain why it is beneficial for maintenance to know the remaining useful lifetime of a machine, a tool or any other industrial asset. Estimating the remaining useful lifetime of an asset can be beneficial for several purposes in a variety of industrial contexts. For one, it can offer support in the better scheduling of maintenance operations, for example for offshore wind turbines. For such assets, maintenance can only be performed under the right weather conditions. But also for other assets, maintenance is an important part of its lifecycle. Traditionally, maintenance is done in a corrective way, such that it is performed at the moment an asset has failed. In that case, the failing part is identified and rectified or replaced, to ensure that the asset can subsequently resume normal operation. Especially for strongly interdependent production lines or industrial assets operating in critical environments, an unplanned downtime is to be avoided and often costly. For this reason, more recently, for an increasing number of assets preventive maintenance is performed. In that case, maintenance tasks are scheduled at regular intervals, avoiding future asset failure to a maximum extent – but with the risk to replace assets that are still working correctly and still could for a while. Hence, the optimal time for replacement is wanted: The time when the asset still works correctly but will probably fail soon. And this is where the remaining useful lifetime comes into play. Nowadays, ever more assets are equipped with different types of sensors gathering data. This started the trend towards predictive maintenance, in which the maintenance moments are decided upon in a data-driven way. Therewith, maintenance is only performed when actually needed. This is a very broad domain which encompasses a variety of topics. Besides the estimation of the asset's remaining useful lifetime – the main topic of this AI starter kit – further related problems to solve for a fully predictive maintenance approach are failure prediction, detection, and diagnosis for root cause analysis. The remaining useful lifetime of industrial assets is defined as an estimate of the remaining time that an item, component, or system is estimated to function as expected. It is expressed as the number of hours, cycles, batches or any other quantity. In the figure we see datapoints collected from an arbitrary sensor in green. Let's say it measures the tool radius of a milling machine. With increasing time, the radius decreases as the tool wears out. At a certain point, the tool is considered too worn out to still further use it in production, which can influence the quality of the produced parts or lead to an unstable production process and consequent damage to the machine. The main questions thus is for how long the tool will it still be able to function properly? By applying Machine Learning algorithms, the continuation of this time series data can be forecasted, denoted in pink. From this forecast, the remaining useful lifetime can be estimated for a given asset. In this case, the machine should be maintained within the next 10 to 15 cycles. With this information, maintenance tasks can be scheduled accordingly – with a tool still functioning and without unexpected downtime. Hence, by estimating the remaining useful lifetime, we minimize the risk of failure and maintenance cost. An accurate prediction of an asset's lifetime can also help to optimise the operational efficiency of an asset, by more optimally planning the use of that asset before its end of life, or adapting its operational use to extend its useful lifetime. In manufacturing settings, where a downtime of the production line results in several operational problems and associated costs, remaining useful lifetime prediction can avoid unplanned downtime. In this AI Starter Kit, we will show its use to avoid safety-critical situations, by illustrating the solution methodology for predicting the remaining useful lifetime of aircrafts engines. To summarize, by applying predictive maintenance it is possible to: - better schedule maintenance operations - optimize operational efficiency - avoid unplanned downtime - anticipate and avoid safety-critical situations. This analysis can be performed for various machines, tools or processes as long as this degradation over time can be measured accordingly, just like resistance, length, temperature or similar measures. Predicting the remaining useful lifetime is typically very challenging for several reasons: First of all, usually a multitude of heterogenous sensor data is measured at several places in the machine. This data is often captured at a high-frequency, consisting of vibration data, acoustic emission, accelerometer data, and many more machine-internal parameters. Secondly, in some settings, also characteristics of the surrounding environment influencing the lifetime of the asset are captured. Further, typically, data that is gathered over a long period of time needs to be available to train the model. Given that industrial assets are often very complex – as they consist of both electrical and mechanical components - it generally is a non-trivial exercise to predict their end of life. On top of that, the data typically comes with a lot of variation, due to varying machine types operating in heterogenous operating conditions with different machine configurations. Therefore, the main challenge in this process is to extract meaningful characteristics that can be used to predict the end of life from the gathered data. This is complicated by the fact that usually very little domain expertise on the operating conditions of these assets is available. Additionally, the environment in which they operate is typically very dynamic. In this AI Starter Kit, we will guide you through a data-driven methodology based on deep learning to tackle this challenge. In the next video, we will concentrate on the data itself that we selected to illustrate the approach. We will explain which information is available and what we can learn from it.","tags":"Remaining Useful Life Prediction","url":"/Remaining Useful Life Prediction/2022-05-02-introduction.html","loc":"/Remaining Useful Life Prediction/2022-05-02-introduction.html"},{"title":"Introduction","text":"Introduction Welcome to the video tutorial for the AI Starter Kit on data exploration. In this first video, we will discuss why data exploration is the first step in answering any data science question. Before starting to use complex algorithms to solve a data science problem, you need to gain a thorough understanding of the corresponding data and the underlying business challenge you are trying to address. Data exploration is the process through which you gain such an understanding. This will eventually enable you to derive viable working hypotheses related to the problem at hand, which is useful for the further analysis and modelling of the data. Throughout this process you will be faced with different types of variables, for example numerical or categorical, which require different types of treatment. In addition, looking at individual variables in isolation is not sufficient, as this does not consider the, often complex, interactions between different variables. Furthermore, you will need to identify which are the relevant variables in your dataset and, on the other hand, which ones worsen its outcome rather than improving the quality of your analysis. Finally, data exploration will help you to point out which data quality issues your dataset may have and which actions you should undertake to mitigate them. Amongst others, data exploration is useful to identify usual and unusual values for a variable but also, to study the evolution of a variable over time. It can be helpful to uncover significant relationships between different variables or to assess data quality and suitability. The goal behind this Starter Kit is to lay out a series of analyses that will teach you how to explore individual variables, look at how pairs of variables are related and study the complex interactions between groups of variables. You will learn how to conduct a quantitative and visual inspection of your dataset and you will be prepared for the next step in your advanced analytics workflow. In the tutorial of this AI Starter Kit, we follow a systematic approach to explore a dataset, based on the number of variables under consideration. We start by exploring individual variables in isolation, which is the simplest form of analysis and is called univariate analysis. We continue by studying pairs of variables, which is called bivariate analysis. And finally, we study the relationship between several variables, called multivariate analysis before we conclude on our findings. But before diving into this, in the next video, we will first introduce you to the dataset that we will use in this analysis and perform some basic preprocessing on it.","tags":"Data Exploration","url":"/Data Exploration/2022-09-02-introduction.html","loc":"/Data Exploration/2022-09-02-introduction.html"},{"title":"Introduction","text":"Introduction Description Most data mining and machine learning algorithms do not work well if you just feed them your raw data: such data often contains noise and the most relevant distinguishing information is often implicit. For example, raw sensor data just contains a timestamp and a corresponding value, while interesting aspects of it might be the trends contained within or the number of times a threshold is exceeded. Feature engineering is the process of extracting from the raw data the most relevant distinguishing characteristics that will be presented to the algorithm during modeling. It is a way of deriving new information from the existing data, so that its characteristics are more explicitly represented. The resulting features are eventually feed to the AI/ML algorithm of the model. In practice, the feature engineering step is achieved by the selection of the most appropriate parameters, or the composition of new features by manipulation, transformation and combination of the raw data. Feature engineering is one of the most important and creative steps in the data science workflow. However, there is no clearly-defined formal process for engineering features and, consequently, this requires a lot of creativity, a good understanding of the domain and of the available data, some trial-and-error, etc. It is also desirable to have upfront an idea of the modeling and analysis task for which you want to use the resulting features, as this might help you to identify relevant features. Business goal The overall goal of this Starter Kit is to present some advanced feature engineering steps related to feature construction and extraction . By keeping in mind what is your business question and what is the corresponding data science task , you will be able to derive valuable features that can be used in the next stage of your analysis. Application context Feature engineering is one of the steps in the data science workflow with the most decisive impact on the accuracy of the model you want to develop. It is the final step before actually training the model and it defines how the input data will be fed to the model. Starter Kit outline In this Starter Kit we use a dataset from the Regional Rail System of Pennsylvania. Before starting the feature extraction per se , we will first apply some basic preprocessing to the dataset and have a high-level overview of the data. We will then derive several interesting features from the dataset that can be used to characterise train rides and delays.","tags":"Feature Engineering","url":"/Feature Engineering/2023-04-01-introduction.html","loc":"/Feature Engineering/2023-04-01-introduction.html"},{"title":"Introduction","text":"Introduction Welcome to the tutorial for the AI Starter Kit on time-series pre-processing. In this first video we will provide an overview of the actual challenge we're tackling. We will introduce the most important problems occurring in time series data and explain why it is beneficial to first improve your data before you start any other data-modelling tasks. Time series are characterised as a collection of data points obtained at successive times, most often with equal intervals between them. Since an increasing amount of assets is instrumented with sensors, this type of data is omnipresent, for example when monitoring industrial machinery or resulting from wearables tracking one's health state. Depending on the application domain, time series data is exploited for various purposes. It is used, amongst others, for profiling energy consumption of households predicting imminent failures of a production line, estimating the remaining useful lifetime of a machine, and many more industrial use cases. Typically, time series data cannot be used easily as-is by machine learning algorithms. This can be for example due to data quality issues such as missing values and sensor misreadings. Or because some algorithms are not fit to deal with the continuous nature of this type of data or the associated - often high – frequency with which it is gathered. To illustrate the various methods and techniques in this Starter Kit, we will use a publicly available real-world dataset that consists of sensor data measurements from wind turbines. It exhibits typical characteristics of industrial time series datasets as it is very noisy and contains outliers and missing values. Moreover, it exhibits seasonal patterns across multiple years, as the machine behaviour is influenced by the meteorological conditions. We will use this dataset to illustrate how resampling and smoothing can be applied to gain a better understanding of the behaviour of the time series, how the quality of the data can be improved through normalization and outlier detection, and * how missing data can be imputed in various ways. In the next video, we will perform some initial exploration on the dataset in order to gain some basic understanding of the data.","tags":"Time Series Preprocessing","url":"/Time Series Preprocessing/2022-05-02-introduction.html","loc":"/Time Series Preprocessing/2022-05-02-introduction.html"},{"title":"Introduction","text":"Introduction Welcome to the tutorial for the AI Starter Kit on resource demand forecasting. In this first video we will provide an overview of the actual business case we are tackling. We will introduce the most important concepts and explain why it is beneficial for resource planning to know the expected demand in the future. Resource demand forecasting concerns accurately predicting the future need for a resource, typically using historical information. It is one of the most essential steps in resource demand management, which tries to ensure that sufficient resources are available to satisfy a fluctuating demand. Resource demand forecasting allows one to plan ahead and guarantee that sufficient resources are available when needed and to avoid costly countermeasures in case of shortage. Precisely forecasting the demand of a given resource can be beneficial for several purposes in a wide range of industrial contexts. When knowing the future demand, only the number of resources actually required need to be allocated to a specific task or process. The field ranges from forecasting the number of products that need to be manufactured based on historical sales figures, over predicting the parking demand or traffic density in a particular neighbourhood in order to control traffic in more intelligent ways, to estimating the amount of energy that will be consumed and produced for a particular city or region. Especially in the later field, the demand is influenced by a mixture of external factors such as the weather conditions. For energy suppliers it is more beneficial to buy electricity on the day-ahead market than on the spot market. Consequently, the more accurate the energy consumption can be predicted, the lower the cost. Given that more and more houses and appliances are enabled with a smart meter, an increasing amount of data becomes available that can enable and improve this prediction. Furthermore, these kinds of predictions also allow energy suppliers to better balance demand and supply and helps them to ensure proper grid operation, but also to avoid negative prices, for example in times of high solar irradiation. In this example we see the active power of a single household near Paris shown in green. In orange, the outside temperature is given. The two are strongly correlated – the demand increases with decreasing temperatures since most French households use electrical heating. Therefore, particularly in winter times, the grid has to be stable when facing high demands. In this Starter Kit, we will demonstrate how to effectively forecast the demand for this household for the future by taking external factors into account. In the following videos, we will first start by gaining some first insights in the data in front of us and perform some initial pre-processing steps to prepare the data for the analysis. Subsequently, we will illustrate you how to gain deeper insights by presenting a number of visual and statistical techniques in order to explore the data for identifying interesting patterns. In a next step, useful characteristics or features will be extracted from the dataset that can as serve as input for the forecasting models. Finally, in the last video, we will explain you how to correctly compare the performance of multiple forecasting models. Let us go ahead with the next video on data preparation, in which we will describe how to deal with the typical problems of raw data and how to perform data fusion when multiple sources of data are available. Additional information The video material in this website was developed in the context of the SKAIDive project , financially supported by the European Social Fund , the European Union and Flanders. For more information, please contact us at elucidatalab@sirris.be","tags":"Resource Demand Forecasting","url":"/Resource Demand Forecasting/2022-05-02-introduction.html","loc":"/Resource Demand Forecasting/2022-05-02-introduction.html"},{"title":"Introduction","text":"Introduction Welcome to the tutorial for the AI Starter Kit on remaining useful lifetime prediction! In this first video we will provide an overview of the actual business case we're tackling. We will introduce the most important concepts and explain why it is beneficial for maintenance to know the remaining useful lifetime of a machine, a tool or any other industrial asset. Estimating the remaining useful lifetime of an asset can be beneficial for several purposes in a variety of industrial contexts. For one, it can offer support in the better scheduling of maintenance operations, for example for offshore wind turbines. For such assets, maintenance can only be performed under the right weather conditions. But also for other assets, maintenance is an important part of its lifecycle. Traditionally, maintenance is done in a corrective way, such that it is performed at the moment an asset has failed. In that case, the failing part is identified and rectified or replaced, to ensure that the asset can subsequently resume normal operation. Especially for strongly interdependent production lines or industrial assets operating in critical environments, an unplanned downtime is to be avoided and often costly. For this reason, more recently, for an increasing number of assets preventive maintenance is performed. In that case, maintenance tasks are scheduled at regular intervals, avoiding future asset failure to a maximum extent – but with the risk to replace assets that are still working correctly and still could for a while. Hence, the optimal time for replacement is wanted: The time when the asset still works correctly but will probably fail soon. And this is where the remaining useful lifetime comes into play. Nowadays, ever more assets are equipped with different types of sensors gathering data. This started the trend towards predictive maintenance, in which the maintenance moments are decided upon in a data-driven way. Therewith, maintenance is only performed when actually needed. This is a very broad domain which encompasses a variety of topics. Besides the estimation of the asset's remaining useful lifetime – the main topic of this AI starter kit – further related problems to solve for a fully predictive maintenance approach are failure prediction, detection, and diagnosis for root cause analysis. The remaining useful lifetime of industrial assets is defined as an estimate of the remaining time that an item, component, or system is estimated to function as expected. It is expressed as the number of hours, cycles, batches or any other quantity. In the figure we see datapoints collected from an arbitrary sensor in green. Let's say it measures the tool radius of a milling machine. With increasing time, the radius decreases as the tool wears out. At a certain point, the tool is considered too worn out to still further use it in production, which can influence the quality of the produced parts or lead to an unstable production process and consequent damage to the machine. The main questions thus is for how long the tool will it still be able to function properly? By applying Machine Learning algorithms, the continuation of this time series data can be forecasted, denoted in pink. From this forecast, the remaining useful lifetime can be estimated for a given asset. In this case, the machine should be maintained within the next 10 to 15 cycles. With this information, maintenance tasks can be scheduled accordingly – with a tool still functioning and without unexpected downtime. Hence, by estimating the remaining useful lifetime, we minimize the risk of failure and maintenance cost. An accurate prediction of an asset's lifetime can also help to optimise the operational efficiency of an asset, by more optimally planning the use of that asset before its end of life, or adapting its operational use to extend its useful lifetime. In manufacturing settings, where a downtime of the production line results in several operational problems and associated costs, remaining useful lifetime prediction can avoid unplanned downtime. In this AI Starter Kit, we will show its use to avoid safety-critical situations, by illustrating the solution methodology for predicting the remaining useful lifetime of aircrafts engines. To summarize, by applying predictive maintenance it is possible to: - better schedule maintenance operations - optimize operational efficiency - avoid unplanned downtime - anticipate and avoid safety-critical situations. This analysis can be performed for various machines, tools or processes as long as this degradation over time can be measured accordingly, just like resistance, length, temperature or similar measures. Predicting the remaining useful lifetime is typically very challenging for several reasons: First of all, usually a multitude of heterogenous sensor data is measured at several places in the machine. This data is often captured at a high-frequency, consisting of vibration data, acoustic emission, accelerometer data, and many more machine-internal parameters. Secondly, in some settings, also characteristics of the surrounding environment influencing the lifetime of the asset are captured. Further, typically, data that is gathered over a long period of time needs to be available to train the model. Given that industrial assets are often very complex – as they consist of both electrical and mechanical components - it generally is a non-trivial exercise to predict their end of life. On top of that, the data typically comes with a lot of variation, due to varying machine types operating in heterogenous operating conditions with different machine configurations. Therefore, the main challenge in this process is to extract meaningful characteristics that can be used to predict the end of life from the gathered data. This is complicated by the fact that usually very little domain expertise on the operating conditions of these assets is available. Additionally, the environment in which they operate is typically very dynamic. In this AI Starter Kit, we will guide you through a data-driven methodology based on deep learning to tackle this challenge. In the next video, we will concentrate on the data itself that we selected to illustrate the approach. We will explain which information is available and what we can learn from it.","tags":"Remaining Useful Life Prediction","url":"/Remaining Useful Life Prediction/2022-05-02-introduction.html","loc":"/Remaining Useful Life Prediction/2022-05-02-introduction.html"},{"title":"Introduction","text":"Introduction Welcome to the video tutorial for the AI Starter Kit on data exploration. In this first video, we will discuss why data exploration is the first step in answering any data science question. Before starting to use complex algorithms to solve a data science problem, you need to gain a thorough understanding of the corresponding data and the underlying business challenge you are trying to address. Data exploration is the process through which you gain such an understanding. This will eventually enable you to derive viable working hypotheses related to the problem at hand, which is useful for the further analysis and modelling of the data. Throughout this process you will be faced with different types of variables, for example numerical or categorical, which require different types of treatment. In addition, looking at individual variables in isolation is not sufficient, as this does not consider the, often complex, interactions between different variables. Furthermore, you will need to identify which are the relevant variables in your dataset and, on the other hand, which ones worsen its outcome rather than improving the quality of your analysis. Finally, data exploration will help you to point out which data quality issues your dataset may have and which actions you should undertake to mitigate them. Amongst others, data exploration is useful to identify usual and unusual values for a variable but also, to study the evolution of a variable over time. It can be helpful to uncover significant relationships between different variables or to assess data quality and suitability. The goal behind this Starter Kit is to lay out a series of analyses that will teach you how to explore individual variables, look at how pairs of variables are related and study the complex interactions between groups of variables. You will learn how to conduct a quantitative and visual inspection of your dataset and you will be prepared for the next step in your advanced analytics workflow. In the tutorial of this AI Starter Kit, we follow a systematic approach to explore a dataset, based on the number of variables under consideration. We start by exploring individual variables in isolation, which is the simplest form of analysis and is called univariate analysis. We continue by studying pairs of variables, which is called bivariate analysis. And finally, we study the relationship between several variables, called multivariate analysis before we conclude on our findings. But before diving into this, in the next video, we will first introduce you to the dataset that we will use in this analysis and perform some basic preprocessing on it.","tags":"Data Exploration","url":"/Data Exploration/2022-09-02-introduction.html","loc":"/Data Exploration/2022-09-02-introduction.html"},{"title":"Introduction","text":"Introduction Description Most data mining and machine learning algorithms do not work well if you just feed them your raw data: such data often contains noise and the most relevant distinguishing information is often implicit. For example, raw sensor data just contains a timestamp and a corresponding value, while interesting aspects of it might be the trends contained within or the number of times a threshold is exceeded. Feature engineering is the process of extracting from the raw data the most relevant distinguishing characteristics that will be presented to the algorithm during modeling. It is a way of deriving new information from the existing data, so that its characteristics are more explicitly represented. The resulting features are eventually feed to the AI/ML algorithm of the model. In practice, the feature engineering step is achieved by the selection of the most appropriate parameters, or the composition of new features by manipulation, transformation and combination of the raw data. Feature engineering is one of the most important and creative steps in the data science workflow. However, there is no clearly-defined formal process for engineering features and, consequently, this requires a lot of creativity, a good understanding of the domain and of the available data, some trial-and-error, etc. It is also desirable to have upfront an idea of the modeling and analysis task for which you want to use the resulting features, as this might help you to identify relevant features. Business goal The overall goal of this Starter Kit is to present some advanced feature engineering steps related to feature construction and extraction . By keeping in mind what is your business question and what is the corresponding data science task , you will be able to derive valuable features that can be used in the next stage of your analysis. Application context Feature engineering is one of the steps in the data science workflow with the most decisive impact on the accuracy of the model you want to develop. It is the final step before actually training the model and it defines how the input data will be fed to the model. Starter Kit outline In this Starter Kit we use a dataset from the Regional Rail System of Pennsylvania. Before starting the feature extraction per se , we will first apply some basic preprocessing to the dataset and have a high-level overview of the data. We will then derive several interesting features from the dataset that can be used to characterise train rides and delays.","tags":"Feature Engineering","url":"/Feature Engineering/2023-04-01-introduction.html","loc":"/Feature Engineering/2023-04-01-introduction.html"},{"title":"Introduction","text":"Introduction Welcome to the tutorial for the AI Starter Kit on time-series pre-processing. In this first video we will provide an overview of the actual challenge we're tackling. We will introduce the most important problems occurring in time series data and explain why it is beneficial to first improve your data before you start any other data-modelling tasks. Time series are characterised as a collection of data points obtained at successive times, most often with equal intervals between them. Since an increasing amount of assets is instrumented with sensors, this type of data is omnipresent, for example when monitoring industrial machinery or resulting from wearables tracking one's health state. Depending on the application domain, time series data is exploited for various purposes. It is used, amongst others, for profiling energy consumption of households predicting imminent failures of a production line, estimating the remaining useful lifetime of a machine, and many more industrial use cases. Typically, time series data cannot be used easily as-is by machine learning algorithms. This can be for example due to data quality issues such as missing values and sensor misreadings. Or because some algorithms are not fit to deal with the continuous nature of this type of data or the associated - often high – frequency with which it is gathered. To illustrate the various methods and techniques in this Starter Kit, we will use a publicly available real-world dataset that consists of sensor data measurements from wind turbines. It exhibits typical characteristics of industrial time series datasets as it is very noisy and contains outliers and missing values. Moreover, it exhibits seasonal patterns across multiple years, as the machine behaviour is influenced by the meteorological conditions. We will use this dataset to illustrate how resampling and smoothing can be applied to gain a better understanding of the behaviour of the time series, how the quality of the data can be improved through normalization and outlier detection, and * how missing data can be imputed in various ways. In the next video, we will perform some initial exploration on the dataset in order to gain some basic understanding of the data.","tags":"Time Series Preprocessing","url":"/Time Series Preprocessing/2022-05-02-introduction.html","loc":"/Time Series Preprocessing/2022-05-02-introduction.html"},{"title":"Introduction","text":"Introduction Welcome to the tutorial for the AI Starter Kit on resource demand forecasting. In this first video we will provide an overview of the actual business case we are tackling. We will introduce the most important concepts and explain why it is beneficial for resource planning to know the expected demand in the future. Resource demand forecasting concerns accurately predicting the future need for a resource, typically using historical information. It is one of the most essential steps in resource demand management, which tries to ensure that sufficient resources are available to satisfy a fluctuating demand. Resource demand forecasting allows one to plan ahead and guarantee that sufficient resources are available when needed and to avoid costly countermeasures in case of shortage. Precisely forecasting the demand of a given resource can be beneficial for several purposes in a wide range of industrial contexts. When knowing the future demand, only the number of resources actually required need to be allocated to a specific task or process. The field ranges from forecasting the number of products that need to be manufactured based on historical sales figures, over predicting the parking demand or traffic density in a particular neighbourhood in order to control traffic in more intelligent ways, to estimating the amount of energy that will be consumed and produced for a particular city or region. Especially in the later field, the demand is influenced by a mixture of external factors such as the weather conditions. For energy suppliers it is more beneficial to buy electricity on the day-ahead market than on the spot market. Consequently, the more accurate the energy consumption can be predicted, the lower the cost. Given that more and more houses and appliances are enabled with a smart meter, an increasing amount of data becomes available that can enable and improve this prediction. Furthermore, these kinds of predictions also allow energy suppliers to better balance demand and supply and helps them to ensure proper grid operation, but also to avoid negative prices, for example in times of high solar irradiation. In this example we see the active power of a single household near Paris shown in green. In orange, the outside temperature is given. The two are strongly correlated – the demand increases with decreasing temperatures since most French households use electrical heating. Therefore, particularly in winter times, the grid has to be stable when facing high demands. In this Starter Kit, we will demonstrate how to effectively forecast the demand for this household for the future by taking external factors into account. In the following videos, we will first start by gaining some first insights in the data in front of us and perform some initial pre-processing steps to prepare the data for the analysis. Subsequently, we will illustrate you how to gain deeper insights by presenting a number of visual and statistical techniques in order to explore the data for identifying interesting patterns. In a next step, useful characteristics or features will be extracted from the dataset that can as serve as input for the forecasting models. Finally, in the last video, we will explain you how to correctly compare the performance of multiple forecasting models. Let us go ahead with the next video on data preparation, in which we will describe how to deal with the typical problems of raw data and how to perform data fusion when multiple sources of data are available. Additional information The video material in this website was developed in the context of the SKAIDive project , financially supported by the European Social Fund , the European Union and Flanders. For more information, please contact us at elucidatalab@sirris.be","tags":"Resource Demand Forecasting","url":"/Resource Demand Forecasting/2022-05-02-introduction.html","loc":"/Resource Demand Forecasting/2022-05-02-introduction.html"},{"title":"Introduction","text":"Introduction Welcome to the tutorial for the AI Starter Kit on advanced visualisation. We will introduce you to a number of more advanced visualisation techniques. You will learn how these can be used creatively to uncover more elaborate insights from your data than would be possible with the more general out-of-the-box visualisations and plots. Data visualisation is an important activity in several phases of a data science project. It allows you to understand several characteristics of a dataset, to discover interesting insights, to validate analysis results, to communicate these results to non-experts via intuitive dashboards, and so on. Popular visualisations often used by data scientists are boxplots, distribution plots, as well as bar and line charts. These are basic out-of-the-box visualisations that are generally applicable, but that are often hard to interpret for non-experts. And more importantly, they do not always reveal interesting insights. More advanced visualisations exploit the human eye's extraordinary visual pattern recognition abilities. A clever visualisation of data can already reveal interesting patterns and insights, even before any complex algorithm is applied. On top of that, they can help in formulating hypotheses to be validated further in the data analytics process or to identify features that can be useful for data-driven modelling. Before we dive deeper in these more advanced visualisation techniques, let us discuss some examples. many time series show some underlying periodicity or seasonality. Depending on the length and granularity of these time series, these seasonal effects are often only hardly visible. In this Starter Kit, we will introduce to you to a number of visualisations that make it very easy to identify for example monthly patterns. Another field where advanced visualisations can help is anomaly detection. Such anomalies appear for example when a machine is failing, and are especially hard to detect when it comes to multi-dimensional data originating from multiple sensors. In this Starter Kit, we will use advanced visualisations such as timeline plots, heatmaps, calendar maps, area plots and scatter plots to visually explore a dataset and gradually build up more knowledge about it. We use a publicly-available dataset that consists of bike counter data, which contains hourly information on the number of bikes that cross at six different spots in Seattle. By visualising this data, we will be able to: explain certain characteristics of the data, such as some nodes having more crossings than others, identify global trends, such as an increase in traffic over the years, and seasonal trends, such as fluctuating popularity within a year, recognize structural patterns, such as distinct weekday and weekend traffic patterns, and finally detect outliers, such as weekend traffic patterns that occur on weekdays. In the next video, we will explain the data in more detail before we continue in the following videos with a number of more advanced visualisation techniques to extract insights from this data.","tags":"Advanced Data Visualization","url":"/Advanced Data Visualization/2022-05-02-introduction.html","loc":"/Advanced Data Visualization/2022-05-02-introduction.html"},{"title":"Introduction","text":"Introduction Welcome to the video tutorial for the AI Starter Kit on data exploration. In this first video, we will discuss why data exploration is the first step in answering any data science question. Before starting to use complex algorithms to solve a data science problem, you need to gain a thorough understanding of the corresponding data and the underlying business challenge you are trying to address. Data exploration is the process through which you gain such an understanding. This will eventually enable you to derive viable working hypotheses related to the problem at hand, which is useful for the further analysis and modelling of the data. Throughout this process you will be faced with different types of variables, for example numerical or categorical, which require different types of treatment. In addition, looking at individual variables in isolation is not sufficient, as this does not consider the, often complex, interactions between different variables. Furthermore, you will need to identify which are the relevant variables in your dataset and, on the other hand, which ones worsen its outcome rather than improving the quality of your analysis. Finally, data exploration will help you to point out which data quality issues your dataset may have and which actions you should undertake to mitigate them. Amongst others, data exploration is useful to identify usual and unusual values for a variable but also, to study the evolution of a variable over time. It can be helpful to uncover significant relationships between different variables or to assess data quality and suitability. The goal behind this Starter Kit is to lay out a series of analyses that will teach you how to explore individual variables, look at how pairs of variables are related and study the complex interactions between groups of variables. You will learn how to conduct a quantitative and visual inspection of your dataset and you will be prepared for the next step in your advanced analytics workflow. In the tutorial of this AI Starter Kit, we follow a systematic approach to explore a dataset, based on the number of variables under consideration. We start by exploring individual variables in isolation, which is the simplest form of analysis and is called univariate analysis. We continue by studying pairs of variables, which is called bivariate analysis. And finally, we study the relationship between several variables, called multivariate analysis before we conclude on our findings. But before diving into this, in the next video, we will first introduce you to the dataset that we will use in this analysis and perform some basic preprocessing on it.","tags":"Data Exploration","url":"/Data Exploration/2022-09-02-introduction.html","loc":"/Data Exploration/2022-09-02-introduction.html"},{"title":"Introduction","text":"Introduction Description Most data mining and machine learning algorithms do not work well if you just feed them your raw data: such data often contains noise and the most relevant distinguishing information is often implicit. For example, raw sensor data just contains a timestamp and a corresponding value, while interesting aspects of it might be the trends contained within or the number of times a threshold is exceeded. Feature engineering is the process of extracting from the raw data the most relevant distinguishing characteristics that will be presented to the algorithm during modeling. It is a way of deriving new information from the existing data, so that its characteristics are more explicitly represented. The resulting features are eventually feed to the AI/ML algorithm of the model. In practice, the feature engineering step is achieved by the selection of the most appropriate parameters, or the composition of new features by manipulation, transformation and combination of the raw data. Feature engineering is one of the most important and creative steps in the data science workflow. However, there is no clearly-defined formal process for engineering features and, consequently, this requires a lot of creativity, a good understanding of the domain and of the available data, some trial-and-error, etc. It is also desirable to have upfront an idea of the modeling and analysis task for which you want to use the resulting features, as this might help you to identify relevant features. Business goal The overall goal of this Starter Kit is to present some advanced feature engineering steps related to feature construction and extraction . By keeping in mind what is your business question and what is the corresponding data science task , you will be able to derive valuable features that can be used in the next stage of your analysis. Application context Feature engineering is one of the steps in the data science workflow with the most decisive impact on the accuracy of the model you want to develop. It is the final step before actually training the model and it defines how the input data will be fed to the model. Starter Kit outline In this Starter Kit we use a dataset from the Regional Rail System of Pennsylvania. Before starting the feature extraction per se , we will first apply some basic preprocessing to the dataset and have a high-level overview of the data. We will then derive several interesting features from the dataset that can be used to characterise train rides and delays.","tags":"Feature Engineering","url":"/Feature Engineering/2023-04-01-introduction.html","loc":"/Feature Engineering/2023-04-01-introduction.html"},{"title":"Introduction","text":"Introduction Welcome to the tutorial for the AI Starter Kit on time-series pre-processing. In this first video we will provide an overview of the actual challenge we're tackling. We will introduce the most important problems occurring in time series data and explain why it is beneficial to first improve your data before you start any other data-modelling tasks. Time series are characterised as a collection of data points obtained at successive times, most often with equal intervals between them. Since an increasing amount of assets is instrumented with sensors, this type of data is omnipresent, for example when monitoring industrial machinery or resulting from wearables tracking one's health state. Depending on the application domain, time series data is exploited for various purposes. It is used, amongst others, for profiling energy consumption of households predicting imminent failures of a production line, estimating the remaining useful lifetime of a machine, and many more industrial use cases. Typically, time series data cannot be used easily as-is by machine learning algorithms. This can be for example due to data quality issues such as missing values and sensor misreadings. Or because some algorithms are not fit to deal with the continuous nature of this type of data or the associated - often high – frequency with which it is gathered. To illustrate the various methods and techniques in this Starter Kit, we will use a publicly available real-world dataset that consists of sensor data measurements from wind turbines. It exhibits typical characteristics of industrial time series datasets as it is very noisy and contains outliers and missing values. Moreover, it exhibits seasonal patterns across multiple years, as the machine behaviour is influenced by the meteorological conditions. We will use this dataset to illustrate how resampling and smoothing can be applied to gain a better understanding of the behaviour of the time series, how the quality of the data can be improved through normalization and outlier detection, and * how missing data can be imputed in various ways. In the next video, we will perform some initial exploration on the dataset in order to gain some basic understanding of the data.","tags":"Time Series Preprocessing","url":"/Time Series Preprocessing/2022-05-02-introduction.html","loc":"/Time Series Preprocessing/2022-05-02-introduction.html"},{"title":"Introduction","text":"Introduction Welcome to the tutorial for the AI Starter Kit on resource demand forecasting. In this first video we will provide an overview of the actual business case we are tackling. We will introduce the most important concepts and explain why it is beneficial for resource planning to know the expected demand in the future. Resource demand forecasting concerns accurately predicting the future need for a resource, typically using historical information. It is one of the most essential steps in resource demand management, which tries to ensure that sufficient resources are available to satisfy a fluctuating demand. Resource demand forecasting allows one to plan ahead and guarantee that sufficient resources are available when needed and to avoid costly countermeasures in case of shortage. Precisely forecasting the demand of a given resource can be beneficial for several purposes in a wide range of industrial contexts. When knowing the future demand, only the number of resources actually required need to be allocated to a specific task or process. The field ranges from forecasting the number of products that need to be manufactured based on historical sales figures, over predicting the parking demand or traffic density in a particular neighbourhood in order to control traffic in more intelligent ways, to estimating the amount of energy that will be consumed and produced for a particular city or region. Especially in the later field, the demand is influenced by a mixture of external factors such as the weather conditions. For energy suppliers it is more beneficial to buy electricity on the day-ahead market than on the spot market. Consequently, the more accurate the energy consumption can be predicted, the lower the cost. Given that more and more houses and appliances are enabled with a smart meter, an increasing amount of data becomes available that can enable and improve this prediction. Furthermore, these kinds of predictions also allow energy suppliers to better balance demand and supply and helps them to ensure proper grid operation, but also to avoid negative prices, for example in times of high solar irradiation. In this example we see the active power of a single household near Paris shown in green. In orange, the outside temperature is given. The two are strongly correlated – the demand increases with decreasing temperatures since most French households use electrical heating. Therefore, particularly in winter times, the grid has to be stable when facing high demands. In this Starter Kit, we will demonstrate how to effectively forecast the demand for this household for the future by taking external factors into account. In the following videos, we will first start by gaining some first insights in the data in front of us and perform some initial pre-processing steps to prepare the data for the analysis. Subsequently, we will illustrate you how to gain deeper insights by presenting a number of visual and statistical techniques in order to explore the data for identifying interesting patterns. In a next step, useful characteristics or features will be extracted from the dataset that can as serve as input for the forecasting models. Finally, in the last video, we will explain you how to correctly compare the performance of multiple forecasting models. Let us go ahead with the next video on data preparation, in which we will describe how to deal with the typical problems of raw data and how to perform data fusion when multiple sources of data are available. Additional information The video material in this website was developed in the context of the SKAIDive project , financially supported by the European Social Fund , the European Union and Flanders. For more information, please contact us at elucidatalab@sirris.be","tags":"Resource Demand Forecasting","url":"/Resource Demand Forecasting/2022-05-02-introduction.html","loc":"/Resource Demand Forecasting/2022-05-02-introduction.html"},{"title":"Introduction","text":"Introduction Welcome to the tutorial for the AI Starter Kit on advanced visualisation. We will introduce you to a number of more advanced visualisation techniques. You will learn how these can be used creatively to uncover more elaborate insights from your data than would be possible with the more general out-of-the-box visualisations and plots. Data visualisation is an important activity in several phases of a data science project. It allows you to understand several characteristics of a dataset, to discover interesting insights, to validate analysis results, to communicate these results to non-experts via intuitive dashboards, and so on. Popular visualisations often used by data scientists are boxplots, distribution plots, as well as bar and line charts. These are basic out-of-the-box visualisations that are generally applicable, but that are often hard to interpret for non-experts. And more importantly, they do not always reveal interesting insights. More advanced visualisations exploit the human eye's extraordinary visual pattern recognition abilities. A clever visualisation of data can already reveal interesting patterns and insights, even before any complex algorithm is applied. On top of that, they can help in formulating hypotheses to be validated further in the data analytics process or to identify features that can be useful for data-driven modelling. Before we dive deeper in these more advanced visualisation techniques, let us discuss some examples. many time series show some underlying periodicity or seasonality. Depending on the length and granularity of these time series, these seasonal effects are often only hardly visible. In this Starter Kit, we will introduce to you to a number of visualisations that make it very easy to identify for example monthly patterns. Another field where advanced visualisations can help is anomaly detection. Such anomalies appear for example when a machine is failing, and are especially hard to detect when it comes to multi-dimensional data originating from multiple sensors. In this Starter Kit, we will use advanced visualisations such as timeline plots, heatmaps, calendar maps, area plots and scatter plots to visually explore a dataset and gradually build up more knowledge about it. We use a publicly-available dataset that consists of bike counter data, which contains hourly information on the number of bikes that cross at six different spots in Seattle. By visualising this data, we will be able to: explain certain characteristics of the data, such as some nodes having more crossings than others, identify global trends, such as an increase in traffic over the years, and seasonal trends, such as fluctuating popularity within a year, recognize structural patterns, such as distinct weekday and weekend traffic patterns, and finally detect outliers, such as weekend traffic patterns that occur on weekdays. In the next video, we will explain the data in more detail before we continue in the following videos with a number of more advanced visualisation techniques to extract insights from this data.","tags":"Advanced Data Visualization","url":"/Advanced Data Visualization/2022-05-02-introduction.html","loc":"/Advanced Data Visualization/2022-05-02-introduction.html"},{"title":"Introduction","text":"Introduction Welcome to the tutorial for the AI Starter Kit on remaining useful lifetime prediction! In this first video we will provide an overview of the actual business case we're tackling. We will introduce the most important concepts and explain why it is beneficial for maintenance to know the remaining useful lifetime of a machine, a tool or any other industrial asset. Estimating the remaining useful lifetime of an asset can be beneficial for several purposes in a variety of industrial contexts. For one, it can offer support in the better scheduling of maintenance operations, for example for offshore wind turbines. For such assets, maintenance can only be performed under the right weather conditions. But also for other assets, maintenance is an important part of its lifecycle. Traditionally, maintenance is done in a corrective way, such that it is performed at the moment an asset has failed. In that case, the failing part is identified and rectified or replaced, to ensure that the asset can subsequently resume normal operation. Especially for strongly interdependent production lines or industrial assets operating in critical environments, an unplanned downtime is to be avoided and often costly. For this reason, more recently, for an increasing number of assets preventive maintenance is performed. In that case, maintenance tasks are scheduled at regular intervals, avoiding future asset failure to a maximum extent – but with the risk to replace assets that are still working correctly and still could for a while. Hence, the optimal time for replacement is wanted: The time when the asset still works correctly but will probably fail soon. And this is where the remaining useful lifetime comes into play. Nowadays, ever more assets are equipped with different types of sensors gathering data. This started the trend towards predictive maintenance, in which the maintenance moments are decided upon in a data-driven way. Therewith, maintenance is only performed when actually needed. This is a very broad domain which encompasses a variety of topics. Besides the estimation of the asset's remaining useful lifetime – the main topic of this AI starter kit – further related problems to solve for a fully predictive maintenance approach are failure prediction, detection, and diagnosis for root cause analysis. The remaining useful lifetime of industrial assets is defined as an estimate of the remaining time that an item, component, or system is estimated to function as expected. It is expressed as the number of hours, cycles, batches or any other quantity. In the figure we see datapoints collected from an arbitrary sensor in green. Let's say it measures the tool radius of a milling machine. With increasing time, the radius decreases as the tool wears out. At a certain point, the tool is considered too worn out to still further use it in production, which can influence the quality of the produced parts or lead to an unstable production process and consequent damage to the machine. The main questions thus is for how long the tool will it still be able to function properly? By applying Machine Learning algorithms, the continuation of this time series data can be forecasted, denoted in pink. From this forecast, the remaining useful lifetime can be estimated for a given asset. In this case, the machine should be maintained within the next 10 to 15 cycles. With this information, maintenance tasks can be scheduled accordingly – with a tool still functioning and without unexpected downtime. Hence, by estimating the remaining useful lifetime, we minimize the risk of failure and maintenance cost. An accurate prediction of an asset's lifetime can also help to optimise the operational efficiency of an asset, by more optimally planning the use of that asset before its end of life, or adapting its operational use to extend its useful lifetime. In manufacturing settings, where a downtime of the production line results in several operational problems and associated costs, remaining useful lifetime prediction can avoid unplanned downtime. In this AI Starter Kit, we will show its use to avoid safety-critical situations, by illustrating the solution methodology for predicting the remaining useful lifetime of aircrafts engines. To summarize, by applying predictive maintenance it is possible to: - better schedule maintenance operations - optimize operational efficiency - avoid unplanned downtime - anticipate and avoid safety-critical situations. This analysis can be performed for various machines, tools or processes as long as this degradation over time can be measured accordingly, just like resistance, length, temperature or similar measures. Predicting the remaining useful lifetime is typically very challenging for several reasons: First of all, usually a multitude of heterogenous sensor data is measured at several places in the machine. This data is often captured at a high-frequency, consisting of vibration data, acoustic emission, accelerometer data, and many more machine-internal parameters. Secondly, in some settings, also characteristics of the surrounding environment influencing the lifetime of the asset are captured. Further, typically, data that is gathered over a long period of time needs to be available to train the model. Given that industrial assets are often very complex – as they consist of both electrical and mechanical components - it generally is a non-trivial exercise to predict their end of life. On top of that, the data typically comes with a lot of variation, due to varying machine types operating in heterogenous operating conditions with different machine configurations. Therefore, the main challenge in this process is to extract meaningful characteristics that can be used to predict the end of life from the gathered data. This is complicated by the fact that usually very little domain expertise on the operating conditions of these assets is available. Additionally, the environment in which they operate is typically very dynamic. In this AI Starter Kit, we will guide you through a data-driven methodology based on deep learning to tackle this challenge. In the next video, we will concentrate on the data itself that we selected to illustrate the approach. We will explain which information is available and what we can learn from it.","tags":"Remaining Useful Life Prediction","url":"/Remaining Useful Life Prediction/2022-05-02-introduction.html","loc":"/Remaining Useful Life Prediction/2022-05-02-introduction.html"},{"title":"Introduction","text":"Introduction Welcome to the video tutorial for the AI Starter Kit on data exploration. In this first video, we will discuss why data exploration is the first step in answering any data science question. Before starting to use complex algorithms to solve a data science problem, you need to gain a thorough understanding of the corresponding data and the underlying business challenge you are trying to address. Data exploration is the process through which you gain such an understanding. This will eventually enable you to derive viable working hypotheses related to the problem at hand, which is useful for the further analysis and modelling of the data. Throughout this process you will be faced with different types of variables, for example numerical or categorical, which require different types of treatment. In addition, looking at individual variables in isolation is not sufficient, as this does not consider the, often complex, interactions between different variables. Furthermore, you will need to identify which are the relevant variables in your dataset and, on the other hand, which ones worsen its outcome rather than improving the quality of your analysis. Finally, data exploration will help you to point out which data quality issues your dataset may have and which actions you should undertake to mitigate them. Amongst others, data exploration is useful to identify usual and unusual values for a variable but also, to study the evolution of a variable over time. It can be helpful to uncover significant relationships between different variables or to assess data quality and suitability. The goal behind this Starter Kit is to lay out a series of analyses that will teach you how to explore individual variables, look at how pairs of variables are related and study the complex interactions between groups of variables. You will learn how to conduct a quantitative and visual inspection of your dataset and you will be prepared for the next step in your advanced analytics workflow. In the tutorial of this AI Starter Kit, we follow a systematic approach to explore a dataset, based on the number of variables under consideration. We start by exploring individual variables in isolation, which is the simplest form of analysis and is called univariate analysis. We continue by studying pairs of variables, which is called bivariate analysis. And finally, we study the relationship between several variables, called multivariate analysis before we conclude on our findings. But before diving into this, in the next video, we will first introduce you to the dataset that we will use in this analysis and perform some basic preprocessing on it.","tags":"Data Exploration","url":"/Data Exploration/2022-09-02-introduction.html","loc":"/Data Exploration/2022-09-02-introduction.html"},{"title":"Introduction","text":"Introduction Description Most data mining and machine learning algorithms do not work well if you just feed them your raw data: such data often contains noise and the most relevant distinguishing information is often implicit. For example, raw sensor data just contains a timestamp and a corresponding value, while interesting aspects of it might be the trends contained within or the number of times a threshold is exceeded. Feature engineering is the process of extracting from the raw data the most relevant distinguishing characteristics that will be presented to the algorithm during modeling. It is a way of deriving new information from the existing data, so that its characteristics are more explicitly represented. The resulting features are eventually feed to the AI/ML algorithm of the model. In practice, the feature engineering step is achieved by the selection of the most appropriate parameters, or the composition of new features by manipulation, transformation and combination of the raw data. Feature engineering is one of the most important and creative steps in the data science workflow. However, there is no clearly-defined formal process for engineering features and, consequently, this requires a lot of creativity, a good understanding of the domain and of the available data, some trial-and-error, etc. It is also desirable to have upfront an idea of the modeling and analysis task for which you want to use the resulting features, as this might help you to identify relevant features. Business goal The overall goal of this Starter Kit is to present some advanced feature engineering steps related to feature construction and extraction . By keeping in mind what is your business question and what is the corresponding data science task , you will be able to derive valuable features that can be used in the next stage of your analysis. Application context Feature engineering is one of the steps in the data science workflow with the most decisive impact on the accuracy of the model you want to develop. It is the final step before actually training the model and it defines how the input data will be fed to the model. Starter Kit outline In this Starter Kit we use a dataset from the Regional Rail System of Pennsylvania. Before starting the feature extraction per se , we will first apply some basic preprocessing to the dataset and have a high-level overview of the data. We will then derive several interesting features from the dataset that can be used to characterise train rides and delays.","tags":"Feature Engineering","url":"/Feature Engineering/2023-04-01-introduction.html","loc":"/Feature Engineering/2023-04-01-introduction.html"}]};