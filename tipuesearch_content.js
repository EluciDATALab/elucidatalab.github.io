var tipuesearch = {"pages":[{"title":"Welcome","text":"Category: Blog Welcome to the official ElcuiDATA Lab bloging and starterkit platfrom. Here you will hopefully find some latest machine learning content and tutorials. Enjoy!","tags":"Blog","url":"/Blog/first-post.html","loc":"/Blog/first-post.html"},{"title":"Key Take Away Messages","text":"Key Take Away Messages In the video tutorial for this AI Starter Kit, we demonstrated forecasting the demand for a particular resource at a particular point in the future, illustrated on the case of energy demand forecasting. In doing so, we have taken you through the different steps in the data science workflow. We first performed a number of required data preprocessing steps in order to prepare the data for further analysis. This included fusing the electricity and weather data, requiring to resampling the former data to a lower sampling rate. Subsequently, we explored the fused dataset in order to better understand which factors influence the household energy consumption. To this end, we resorted to both visual and statistical exploration methods. Based on this, we could identify a number of yearly, weekly, and daily patterns, including particular behavior during weekends and some holiday periods. This information formed the basis for the feature extraction that followed next, such as the energy consumption a week before, the day of the week and the hour of the day. Finally, we served the extracted features to two different models, namely Random Forest Regression and Support Vector Regression to perform the actual forecasting. Through different experiments, we analyzed the influence of the training strategy, type of machine learning model and effect of data normalization on the model performance. Based on this, it became clear that achieving good prediction results depends on different factors, including the appropriate data preprocessing, the amount of available training data and the algorithm parametrisation. As next steps, we invite you to further explore each of these influences in more detail. Especially with respect to the hyperparameter tuning, it proves worth to further explore the different parameter settings and study how these changes impact the model performance. Furthermore, you can also try to adapt the different steps to the context of your own dataset. While the details of each of the steps might differ, the methodological steps we presented are typically the required phases you need to go through when solving a resource demand forecasting problem. We thank you for completing this video series and hope to welcome you in another AI Starter Kit tutorial.","tags":"Resource demand Forecasting","url":"/Resource demand Forecasting/2020-01-08-key-take-away-messages.html","loc":"/Resource demand Forecasting/2020-01-08-key-take-away-messages.html"},{"title":"Data Modelling and Forecasting","text":"Data Modelling and Forecasting Now that we gained the necessary insights from the data, extracted a number of meaningful features, and gained some theoretical background on regression models, we will use the AI Starter Kit to discover the most important factors for training a machine learning model. More specifically, we will analyze the influence of the training strategy, type of machine learning model and effect of data normalization on the model performance. To evaluate the performance of the models the mean absolute error or MAE is used - a metric commonly used in literature for this purpose. This metric quantifies to what extent the model forecasts are close to the real values. As the name suggests, the mean absolute error is an average of the absolute errors. It holds that the lower the MAE of the model, the better its performance. As just mentioned, the training strategy, the machine learning model and the data normalization may influence the quality of the predictions. Before we start training a model, let us dive a bit deeper in these three influences. First of all, let us turn to the training strategy. The training data typically highly influences quality of the model. Therefore, in this starter kit, we will experiment with six different training strategies, to study their influence on the quality of the resulting model. First, we will use 1 month before the test month as training data. In the Starter Kit, several models are trained for the predictions of each individual month. Each model is trained on one month before the month we are making predictions for. A one-month gap is introduced between the training and the test month in order to avoid that the last day of the training set is also included in the first day of the test set. For example, the models that will make the predictions for April 2008 will be trained on the data from February 2008. Secondly, we will use 6 months before the test month as training data. This is thus highly similar to the above experiment, but with 6 months of training data. It still includes a 1-month gap. Or we can go back even further in time and use 1 year before the test month as training data. Similarly, we can use 1 month the year before as training data. This is similar to strategy number 1, but with a gap of 11 months. This way, the training data and test data are taken from the same month, one year apart. Further, we can also use all months before the test month as training data. For each test month, a model is trained using all the data prior to this (including a 1-month gap). This strategy simulates the scenario where the model is retrained as new data comes in. Since this requires training several models based on potentially large training sets, this might require a certain computational time. Finally, we experiment with training on the first year of data and make predictions on the rest. This strategy is commonly used for training these kinds of models. With these training strategies, we will see how strongly the amount of data but also the seasonal pattern will influence the quality of the model. As already introduced in the former video, we will train two different types of models, namely a Random Forest Regressor and a Support Vector Regressor. Besides these models, we will also use a simple benchmark model that we use to compare the performance of the models against. For the prediction of the energy of a certain day, this benchmark model simply takes the energy consumption of the previous day at the same time. Note that for this approach no model is built and consequently it does not include any training phase. Finally, a short note on normalization. We saw already in the video on data understanding that the scale for the outside temperature and global active power are quite different. This is also true for the remaining features that we introduced. Therefore, it might be necessary to normalize the data, that is, rescale it such that all input and output values are between 0 and 1. This is indeed a requirement for the correct training of some machine learning models. We will test in the Starter Kit whether it makes a difference for the models suggested. At first, let us run the baseline model as we will later on compare all model results to these results. The mean absolute error is 604.45. Putting this number into relation with the mean global active power of 1118, the error is comparably high. For reasons of comparability, each result will be shown in the table just below the interface. As discussed above, we first want to analyze the influence of the training strategy. Therefore, we train the Support Vector Regressor on strategy 1, so for 1 month of data, and on strategy 2, for 6 months of data. In this first experiment, we will not normalize the data beforehand. For both strategy 1 and 2, the mean absolute error is even bigger than for the benchmark model. Therefore, we are going to use the normalized data instead. With that, the model predictions improve and for both strategies, this results in better forecasts than the baseline model. We can do the same for the Random Forest Regressor. We train if for both strategies – namely strategy 1 and 2 – and on the normalized and non-normalized data. In all four cases, the results are better than those obtained by both the baseline model and the Support Vector Regressor. Now it is up to you. Train different models and find out which model returns the best results and which influences are the strongest. If you want, you can pause the video for this. These are the basic findings when training all models: Concerning the training strategy, we see that for the Support Vector Regressor the standard train-test split has the best performance, although using all months and using a one-year window before the test month as training data both have a similar performance. Furthermore, we clearly see that using a one-year window before the test month as training data leads to better performance than the other training strategies. We note that for both this \"one year window\" strategy and the standard train-test split, the training data is one year. The difference, however, is that for the former the training set changes, while for the latter it is fixed meaning that all the data from 2017 is used. A general trend that can be observed is that the performance increases as a larger training set is used. We therefore urge the user to look at the predictions made using the training strategy where all months before the test month are used as training data: we can expect that the predictions for the later months will be better than for the first months since more training data was used. However, this is hard to see by eye in the prediction plot directly. Regarding the chosen model, we can see that both the Support Vector Regressor and Random Forest Regressor outperform the simple baseline we set up. Further, the Random Forest Regressor outperforms the Support Vector Regressor for all training strategies. This shows the importance of knowing which model is best suited for your problem and testing different ones. Important to note though, is that no extensive hyperparameter tuning was performed, which could alter this observation. This is required for a proper validation of the algorithms and therefore we encourage the user to also experiment with altering the hyperparameters and study the influence on the results. Finally, normalizing the data, rescaling it such that the input and output variables all have values within a similar range - in this case, between 0 and 1 - is a common step when setting up machine learning models. This is because, depending on the model, they work much better with values of this order of magnitude. We see that normalization indeed greatly improves the predictions for the Support Vector Regressor but has little influence on the random forest regressor. Indeed, algorithms such as random forests and decision trees are not influenced by the scale of the input variables. We hope that you have gained more insights on how the training strategy, the type of machine learning model and data normalization influence the model performance and are familiar now with the usage of the interface. We suggest that you try a number of additional combinations of settings and think about in which way they influence the quality of the model. In the next video, we will summarize the key take away messages and provide you with a number of suggestions for additional experiments to gain additional insights.","tags":"Resource demand Forecasting","url":"/Resource demand Forecasting/2020-01-07-data-modelling-and-forecasting.html","loc":"/Resource demand Forecasting/2020-01-07-data-modelling-and-forecasting.html"},{"title":"Data Science Theory - Advanced Regression Models","text":"Data Science Theory: Advanced Regression Models Before deciding on the most appropriate algorithm to solve a particular data science problem, a first step is to decide which type of task you are trying to solve. In order to do so, you usually need to start with finding the answer to a number of questions, based on the case under consideration. Without a clear understanding of the use case, even the best data science model will not help. A first question to answer in that respect is which type of outcome is expected from the use case owner. Is the aim to predict a category, such as ‘normal', ‘degrading' or ‘failed'? If yes, the next question to answer is whether labelled data is available or not. Labelled data is data for which examples are available that are annotated by a domain expert with the classes to predict. Put differently, for each data point or set of data points, a class is defined. Usually, the number of unique classes is rather small. This data will be used by the algorithm for training a model. Once trained, it can be evaluated on a test data set, for which the classes are known but will not be visible to the model. Evaluating the ratio of correctly predicted classes gives a measure of the quality of the model. Often used algorithms for classification are k-Nearest Neighbors, Decision Trees or Support Vector Machines. But what can we do if we do not have information available on possible classes? In that case, introducing a similarity between the examples that you have available makes it possible to cluster different data points into groups. These groups can then be used to gain deeper insights into the data, and in some cases can be mapped to particular classes. Partitioning-based clustering, hierarchical clustering or density-based clustering are often used techniques for this purpose. The situation is different in case a numerical value should be predicted. It is similar to the classification task, but the prediction range is continuous. For these so-called regression tasks, Ordinary Least Squares or Support Vector Regression are often used. If the goal is neither to predict a class nor a numerical value, but rather a future state, one typically turns to graphical modelling algorithms in order to predict these states. These techniques also allow one to include the available background knowledge into the modelling process. Examples of graphical modelling techniques are Hidden Markov Models and Bayesian Networks. To make the difference between the single categories a bit more clear, we discuss some examples: In case no labelled data is available, clustering of data points can provide insights in different modes in the data. An example is performance benchmarking of industrial assets. Let's assume the data to analyze comes from a wind turbine park. When looking at several measurements, like for example of the power curve, the wind speed, and the wind direction, we can identify different modes in which the single wind turbines are operating. In contrast, assume that we are interested in the expected power production of a particular wind turbine in the following days for which we have a weather forecast. We can use this information as input variables for a regression model and therewith predict the power production. If labels are attached to the gathered data, for example on the root cause of particular sensor failures, a classification algorithm can be used to train a model that is able to determine which fault is associated with a certain set of sensor readings. With this knowledge, we can decide which type of algorithms is suitable for the forecasting of electricity consumption. Let us use the decision tree for this: Do we want to predict a category? No, right, we are looking for a continuous value. So, we go to the right. And yes, we need to predict a value. Therefore, it is a regression task we are facing here. The question we want to answer is precisely: How much electricity will be consumed in the next hour, taking into account historical information regarding the electricity consumption and the outside temperature? There is a bunch of regression algorithms that can be used in various contexts. In our case, we have a comparably small feature set and all of them are numerical. Therefore, we will go for two commonly used algorithms for the prediction, namely Random Forest Regressors and Support Vector Regressors. We will introduce both algorithms in more detail in the remainder of this video. Random Forest Regression We start with Random Forest Regression. The base to build a random forest is a decision tree – which works similarly to the one we just used to determine which class of algorithms is suitable for the electricity forecasting. Since in a random forest, the model is defined by a combination of trees, it is a so-called ensemble method. Ensemble methods help improve machine learning results by combining several models. This approach allows the production of better predictive performance compared to a single model. From each decision tree a value is predicted, and the final prediction will be a weighted function of all predictions. Here we see a very simplistic version of a random forest regressor with only three decision trees. All trees are trained in parallel and each one will predict a value for a given set of input variables. The final prediction in this case would be the mean value of all predictions, ergo 10,67. In order to improve the performance of a model, you need to tune the algorithm's hyperparameters. Hyperparameters can be considered as the algorithm's settings, or put simply, the knobs that you can turn to gain a better result. These hyperparameters are tuned during the training phase by the data scientist. In the case of a random forest, the most important hyperparameters include the number of decision trees in the forest, the maximum depth of each of these trees, and the minimum number of examples and maximum number of features to consider when splitting an internal node in one of these trees. Support Vector Regression Another type of regression approach is support vector regression. While support vectors are mainly used in the field of classification, with some adaptions, it also works for regression tasks. It works similarly to an ordinary least squares regression where the linear regression line is targeted with the smallest overall deviation from the data points. This is very handy in case of linear dependencies and for clean data. But as soon as there are several outliers in the data set or the relation between the data points is non-linear, the quality of the model can decrease significantly. Especially in the context of industrial data, this can never be fully avoided. For Support Vector Regression a band of width epsilon ε is defined. We call that band the hyperplane . The aim is to search the hyperplane that includes most points while at the same time the sum of the distance of the outlying points may not exceed a given threshold. The training instances closest to the hyperplane that help define the margins are called Support Vectors . As for random forest regression, also support vector regression has a number of important hyperparameters that can be adjusted to optimize the performance. A first important hyperparameter is the choice for the type of kernel to use. A kernel is a set of mathematical functions that takes data as input and transform it into the required form. This kernel is used for finding a hyperplane in a higher dimensional space. The most widely used kernels include Linear, Non-Linear, Polynomial, Radial Basis Function (RBF) and Sigmoid. The selection of the type of kernel typically depends on the characteristics of the dataset. The cost parameter C tells the SVR optimization how much you want to avoid a wrong regression for each of the training examples. For large values of C, the optimization will choose a smaller-margin hyperplane if that hyperplane does a better job of getting all the training points predicted correctly, and vice versa. The size of this margin can be set by epsilon, which specifies the band within which no penalty is associated in the training loss function with points predicted within a distance epsilon from the actual value. Now that we gained some more knowledge on these two frequently used regression approaches, in the next video we will explain how to train a regression model for our household energy consumption prediction problem.","tags":"Resource demand Forecasting","url":"/Resource demand Forecasting/2020-01-06-data-science-theory-advanced-regression-models.html","loc":"/Resource demand Forecasting/2020-01-06-data-science-theory-advanced-regression-models.html"},{"title":"Statistical Data Exploration and Feature Engineering","text":"Statistical Data Exploration and Feature Engineering In the former video, we performed a visual data exploration. We could already gain quite some insights from the figures we showed. In this video, we will concentrate stronger on statistics in order to verify our findings. Finally, we will prepare the datasets for modelling purposes by extracting a number of distinguishing features which will serve as input for the machine learning algorithm. In order to find repetitive patterns in time series data, we can use autocorrelation. It provides the correlation between the time series and a delayed copy of itself. In case of a perfect periodicity as shown in the figure, the autocorrelation equals 1 if we delay one copy by the periodicity. In order to investigate the autocorrelation in case of the global active power, we visualize the autocorrelation with time lags of 1 day, 2 days, and so on. Note that we do not show the results of a delay of 0 days, since the correlation with the exact same day would trivially be 1. For bigger delays, a clear peak occurs after 1 day, indicating the consumption of a day is highly correlated with the consumption of the previous day. And more obviously, several peaks occur every 7 days, which confirms the existence of a weekly pattern. That means that the consumption of a day is highly correlated with the consumption of the same day one or several weeks earlier. In the data exploration video, we further saw that the temperature and the global active power show an opposite trend. In order to verify the strength of this relationship, we can compute the correlation between these two variables. For this purpose, we compute Pearson's correlation. The Starter Kit allows to calculate the correlation between the temperature and the global active power for different resampling rates. Why is this important? One reason is the time scale for changes. The temperature typically changes less drastically over time than energy consumption does. In the figure, on the diagonal, we see the correlation of the two time series with each other, resulting in a perfect correlation of 1 as expected. On the opposite, along the antidiagonal, we see the correlation between the global active power and the outside temperature. In case of a sampling rate of 1 hour, the negative correlation is weak with roughly -0.2. With an increased resampling rate of 1 week, the negative correlation is more evident with a value of -0.75. This confirms our insight from the visual inspection. With a larger sampling rate, the short-term fluctuations – for example from your washing machine – are smoothed out and leaves us with the seasonal pattern. From these insights, we can now start extracting features from the extended dataset that we can use later on for the electricity consumption forecasting. A feature is an individual measurable property of the phenomenon being observed. Choosing informative, discriminating, and independent features is a crucial step for building effective machine learning algorithms. Based on the previous visual and statistical observations, we will consider the following features to predict the energy consumption for the hour ahead: energy consumption of the day before because we saw that there was a peak in the autocorrelation with a delay of one day; energy consumption of the 7 days before due to the weekly pattern; the hour of the day, which will allow to distinguish between different periods of the day (i.e., night, morning, afternoon, evening); the day of the week, which will allow to distinguish between weekdays and weekend days whether the day is a holiday or not; the month of the year, which will allow to distinguish between yearly seasons; and finally, the temperature. That results in the final dataset that will be used as input for the machine learning algorithm to learn the model. Before discussing the modelling step in more detail, in the next video, we will provide a theoretical overview of the approaches chosen for the electricity consumption forecast.","tags":"Resource demand Forecasting","url":"/Resource demand Forecasting/2020-01-05-statistical-data-exploration-and-feature-engineering.html","loc":"/Resource demand Forecasting/2020-01-05-statistical-data-exploration-and-feature-engineering.html"},{"title":"Visual Data Exploration","text":"Visual Data Exploration Welcome to the third video of the tutorial for the AI Starter Kit on resource demand forecasting! Let us now explore the extended dataset in order to understand which factors influence the household energy consumption. Based on this exploration, we will later on identify which features are useful for training a machine learning model to forecast energy consumption. Time plots are a common way for exploring time series data, such as the energy consumption that we consider here. We explore time plots in different time frames in order to highlight different seasonal patterns in the data. We start with a yearly plot, which allows us to identify global patterns. Then, a monthly plot allows us to zoom in and understand relationships between energy consumption in different weeks and days along the year. And finally, a weekly plot allows us to zoom in even further and identify daily and hourly patterns. We start with the yearly plot. The data ranges from January 2007 to end of 2010. We can easily see the yearly pattern for the global active power with peaks in winter and valleys in summer. With these settings though, we see the temperature values only as a flat line because they are about two magnitudes smaller. When only displaying the temperature, we see a similar pattern with high values in summer and low values in winter, as expected. In order to make the two-time series comparable, we normalize the data, such that the maximal value of each time series corresponds to 1 and the lowest one corresponds to zero. Like this, we can see that the two show an opposite evolution over the year. For the monthly data, we progress analogously. While we can again clearly see the annual effect, there is no clear pattern recurring every month. One interesting thing that can be observed is that strong dips in power consumption tend to happen at the same time in February and November in 2007 and 2008. Most likely, these are holidays which tend to happen at the same time for the household under investigation. This can be best seen when the sampling rate is changed. Instead of showing the data per day, we can have a look at the rolling mean of the active power over three days to see the pattern more clearly. We further explore the evolution of the energy consumption in a shorter time frame. We want to verify whether during a week, the energy consumption exhibits the same patterns. In the following plot, the user can inspect and compare the energy consumption in December of different years. We mark weekends in yellow and Christmas day in orange. What can you notice when you compare weekdays and weekends? And how about the holidays? Exactly, the mean consumption is higher on the weekends. The reason might be that more members of the household are at home during the weekends. Comparing the week around Christmas 2008 and 2009 shows interesting patterns. While the household was obviously at home and maybe even with guests in 2009, it looks like they were not around in 2008 for several days. Finally, we analyze the daily pattern. For this, rather than showing the resampled dataset, we use the original dataset, which provides consumption with a sampling rate of 1 minute and which can show more detailed patterns. Note that this means we are using the original units of kW. In the figure, we see how the power consumption follows a daily recurring pattern, with peaks in the morning and evening and troughs at night and noon – on weekdays. The 5th of December was a Saturday, and we can clearly see how the curve of that day deviates from the weekdays. During the night, we still see a low and more or less constant power usage, most likely due to appliances like refrigerators that run continuously. In the next video, we will discuss the statistical significance of the patterns detected and prepare the data for machine learning modelling by extracting meaningful characteristics or features.","tags":"Resource demand Forecasting","url":"/Resource demand Forecasting/2020-01-04-visual-data-exploration.html","loc":"/Resource demand Forecasting/2020-01-04-visual-data-exploration.html"},{"title":"Data Preprocessing","text":"Data Preprocessing Welcome to the second video of the tutorial for the AI Starter Kit on resource demand forecasting! In this video, we will detail the dataset that we will use and perform the necessary preprocessing steps in order to prepare the data for further analysis. In this AI Starter Kit, we will work with a publicly available energy consumption dataset from the UCI repository. This dataset contains measurements of electric power consumption in one household, located near Paris, with a one-minute sampling rate over a period of almost 4 years. Different electrical quantities and sub-metering values are available, as you can see in the table. Let us first have a look at the single variables given in the table: First of all, and most importantly, the global active and reactive powers are given. The active power specifies the amount of energy the single electric receivers transform into mechanical work and heat. This useful effect is called 'active energy'. On the contrary, several receivers, such as motors, transformer, or inductive cookers need magnetic fields in order to work. This amount is called reactive power as these receivers are generally inductive: This means, they absorb energy from the network in order to create magnetic fields and return it while they disappear. This results in an additional electrical consumption that is not useful for the receivers. Further, the voltage and current intensity are given in Volts and Ampere, respectively. Finally, the submeterings provide us a more detailed insights where the power is consumed. Submetering 1 corresponds to the kitchen, containing mainly a dishwasher, an oven, and a microwave. Submetering 2 corresponds to the laundry room, containing a washing-machine, a tumble-drier, a refrigerator and a light. Lastly, submetering 3 corresponds to an electric water-heater and an air-conditioner. We expect, that the different submeterings obey different patterns: While an oven mainly consumes power during lunch and dinner time, a refrigerator has a more or less constant consumption during the entire day. As mentioned before, we will also take weather measurements into account. For this, we use measurements recorded close to the city of Paris, where the household is located, provided by the Wunderground website. It contains information on climate conditions on a 30-minutes base. The dataset contains the outside temperature and the dew point in degrees Celsius, the air humidity in percentage, the pressure at sea level in hPa, the visibility in km and the wind direction expressed in degrees. The weather data is measured by a set of connected sensors. For these, it is rare that the data availability is uninterrupted for such a long period. For this reason, we first check the general statistics for the data set. For most variables, the minimal value is -9999, which indicates missing values as also specified in the documentation of the data. We will replace these with so-called 'Not a Number' values or NaNs such that they will not interfere too much during data modelling. In a next step, we fuse the electricity data and the weather data. Note, that the temporal resolution of the two is not the same – the power data is measured every minute while the weather information is available only every 30 minutes. The integration of these two datasets thus requires careful consideration. Two options are possible: we can either upsample the weather data or downsample the household data. That means that we either linearly interpolate the information from weather data on a one-minute interval or aggregate the power data on bigger intervals. In this use case, the latter is more reasonable, as otherwise, we would need to find a way to impute missing data in the climate dataset. Let us first analyze the effect of downsampling the power data. Therefore, we calculate the rolling mean values of the original power over a given time window. You can choose between a sampling rate of 30 or 60 minutes, or 4 hours. The light blue time series shows the original data, the dark blue one the downsampled one. In all cases, the data looks smoother as the original data, as taking the mean value smooths out the major short-time power peaks but at the same time results in some loss of information. With the larger window size of 60 minutes this effect is stronger than for the smaller one. A window size of 4 hours arguably removes too much variation. Note that the weather data is available every 30 minutes such that in case of an hourly time window for the power data also the weather data has to be resampled. Since the information loss for the 60-minute interval is reasonable, we decide to proceed with this sampling rate. This also allows us to reduce the size of the dataset, which will reduce the computation time required for the analysis. In order to merge the two data sets, we transform the active power from a minutes-based power measurement given in kW to an hourly consumption given in Wh. Therefore, we sum the power over one hour and divide it by 60 to get the actual energy. By additionally multiplying by a factor of 1000, we change units from kWh to Wh. For the weather data, we proceed similarly by taking the mean temperature over one hour. With this, we can join the two data sets. For future analysis, we will only take the global consumption and temperature into account. Now that we have preprocessed the dataset, we will illustrate how to gain deeper insights in the data through both visual and statistical data exploration. In the next video, we will start with the visual exploration by means of time plots.","tags":"Resource demand Forecasting","url":"/Resource demand Forecasting/2020-01-03-data-preprocessing.html","loc":"/Resource demand Forecasting/2020-01-03-data-preprocessing.html"},{"title":"Introduction","text":"Introduction Welcome to the tutorial for the AI Starter Kit on resource demand forecasting. In this first video we will provide an overview of the actual business case we are tackling. We will introduce the most important concepts and explain why it is beneficial for resource planning to know the expected demand in the future. Resource demand forecasting concerns accurately predicting the future need for a resource, typically using historical information. It is one of the most essential steps in resource demand management, which tries to ensure that sufficient resources are available to satisfy a fluctuating demand. Resource demand forecasting allows one to plan ahead and guarantee that sufficient resources are available when needed and to avoid costly countermeasures in case of shortage. Precisely forecasting the demand of a given resource can be beneficial for several purposes in a wide range of industrial contexts. When knowing the future demand, only the number of resources actually required need to be allocated to a specific task or process. The field ranges from forecasting the number of products that need to be manufactured based on historical sales figures, over predicting the parking demand or traffic density in a particular neighbourhood in order to control traffic in more intelligent ways, to estimating the amount of energy that will be consumed and produced for a particular city or region. Especially in the later field, the demand is influenced by a mixture of external factors such as the weather conditions. For energy suppliers it is more beneficial to buy electricity on the day-ahead market than on the spot market. Consequently, the more accurate the energy consumption can be predicted, the lower the cost. Given that more and more houses and appliances are enabled with a smart meter, an increasing amount of data becomes available that can enable and improve this prediction. Furthermore, these kinds of predictions also allow energy suppliers to better balance demand and supply and helps them to ensure proper grid operation, but also to avoid negative prices, for example in times of high solar irradiation. In this example we see the active power of a single household near Paris shown in green. In orange, the outside temperature is given. The two are strongly correlated – the demand increases with decreasing temperatures since most French households use electrical heating. Therefore, particularly in winter times, the grid has to be stable when facing high demands. In this Starter Kit, we will demonstrate how to effectively forecast the demand for this household for the future by taking external factors into account. In the following videos, we will first start by gaining some first insights in the data in front of us and perform some initial pre-processing steps to prepare the data for the analysis. Subsequently, we will illustrate you how to gain deeper insights by presenting a number of visual and statistical techniques in order to explore the data for identifying interesting patterns. In a next step, useful characteristics or features will be extracted from the dataset that can as serve as input for the forecasting models. Finally, in the last video, we will explain you how to correctly compare the performance of multiple forecasting models. Let us go ahead with the next video on data preparation, in which we will describe how to deal with the typical problems of raw data and how to perform data fusion when multiple sources of data are available.","tags":"Resource demand Forecasting","url":"/Resource demand Forecasting/2020-01-02-introduction.html","loc":"/Resource demand Forecasting/2020-01-02-introduction.html"},{"title":"Getting Started with the Interactive Experimenting Environment","text":"Getting Started with the Interactive Experimenting Environment In the AI Starter Kits, we mainly work in so-called Jupyter Notebooks . The Jupyter Project is a non-profit, open-source project born in 2004. Jupyter Notebook, that was developed within the project, is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text [1]. If you have never worked with Jupyter Notebooks before, we invite you to read this short tutorial before starting with the actual content of the AI Starter Kits. We provide you some short explanations on how to use the web application to run the python code that was developed. How to get started? For the AI Starterkits, getting started is very easy! Just click at the link provided on the platform and it will bring you directly into Jupyter Hub, which hosts the Jupyter Notebooks for the Starter Kits. For each Starter Kit, we created a so-called environment in which all libraries needed are already installed such that you do not need to take care of this at all. You can get started straight away. How to use the Jupyter Notebooks? Before we have a look at the notebooks themselves, we first have a brief look at the toolbar at the top of each notebook as shown in the following figure. The toolbar provides all buttons to interact with the code in a notebook. We go through the single buttons step by step. Important to understand is that each Jupyter Notebook consists of different cells. In each cell, you can execute a batch of code. In order to create a new cell, you simply click at the button and a new code cell will appear in the notebook. Try it yourself and create a new cell. You notebook should look like this now: In this cell, we can execute a simply command like print(\"Hello World!\") and execute it by pushing the or more easily by pushing the combination SHIFT + Enter . Congratulations, you just executed your first python command in a Jupyter Notebook! Just below the cell, the result in shown: In this case, the words 'Hello World!' are printed. Additionally, a new cell was automatically created just below the former one. Of course, we can also execute several commands within one cell. For this, please enter the following lines in the next cell: a = 5 a b = 3 a + b Before you execute the cell, think about the output you expect. Was the output as expected? Or did you expect to also see the value of a ? Keep in mind that Jupter Notebook will always return the last executed command only. In case you also want to see the value of a , you need to wrap a print(a) command around it. How to add documentation to your notebook? In order to increase the readability of the notebook, it helps to add some documentation around the single commands. This can be helpful in case you want to share the notebooks with others and want to explain why you performed a given command. The easiest way to to this is my changing the cell output in the toolbar: After doing so, you can enter some text in Markdown format around your cells. Test it by entering This was a ** nice ** example ! into a Markdown cell and execute it just as a normal code cell by pushing the button. How to save a Jupyter Notebook? Once you are done with a notebook you have two simple ways to share it with others: * Directly as a Jupyter Notebook. If the recipent has a Jupyter environment, he or she can directly continue working on your code. * Otherwise, you can also export the Notebook in html format: Bibliography [1] https://jupyter.org/","tags":"Resource demand Forecasting","url":"/Resource demand Forecasting/2020-01-01-getting-started.html","loc":"/Resource demand Forecasting/2020-01-01-getting-started.html"},{"title":"Interactive notebook","text":"Instructions for AWS Log in with your username and password Access folder resource_demand_forecasting Open file elucidata_demonstrator_1_3_interactive.ipynb Select Cell > Run all to run all code cells In [ ]: % load_ext autoreload % autoreload 2 from IPython.core.display import display , HTML display ( HTML ( \" \" )) In [ ]: import numpy as np import pandas as pd import matplotlib import warnings warnings . filterwarnings ( 'ignore' ) pd . set_option ( 'display.float_format' , ' {:.3f} ' . format ) from d1_3 import * from d1_3_interactive import run_app In [ ]: data = read_consumption_data () ext_data = read_climate_data () Starter Kit 1.3: Resource Demand Forecasting Business context Resource demand forecasting concerns accurately predicting the future need for a resource, typically using historical information. It is one of the most essential steps in resource demand management, i.e. ensuring sufficient resources are available to satisfy a fluctuating demand. Resource demand forecasting allows one to plan ahead and guarantee that sufficient resources are available when needed and to avoid costly countermeasures in case of shortage. A typical example of resource demand forecasting is provided by the energy sector. For energy suppliers it is more beneficial to buy electricity on the day-ahead market than on the spot market. Consequently, the more accurate the energy consumption can be predicted, the lower the cost. Given that more and more houses and appliances are enabled with a smart meter, an increasing amount of data becomes available that can enable and improve this prediction. Furthermore, these kinds of predictions also allow energy suppliers to better balance demand and supply, and helps them to ensure proper grid operation (and e.g., prepare for possible peak loads in winter time). This Starter Kit will focus energy consumption forecasting as a use case for illustrating how resource demand forecasting using historical data can be realized. Business goal The business goal related to this Starter Kit is forecasting the demand for a particular resource at a particular point in the future based on usage data of that resource that is available from the past. More specifically, we will illustrate a data-driven approach for forecasting the energy consumption of households. Application contexts Forecasting the demand of a particular resource can be useful for several purposes in a variety of industrial contexts: Forecast parking demand or traffic density in a particular neighbourhood in order to control traffic in intelligent ways (e.g. divert to alternative routes or parking spots) Predict battery consumption based on operating conditions in order to intelligently suggest recharging moments or switch to low-power mode Estimate the usage of consumables (e.g., ink, paper) based on product usage data in order to deliver new consumables exactly on time before stock breakage or avoid expensive storage of superfluous stock (e.g. in a manufacturing environment) ... Data requirements In order to realize the business goal, we need a dataset (time series) that includes: a value indicating the demand for the resource at each moment in time. the factors that influence the demand of the resource. These can be internal or external, for example: the energy consumption of a building is influenced by the outside temperature, the amount of people present in the building, etc. the consumption of raw materials is influenced by the type of product that is being produced, the production line settings, etc. the traffic density is influenced by the time of day, the presence of road works, the weather, etc. 'representative' historical data. The amount of data that is needed to make an accurate prediction is typically determined by the length of the temporal patterns in the data. For example, if the data expresses a yearly seasonality, historical data of multiple years is needed. data sampled with the 'appropriate' frequency (i.e., how many times per minute/hour/day you gather data points). This frequency is related to the required forecast frequency, e.g. it is impossible to forecast every 10 minutes when only 15-minute data is available. In some use cases, where resource demand forecasting concerns particular processes, (expert) knowledge on these processes is required (i.e. as labeled data) Starter Kit outline In this Starter Kit, we will illustrate a data-driven approach for forecasting the energy consumption of a household. As a dataset we will use the energy consumption of the household and climate data collected by a nearby weather station. We will use this dataset to illustrate: How to prepare your data for the analysis. We will describe how to handle the typical problems of raw data and how to perform data fusion when multiple sources of data are available. How to get insights from your dataset. We will present visual and numerical techniques in order to explore your data for identifying interesting patterns. How to transform the obtained insights into useful features for building forecasting models. How to compare correctly the performance of multiple forecasting models. Data loading and preprocessing For the household energy consumption use case we consider here, two types of data are relevant: Household data, which refers to the total household energy consumption along with the consumption of the individual energy-consuming appliances Climate data, which contains information of climate factors (e.g. temperature) that can influence on the energy consumption of households In the following subsections, we will load these datasets and perform some basic preprocessing, e.g. correctly handling missing values and fusing both datasets. Household data As a dataset we use the public energy consumption data set from the UCI repository . This dataset contains measurements of electric power consumption in one household, located near Paris, with a one-minute sampling rate over a period of almost 4 years. Different electrical quantities and sub-metering values are available, as can be seen in the table below: Date_Time : the date and time of day of the measurement Global_active_power : household global minute-averaged active power (in kilowatt) Global_reactive_power : household global minute-averaged reactive power (in kilowatt) Voltage : minute-averaged voltage (in volt) Global_intensity : household global minute-averaged current intensity (in ampere) Sub_metering_1 : energy sub-metering No. 1 (in watt-hour of active energy). It corresponds to the kitchen, containing mainly a dishwasher, an oven and a microwave (hot plates are not electric but gas powered). Sub_metering_2 : energy sub-metering No. 2 (in watt-hour of active energy). It corresponds to the laundry room, containing a washing-machine, a tumble-drier, a refrigerator and a light. Sub_metering_3 : energy sub-metering No. 3 (in watt-hour of active energy). It corresponds to an electric water-heater and an air-conditioner. In [ ]: start_date = data . index . min () . date () end_date = data . index . max () . date () printmd ( f \"The dataset contains data from { start_date } to { end_date } .\" ) data . head () From the table below, we can see that for all sensors we have 2.049.280 instances. This already indicates that there are no missing values. Besides this information, we do not spot immediately any strange insights, e.g. an anomaly in sensor data like negative consumed power. In [ ]: data . describe () For the purpose of this Starter Kit, only the Date_Time and the Global_active_power attributes will be considered from now onwards. In [ ]: data = data [[ 'Global_active_power' ]] Climate data As an additional dataset, we use the climate information recorded close to the city of Paris, where the household is located, provided by the Wunderground website. It contains information on climate conditions for every 30 minutes, via the following attributes shown in the table below: time_UTC : the date and hour of the measurement temperature : temperature (degrees Celsius) dew_point : dew point (degrees Celsius) humidity : air humidity (percentage) sea_lvl_pressure : pressure at sea level (hPa) visibility : visibility (km) wind_dir_degrees : wind direction expressed in degrees (degrees) In [ ]: ext_data . head ( 5 ) From the statistics in the table below, we can see the data contains minimum values of -9999 for some variables. According to the documentation , values of -9999 or -999 signify missing values. We will replace these with the proper missing value NaN, as otherwise our algorithms would get confused. In [ ]: ext_data . describe () In [ ]: ext_data . replace ( to_replace =- 9999 , value = np . nan , inplace = True ) ext_data . describe () We can now notice that the temperature values seem to be more plausible. As expected, we also see that not all sensors have the same number of instances, meaning there are missing values. For the purpose of this Starter Kit, only the time_UTC and temperature attributes will be considered from now on. In [ ]: ext_data = ext_data [[ 'temperature' ]] Data fusion The evolution of household energy consumption may be influenced by climatological factors, e.g. lower or higher temperatures can lead to a higher energy consumption for some appliances such as water-heaters and air-conditioners. To understand and analyze this relationship, we need to fuse the household data with the climate data. However, these datasets have different sampling time: the household data is sampled every minute, whereas the climate data is sampled every 30 minutes. The integration of these 2 datasets thus requires careful consideration. Two options are possible: we can either upsample the climate data or downsample the household data. We select the second option, as otherwise, we would need to find a way to impute missing data in the climate dataset, and it allows us to reduce the size of the dataset, which will reduce compute time required for the analysis. The effect of downsampling the household data can be seen below. The user can experiment how the window sizes of 30 minutes, 1 hour, or 4 hours affect the evolution of the global active power. In [ ]: plot_effects_resampling ( data ) As we can see from the plot, resampling reduces the (small) peaks of the original signal. The effect becomes more pronounced for larger window sizes. A window size of 4 hours arguably removes too much variation. A window of 30 minutes or 1 hours on the other hand still captures the general evolution of the active power, which is exactly what is needed for characterizing the typical household consumption. Between 30 or 60min time windows, we decide to use a window size of 60min since this reduces the dataset and speeds up the computation time required later on for the analysis. To downsample the climate dataset, the mean of the values of the respective hour is used. To downsample the household dataset, the sum of each of the values for the corresponding hour is taken An excerpt of the downsampling of household dataset is shown in the table below. In [ ]: In [ ]: ext_data_h = ext_data . resample ( '1H' ) . apply ( np . mean ) data_h = data . resample ( '1H' ) . agg ({ 'Global_active_power' : np . sum , }) data_h [: 3 ] Since Global_reactive_power and Global_active_power are expressed in minute-averaged kW, we will change their units to Wh over an hour, such that this is in line with the resampling frequency. To this end, the following transformations are applied (the summing was already done in the previous step): \\begin{align*} \\mathit{globalactive}\\ Wh &= \\sum&#94;{60}_{min=1} 1000\\cdot\\mathit{globalactivepower}_{\\mathit{min}}\\ kW\\cdot \\frac{1}{60}\\ h\\\\ \\end{align*} The result of this transformation can be seen in the table below. In [ ]: data_h [ 'Global_active_power' ] = data_h [ 'Global_active_power' ] . apply ( lambda x : x * 1000 / 60 ) data_h [: 3 ] Now we can finally merge the two datasets into an extended dataset that will be explored in the next section. In [ ]: new_data = pd . concat (( data_h , ext_data_h [[ 'temperature' ]]), axis = 1 , join = 'inner' ) new_data [: 3 ] Before proceeding, we inspect to what extent this extended dataset contains missing data. In [ ]: new_data . isna () . mean () In case of too many missing values, we could consider data imputation. However, in this case, only the temperature has 0.01% of msising values, so there is no need for imputation. Data Exploration In this section, we explore the extended dataset in order to understand which factors influence the household energy consumption. Based on this exploration, we will later on identify which features are useful for training a machine learning algorithm to forecast energy consumption. There are several approaches to data exploration. In this section we will use: visual exploration by means of time plots statistical data exploration Visual Data Exploration Time plots are a common way for exploring time series data, such as the energy consumption that we consider here. We explore time plots in different time frames in order to highlight different aspects in the data: we start with a yearly plot, which allows us to identify global patterns. a monthly plot allows us to zoom in and understand relationships between energy consumption in different weeks and days a weekly plot allows us to zoom in even further and identify daily and hourly patterns Yearly patterns It is expected that energy consumption depends on the outdoor temperature, since in winter time heaters are used more frequently than in summer time for example. For this reason, we plot the evolution of the energy consumption along with the temperature. In the plot below, we resample the hourly data of both datasets to daily data, so that we don't get lost in details but can observe more global patterns. The user can toggle on or off the normalization of the data. The normalization that is applied is called min-max normalization and is explained in detail in SK 3.4. It applies the following formulate to the data in order to make sure all values are contained within the [0, 1] range: \\begin{align*} \\mathit{X_{scaled}} = \\frac{X - X_{min}}{X_{max} - {X_{min}}}\\\\ \\end{align*} The user is invited to verify, without selecting the option normalization, whether of temperature and global active power have the same trend. The reader can then repeat the analysis using the option normalization. In [ ]: plot_temperature_power_one_year ( new_data ) It should be clear that without normalization, it is difficult to compare the temperature with the global active power because both use different ranges, and the range of the global active power dominates the range of the temperature. With normalization however, the plot above reveals interesting insights. Both the temperature and the global active power follow a seasonal trend. Obviously, the temperature is lower in winter and higher in summer, whereas the reverse is true for the global active power. This is to be expected: energy consumption is higher during winter time. In more formal terms, we can observe that the temperate and global active power exhibit an inverse correlation: when one increases the other decreases, and vice versa. Monthly patterns Let's now see if there is a monthly recurring pattern as well. In the following figure, the active power of the selected year(s) is shown. The user can change the resampling rate to make the insights more clear to them. To make the plot more clear, the user can (de)selected the years they want on the right of the figure. In [ ]: plot_yearly_data ( new_data ) While we can again clearly see the seasonal effect, there is no clear pattern recurring every month. One interesting thing that can be seen is that strong dips in power consumption tend to happen in the same time in February and November in 2007 and 2008. Most likely, these are holidays which tend to happen at the same time for a fixed household. However, these dips are not present at the same time for 2009. Weekly patterns In this section we further explore the evolution of the energy consumption in a more fine grained time frame. We want to verify whether during a week, the energy consumption exhibits the same patterns. In the plot below, the user can inspect and compare the energy consumption in December of different years. We mark weekends in yellow and Christmas day in orange. We invite the user to see for each year if they can notice a difference between week days and weekends and if there are any changes around the Holidays. In [ ]: plot_consumption_with_holidays_weekends ( new_data ) What should be obvious from exploring this plot is the existence of a daily pattern for energy consumption dependent on the day of the week: in most weekend days of the three years, we can observe slightly higher energy consumption than during weekdays. This can depend on the common habits of working at the office or children being at school. The effect of the holidays can also be seen. On Christmas day in 2007, the power consumption is slightly higher than other typical weekdays, but similar values can be seen e.g. exactly one week before. So the holidays don't have a strong impact here. In 2008 on the other hand, the power consumption is lower, indicating that perhaps the household celebrated outside their house. In 2009, there is a peak in consumption on Christmas eve. We can conclude that whether the day is a weekday or in the weekend or a holiday has an influence on the expected power consumption. Daily patterns Next to yearly and monthly patterns, we are also interested in daily patterns. Below, we plot the evolution of the global active power for one week (The first week of December 2009). We can see how the power consumption follows a daily recurring pattern, with peaks in the morning and evening and troughs at night and midday. In fact, this pattern is also already present in the previous plot, where daily morning and evening peaks can also be observed. During the night and the day, we still see low amounts of power usage, most likely due to appliances like refrigerators and heating. In [ ]: plot_weekly_data ( new_data ) Statistical Data Exploration The visualizations above highlighted interesting patterns in the energy consumption of the household that can be exploited for feature extraction. In this section we use statistical methods to verify, and eventually quantify those observations. Autocorrelation In order to find repetitive patterns in time series data, we can use autocorrelation , which provides the correlation between the time series and a delayed copy of itself. In order to investigate this autocorrelation, the plot below visualizes the autocorrelation of the global active power with time lags of 1 day, 2 days, etc. Note that we do not show the results of a delay of 0 days, since the correlation with the exact same day would trivially be 1. In [ ]: plot_auto_correlation ( new_data ) The plot above reveals that a clear peak occurs after 1 day, indicating the consumption of a day is highly correlated with the consumption of the previous day another clear peak occurs every 7 days, which confirms the existence of a weekly pattern, i.e. the consumption of a day is highly correlated with the consumption of the same day a week earlier Correlation As visually observed above, the temperature influences the energy consumption of a household. In order to verify the strength of this relationship, we can compute the correlation between these two variables. For this purpose, we compute Pearson's correlation. The plot below shows again both temperature and consumption and allows to investigate their relationship. This relationship is influenced by the frequency of the dataset, e.g. temperature changes are typically less drastic than energy consumption changes. We invite the user to experiment with the different resampling periods, to understand how that changes the correlation and what frequency leads to the highest correlation. In [ ]: plot_correlation ( new_data ) What should be obvious from the above is that there is a negative correlation between temperature and energy consumption, meaning the consumption rises when temperature drops and vice versa. The larger the resampling window is chosen, the higher is the correlation. This is because with larger windows, we smooth out more the short-term fluctuations, leaving only the seasonal pattern described above. Feature engineering In this section, we will extract several features from the extended dataset that can be used for forecasting energy consumption. A feature is an individual measurable property of the phenomenon being observed. Choosing informative, discriminating and independent features is a crucial step for building effective machine learning algorithms. Based on the previous visual and statistical observation, we will consider the following features to predict the energy consumption for the hour ahead: energy consumption of the day before energy consumption of the 7 days before the hour of the day, which will allow to distinguish between different periods of the day (i.e. night, morning, afternoon, evening) the day of the week, which will allow to distinguish between weekdays and weekend days whether the day is a holiday or not the month of the year, which will allow to distinguish between yearly seasons the temperature In [ ]: new_feats = new_data [[ 'Global_active_power' ]] . rename ( columns = ({ 'Global_active_power' : 'consumption' })) new_feats [ 'Pastday' ] = new_feats [ 'consumption' ] . shift ( 1 , freq = 'D' ) new_feats [ 'Pastweek' ] = new_feats [ 'consumption' ] . shift ( 7 , freq = 'D' ) new_feats [ 'Hour' ] = new_feats . index . hour new_feats [ 'Weekday' ] = new_feats . index . dayofweek new_feats [ 'Month' ] = new_feats . index . month new_feats [ 'Holiday' ] = [ 1 if ( date in holidays . France ()) else 0 for date in new_feats . index ] new_feats [ 'Temperature' ] = new_data [ 'temperature' ] new_feats = new_feats . dropna () Modeling This section allows the user to discover the most important factors for training a machine learning model. More specifically, the user will get insights on the influence of the training strategy , type of machine learning model and effect of data normalization on model performance. To evaluate the performance of the models the mean absolute error (MAE) is used (a metric commonly used in literature for this purpose). This metric quantifies to what extent the model forecasts are close to the real values. As the name suggests, the mean absolute error is an average of the absolute errors. It holds that the lower the MAE of the model, the better its performance. This section ends with an interactive application where the user can test all options in order to find the one that leads the model to achieve the best performance. Training strategies The training data influences models performances. For this experiment, the following options are available: Use 1 month before the test month as training data. In this experiment, several models are trained for the predictions of each individual month. Each model is trained on one month before the month we are making predictions for. One month gap is introduced between the training and the test month (this is done to avoid that the last day of the training set is also included in the first day of the test set). For example, the models that will make the predictions for April 2008 will be trained on the data from February 2008. Use 6 months before the test month as training data. Similar to the above experiment, but with 6 months of training data. It still includes a 1 month gap. Use 1 year before the test month as training data. Similar to the above experiment, but with 1 year of training data, still include a 1 month gap. Use 1 month the year before as training data. Similar to strategy number 1, but with an 11 months gap. This way, the training data and test data are taken from the same month, one year apart. Use all months before the test month as training data. For each test month, train a model using all the data prior to this (including a 1 month gap). This strategy simulates the scenario where the model is retrained as new data comes in. Since this requires training several models based on potentially large training sets, this might require a certain computational time. Use train-test split . Train on the first year of data and make predictions on the rest. This strategy is commonly used for training these kinds of models. Notice that for the benchmark model, there is no training phase. Machine learning models There is a plethora of models that can be used for forecasting. In this notebook, we decide to use two models that are commonly used in literature for the task at hand: Random Forest Regression (RFR) and Support Vector Regression (SVR) . Besides these models, we will also use a simple benchmark model that we use to compare the performance of the other models against. For the prediction of the energy of a certain day, this benchmark model simply takes the energy consumption of the previous day at the same time. We will use these models to forecast the energy consumption based on the past energy measurements (see training strategies). Normalization Before being used for training, data may need to be normalized, i.e. rescaled so that all input and output values are between 0 and 1. This is indeed a requirement for the correct training of some machine learning models. It is left to the user the exercise of discovering which model requires the normalization of the data. In [ ]: from IPython.utils import io PORT = '8095' # in AWS use ports 8050-8060 mode = 'inline' #external if mode == 'inline' : run_app ( new_feats , mode = 'inline' , port = PORT ) else : with io . capture_output () as captured : run_app ( new_feats , mode = 'external' , port = PORT , host = '0.0.0.0' ) HOST = '0.0.0.0' # in AWS replace here by public IP print ( f 'Dash app running on http:// { HOST } : { PORT } ' ) The mean absolute error of each experiment that was run is reported in the table above. Insights from the experiments These are the basic insights the user could have noticed when trying different models, parameters and training strategies. Effect of the training strategy For the SVR, we see that the standard train-test split has the best performance, although using all months and using a one year window before the test month as training data both have a similar performance. For the RFR on the other hand, we clearly see that using a one year window before the test month as training data leads to better performance than the other training strategies. We note that for both this \"one year window\" strategy and the standard train-test split, the training data is one year. The difference however, is that for the former the training set changes, while for the latter it is fixed (using all the data from 2017). A general trend that can be seen is that the performance increases as a larger training set is used. We therefore urge the user to look at the predictions made using the training strategy where all months before the test month are used as training data: we can expect that the predictions for the later months will be better than for the first months since more training data was used. However, this is hard to see by eye in the prediction plot directly. Effect of the chosen model We can see that, for most parameter choices, both the SVR and RFR can outperform the simple benchmark model we set up. The RFR outperforms the SVR for most (if not all) parameter choices and training strategies. This shows the importance of knowing which model is best suited for your problem and testing different ones. Effect of the chosen parameters The search for the optimal set of hyperparameters for your machine learning model is a non-trivial task, since there might be a strong interplay between the parameters so we can not do the optimal parameter search one by one. Here, we let the user try a pre-set number of choices, but more automatic and smart search algorithms exist to tackle this problem. We found that for the RFR, the higher the number of estimators, the better the results, but at a greater computational cost and with diminishing returns. We see that 50 estimators and leaving the max depth at 10 is a good choice. For the SVR, $C=10$ and $\\gamma=0.01$ work well. Effect of normalization Normalizing the data, rescaling it so that the input and output variables all have values within a similar range (in this case, between 0 and 1) is a common step when setting up machine learning models. This is because, depending on the model, they work much better with values of the order 1. We see that normalization indeed greatly improves the predictions for the SVR, but has very little influence on the RFR. Indeed, algorithms such as random forest and decision tree are not influenced by the scale of the input variables. Conclusion In this Starter Kit we demonstrated forecasting the demand for a particular resource at a particular point in the future, illustrated in the case of energy demand forecasting. To this end, we first analysed the business problem, preprocessed the data, visually and numerically explored it to eventually illustrate the use of different models ( Random Forest Regression, Support Vector Regression and benchmark) to perform the actual forecasting with different amounts of available historical data. From the analysis, it may be clear that achieving good prediction results depend on different factors (appropriate data preprocessing, the amount of available training data, algorithm parametrisation, etc.). Additional Information Contributions by Mathias Verbeke, Wannes Meert, Vincent Vercruyssen, Alessandro Murgia, Tom Tourwé, and Robbert Verbeke. This Starter Kit was developed in the context of the EluciDATA project ( http://www.elucidata.be ). For more information, please contact info@elucidata.be .","tags":"Resource demand Forecasting","url":"/Resource demand Forecasting/2020-01-01-interactive-notebook.html","loc":"/Resource demand Forecasting/2020-01-01-interactive-notebook.html"}]};